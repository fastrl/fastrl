{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "cuda not used\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "print(torch.cuda.is_available())\n",
    "if torch.cuda.is_available() and False:\n",
    "    print (\"cuda in use\")\n",
    "    device = torch.device('cuda') \n",
    "    torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "    dtype = torch.cuda.FloatTensor\n",
    "else:\n",
    "    print (\"cuda not used\")\n",
    "    device = torch.device('cpu')\n",
    "    torch.set_default_tensor_type('torch.FloatTensor')\n",
    "    dtype = torch.float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from T. Kohonen '84, p. 183, equation 6.33\n",
    "\n",
    "#Set alpha to one for one-step orthogonalization.\n",
    "class Novelty_Filter():\n",
    "    \n",
    "    def __init__(self,size,epsilon=1e-5):\n",
    "        self.O = torch.eye(size, requires_grad=False, dtype=dtype, device=device)\n",
    "        self.epsilon = epsilon\n",
    "        return\n",
    "        \n",
    "    def addBasis(self,X,cf=1.):\n",
    "        l = self.novelty(X)\n",
    "        n = l.norm()\n",
    "        print(\"norms\",n,X.norm())\n",
    "        if(cf * n > self.epsilon * X.norm()):\n",
    "            print(\"suppressing\")\n",
    "            self.O -= cf * l.ger(l) / n.pow(2)\n",
    "        return\n",
    "    \n",
    "#     def suppressNovelty(self,X,cf=1.):\n",
    "#         #Memorize this (linear) pattern\n",
    "#         n = torch.norm(X)\n",
    "#         if (n > self.epsilon):\n",
    "#             print(\"suppressing\",X,\"norm\",n)\n",
    "#             self.O -= cf * X.ger(X) / n.pow(2)\n",
    "#         return\n",
    "    \n",
    "    def novelty(self,X):\n",
    "        return self.O @ X\n",
    "    \n",
    "    def project(self,X):\n",
    "        return X - self.novelty(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data tensor([[1., 0., 0., 0.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 1., 0., 1.],\n",
      "        [1., 1., 1., 0.]])\n"
     ]
    }
   ],
   "source": [
    "def selu(x):\n",
    "    alpha = 1.6732632423543772848170429916717\n",
    "    scale = 1.0507009873554804934193349852946\n",
    "    return scale * F.elu(x, alpha)\n",
    "\n",
    "class Xor(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Xor, self).__init__() \n",
    "        hidden_nodes = 100\n",
    "        self.l1 = nn.Linear(3,hidden_nodes,bias=False)\n",
    "        self.l1.weight.data.normal_(0.0, np.sqrt(1./3.))\n",
    "        self.head = nn.Linear(hidden_nodes, 1,bias=False)\n",
    "        self.head.weight.data.normal_(0.0, np.sqrt(1./hidden_nodes))\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.l1_out = selu(self.l1(inputs))\n",
    "        self.value = torch.sigmoid(self.head(self.l1_out))\n",
    "        return self.value\n",
    "    \n",
    "    def get_loss(self,target):\n",
    "        self.loss = F.mse_loss(target,self.value)\n",
    "        return self.loss\n",
    "\n",
    "class Stable_Xor(Xor):\n",
    "    \n",
    "    def __init__(self,alpha=0.):\n",
    "        super(Stable_Xor, self).__init__()  \n",
    "        self.alpha = alpha\n",
    "        self.l1_size = self.l1.weight.size()\n",
    "        self.l1_length = self.l1.in_features * self.l1.out_features\n",
    "        self.l1_filter = Novelty_Filter(self.l1_length)\n",
    "        self.head_size = self.head.weight.size()\n",
    "        self.head_length = self.head.in_features * self.head.out_features\n",
    "        self.head_filter = Novelty_Filter(self.head_length)\n",
    "    \n",
    "    def do_post_gradient(self):\n",
    "        with torch.no_grad():\n",
    "            self.l1_vector = self.l1.weight.view(self.l1_length)\n",
    "            novelty = self.l1_filter.novelty(self.l1_vector)\n",
    "            self.l1.weight.grad *= novelty.reshape(self.l1_size).detach()\n",
    "            self.l2_vector = self.head.weight.view(self.head_length)\n",
    "            novelty = self.head_filter.novelty(self.l2_vector)\n",
    "            self.head.weight.grad *= novelty.reshape(self.head_size).detach()\n",
    "#             self.grad1_vector = self.l1.weight.grad.view(self.l1_length)\n",
    "#             novelty = self.l1_filter.novelty(self.grad1_vector)\n",
    "#             self.l1.weight.grad = novelty.reshape(self.l1_size).detach()\n",
    "#             self.grad2_vector = self.head.weight.grad.view(self.head_length)\n",
    "#             novelty = self.head_filter.novelty(self.grad2_vector)\n",
    "#             self.head.weight.grad = novelty.reshape(self.head_size).detach()\n",
    "        \n",
    "    def do_pre_update(self):\n",
    "        certainty_factor = self.alpha/torch.exp(abs(self.loss.data))\n",
    "        print(\"Critic cf:\",certainty_factor)\n",
    "        with torch.no_grad():\n",
    "            self.l1_filter.addBasis(self.l1_vector,certainty_factor)\n",
    "            self.head_filter.addBasis(self.l2_vector,certainty_factor)\n",
    "        \n",
    "data = torch.tensor([[1,0,0,0],[1,0,1,1],[1,1,0,1],[1,1,1,0]], requires_grad=False, dtype=dtype, device=device)\n",
    "print(\"data\",data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "input tensor([1., 0., 0.]) output tensor([1.]) target tensor([0.])\n",
      "loss tensor(1., grad_fn=<MeanBackward0>) tensor([1.], grad_fn=<SigmoidBackward>) tensor([0.])\n",
      "Critic cf: tensor(0.3679)\n",
      "norms tensor(8.1988) tensor(8.1988)\n",
      "suppressing\n",
      "norms tensor(6.7700) tensor(6.7700)\n",
      "suppressing\n",
      "Current Forecasts\n",
      "input tensor([1., 0., 0.]) output tensor([1.]) target tensor([0.])\n",
      "input tensor([1., 0., 1.]) output tensor([1.]) target tensor([1.])\n",
      "input tensor([1., 1., 0.]) output tensor([1.]) target tensor([1.])\n",
      "input tensor([1., 1., 1.]) output tensor([1.]) target tensor([0.])\n",
      "End\n",
      "input tensor([1., 0., 1.]) output tensor([1.]) target tensor([1.])\n",
      "loss tensor(0., grad_fn=<MeanBackward0>) tensor([1.], grad_fn=<SigmoidBackward>) tensor([1.])\n",
      "Critic cf: tensor(1.)\n",
      "norms tensor(4.2137) tensor(6.5418)\n",
      "suppressing\n",
      "norms tensor(3.6762) tensor(5.6981)\n",
      "suppressing\n",
      "Current Forecasts\n",
      "input tensor([1., 0., 0.]) output tensor([1.]) target tensor([0.])\n",
      "input tensor([1., 0., 1.]) output tensor([1.]) target tensor([1.])\n",
      "input tensor([1., 1., 0.]) output tensor([1.]) target tensor([1.])\n",
      "input tensor([1., 1., 1.]) output tensor([1.]) target tensor([0.])\n",
      "End\n",
      "input tensor([1., 1., 0.]) output tensor([1.]) target tensor([1.])\n",
      "loss tensor(0., grad_fn=<MeanBackward0>) tensor([1.], grad_fn=<SigmoidBackward>) tensor([1.])\n",
      "Critic cf: tensor(1.)\n",
      "norms tensor(1.7432) tensor(4.6715)\n",
      "suppressing\n",
      "norms tensor(1.6621) tensor(4.4883)\n",
      "suppressing\n",
      "Current Forecasts\n",
      "input tensor([1., 0., 0.]) output tensor([1.]) target tensor([0.])\n",
      "input tensor([1., 0., 1.]) output tensor([1.]) target tensor([1.])\n",
      "input tensor([1., 1., 0.]) output tensor([1.]) target tensor([1.])\n",
      "input tensor([1., 1., 1.]) output tensor([1.]) target tensor([0.])\n",
      "End\n",
      "input tensor([1., 1., 1.]) output tensor([0.]) target tensor([0.])\n",
      "loss tensor(0., grad_fn=<MeanBackward0>) tensor([0.], grad_fn=<SigmoidBackward>) tensor([0.])\n",
      "Critic cf: tensor(1.)\n",
      "norms tensor(29.6941) tensor(29.5571)\n",
      "suppressing\n",
      "norms tensor(23.8242) tensor(22.4629)\n",
      "suppressing\n",
      "Current Forecasts\n",
      "input tensor([1., 0., 0.]) output tensor([1.]) target tensor([0.])\n",
      "input tensor([1., 0., 1.]) output tensor([1.]) target tensor([1.])\n",
      "input tensor([1., 1., 0.]) output tensor([1.]) target tensor([1.])\n",
      "input tensor([1., 1., 1.]) output tensor([0.]) target tensor([0.])\n",
      "End\n"
     ]
    }
   ],
   "source": [
    "    \n",
    "def showme():\n",
    "    print(\"Current Forecasts\")\n",
    "    for j in range(4):\n",
    "        print(\"input\",data[j][0:3],\"output\",xor(data[j][0:3]).data,\"target\",data[j][3:4])\n",
    "    print(\"End\")\n",
    "    \n",
    "xor = Stable_Xor(1.)\n",
    "xopt = optim.Adam(xor.parameters(), lr=1e-1,weight_decay=0.000001)\n",
    "iterations=100\n",
    "\n",
    "\n",
    "print(\"---\")\n",
    "for j in range(iterations):\n",
    "    xor(data[0][0:3])\n",
    "    xor.get_loss(data[0][3:4]).backward()\n",
    "    xor.do_post_gradient()\n",
    "    xopt.step()\n",
    "    xopt.zero_grad()\n",
    "\n",
    "print(\"input\",data[0][0:3],\"output\",xor(data[0][0:3]).data,\"target\",data[0][3:4])\n",
    "    \n",
    "xor(data[0][0:3])\n",
    "loss=xor.get_loss(data[0][3:4])\n",
    "print(\"loss\",loss,xor.value,data[0][3:4])\n",
    "loss.backward()\n",
    "xor.do_post_gradient()\n",
    "xor.do_pre_update()\n",
    "xopt.step()\n",
    "xopt.zero_grad()\n",
    "\n",
    "showme()\n",
    "for j in range(iterations):\n",
    "    xor(data[1][0:3])\n",
    "    xor.get_loss(data[1][3:4]).backward()\n",
    "    xor.do_post_gradient()\n",
    "    xopt.step()\n",
    "    xopt.zero_grad()\n",
    "    \n",
    "print(\"input\",data[1][0:3],\"output\",xor(data[1][0:3]).data,\"target\",data[1][3:4])\n",
    "xor(data[1][0:3])\n",
    "loss=xor.get_loss(data[1][3:4])\n",
    "print(\"loss\",loss,xor.value,data[1][3:4])\n",
    "loss.backward()\n",
    "xor.do_post_gradient()\n",
    "xor.do_pre_update()\n",
    "xopt.step()\n",
    "xopt.zero_grad()\n",
    "\n",
    "showme()\n",
    "for j in range(iterations):\n",
    "    xor(data[2][0:3])\n",
    "    xor.get_loss(data[2][3:4]).backward()\n",
    "    xor.do_post_gradient()\n",
    "    xopt.step()\n",
    "    xopt.zero_grad()\n",
    "    \n",
    "print(\"input\",data[2][0:3],\"output\",xor(data[2][0:3]).data,\"target\",data[2][3:4])\n",
    "xor(data[2][0:3])\n",
    "loss=xor.get_loss(data[2][3:4])\n",
    "print(\"loss\",loss,xor.value,data[2][3:4])\n",
    "loss.backward()\n",
    "xor.do_post_gradient()\n",
    "xor.do_pre_update()\n",
    "xopt.step()\n",
    "xopt.zero_grad()\n",
    "showme()\n",
    "for j in range(iterations):\n",
    "    xor(data[3][0:3])\n",
    "    xor.get_loss(data[3][3:4]).backward()\n",
    "    xor.do_post_gradient()\n",
    "    xopt.step()\n",
    "    xopt.zero_grad()\n",
    "    \n",
    "print(\"input\",data[3][0:3],\"output\",xor(data[3][0:3]).data,\"target\",data[3][3:4])\n",
    "xor(data[3][0:3])\n",
    "loss=xor.get_loss(data[3][3:4])\n",
    "print(\"loss\",loss,xor.value,data[3][3:4])\n",
    "loss.backward()\n",
    "xor.do_post_gradient()\n",
    "xor.do_pre_update()\n",
    "xopt.step()\n",
    "xopt.zero_grad()\n",
    "    \n",
    "showme()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss tensor(1., grad_fn=<MeanBackward0>) tensor([2.0785e-09], grad_fn=<SigmoidBackward>) tensor([1.])\n",
      "Critic cf: tensor(0.3679)\n",
      "norms tensor(15.2625) tensor(0.0003)\n",
      "suppressing\n",
      "norms tensor(22.7214) tensor(0.0003)\n",
      "suppressing\n",
      "input tensor([1., 1., 0.]) output tensor([0.]) target tensor([1.])\n",
      "loss tensor(1., grad_fn=<MeanBackward0>) tensor([0.], grad_fn=<SigmoidBackward>) tensor([1.])\n",
      "Critic cf: tensor(0.3679)\n",
      "norms tensor(46.8379) tensor(0.0003)\n",
      "suppressing\n",
      "norms tensor(19.6946) tensor(0.0002)\n",
      "suppressing\n",
      "input tensor([1., 1., 1.]) output tensor([0.]) target tensor([0.])\n",
      "loss tensor(0., grad_fn=<MeanBackward0>) tensor([0.], grad_fn=<SigmoidBackward>) tensor([0.])\n",
      "final\n",
      "input tensor([1., 0., 0.]) output tensor([5.6292e-11]) target tensor([0.])\n",
      "input tensor([1., 0., 1.]) output tensor([1.3473e-30]) target tensor([1.])\n",
      "input tensor([1., 1., 0.]) output tensor([0.]) target tensor([1.])\n",
      "input tensor([1., 1., 1.]) output tensor([0.]) target tensor([0.])\n",
      "tensor([[ 1.0000e+00,  3.4445e-28, -3.5305e-28,  ..., -1.9739e-04,\n",
      "          8.1621e-07, -8.5400e-08],\n",
      "        [ 3.4445e-28,  1.0000e+00,  0.0000e+00,  ...,  1.2649e-25,\n",
      "         -2.1162e-40,  2.1255e-41],\n",
      "        [-3.5305e-28,  0.0000e+00,  1.0000e+00,  ..., -1.2965e-25,\n",
      "         -1.6728e-40,  1.9241e-41],\n",
      "        ...,\n",
      "        [-1.9739e-04,  1.2649e-25, -1.2965e-25,  ...,  8.0576e-01,\n",
      "          2.4948e-02, -2.5482e-03],\n",
      "        [ 8.1621e-07, -2.1162e-40, -1.6728e-40,  ...,  2.4948e-02,\n",
      "          9.7143e-01,  2.9429e-03],\n",
      "        [-8.5400e-08,  2.1255e-41,  1.9241e-41,  ..., -2.5482e-03,\n",
      "          2.9429e-03,  9.9969e-01]])\n",
      "tensor([[ 9.9980e-01, -3.1482e-03,  8.6447e-05, -1.9114e-04,  2.5282e-03,\n",
      "          3.9448e-03, -3.5941e-03, -1.1173e-03, -4.1265e-03, -1.1847e-03,\n",
      "         -5.2528e-04, -2.6030e-03, -3.2148e-04, -8.6372e-04, -3.4399e-03,\n",
      "          2.0689e-04,  4.2068e-05, -3.3584e-04, -3.5130e-03, -5.8464e-04,\n",
      "         -8.9328e-04,  3.2713e-03,  2.8768e-03, -6.9724e-04, -3.9455e-04,\n",
      "         -2.0326e-05, -4.7027e-04,  2.5027e-03, -4.0769e-04,  2.4531e-03,\n",
      "         -1.6279e-03,  2.5942e-04, -3.6373e-03, -1.9545e-03,  9.1456e-04,\n",
      "          1.2467e-04,  3.5577e-03,  3.5954e-03,  3.4669e-03, -2.4618e-03,\n",
      "         -3.9599e-05,  2.3801e-04,  3.6440e-03,  1.6863e-03,  4.8856e-04,\n",
      "         -2.6681e-03,  2.6929e-03, -5.1886e-04,  2.2834e-05,  2.9228e-04,\n",
      "         -2.8418e-03, -8.7078e-04, -3.1409e-03, -2.5805e-03, -1.1870e-03,\n",
      "         -8.9811e-04,  1.0802e-03, -1.7397e-04, -2.6855e-03,  5.9476e-04,\n",
      "          3.5346e-03, -8.4241e-04, -6.7937e-04, -9.4206e-04, -4.6163e-03,\n",
      "         -8.6154e-05, -4.7481e-04, -5.8407e-04,  3.9878e-04, -2.4636e-03,\n",
      "         -2.7844e-03,  3.1465e-03,  1.5802e-03,  1.0383e-03, -3.5057e-03,\n",
      "          3.8329e-03,  3.1835e-03, -8.4668e-05,  1.1620e-03, -2.0196e-04,\n",
      "         -3.6182e-03, -4.0352e-03,  6.9796e-04,  1.0742e-03, -9.6614e-04,\n",
      "          3.2422e-03, -3.1479e-04,  7.4936e-05,  1.5104e-04, -1.2613e-03,\n",
      "          1.5313e-04,  8.4461e-04, -2.3382e-04, -2.6000e-03,  2.2469e-04,\n",
      "          2.3735e-03, -2.7877e-03, -3.8637e-03, -5.2053e-04, -3.0514e-03],\n",
      "        [-3.1482e-03,  8.8342e-01,  9.3696e-04, -3.7194e-03,  5.4404e-03,\n",
      "          5.2374e-02, -1.2568e-01, -2.3410e-02, -1.3479e-01, -1.4087e-02,\n",
      "         -2.0086e-03, -2.5721e-02, -5.3233e-03, -1.2634e-02, -3.8632e-02,\n",
      "          3.6554e-03,  5.4608e-04, -5.0055e-03, -1.2117e-01, -5.1947e-03,\n",
      "         -1.5367e-02,  2.3935e-02,  1.1208e-02, -5.0175e-03, -6.0333e-03,\n",
      "         -2.2005e-04, -8.8809e-03,  1.0613e-02, -4.7347e-03,  4.5436e-03,\n",
      "         -1.3971e-02,  4.2582e-03, -1.2609e-01, -8.7633e-02,  2.4345e-03,\n",
      "          1.1389e-03,  2.3464e-02,  3.1248e-02,  2.0040e-02, -2.7648e-02,\n",
      "         -5.6522e-04,  3.2738e-03,  2.4977e-02,  2.1261e-03,  8.1593e-03,\n",
      "         -1.0204e-01,  2.3734e-02, -6.4318e-03,  3.3523e-04,  3.2635e-03,\n",
      "         -3.9262e-02, -7.6433e-03, -1.1016e-02, -9.9031e-02, -1.5798e-02,\n",
      "         -1.3228e-02,  9.2189e-03, -2.8574e-03, -4.9619e-03,  8.1403e-03,\n",
      "          2.5079e-02, -1.2262e-02, -6.4747e-03, -9.3632e-03, -3.5356e-02,\n",
      "         -1.0173e-03, -8.0577e-03, -9.0977e-03,  3.7557e-03, -3.4870e-02,\n",
      "         -1.0790e-01,  1.4227e-02,  2.6770e-03,  9.2579e-03, -2.1972e-02,\n",
      "          8.3788e-02,  2.3541e-02, -1.3977e-03, -3.4009e-03, -3.4453e-03,\n",
      "         -8.3987e-02, -1.3273e-01,  3.9354e-03, -2.6948e-03, -1.9676e-02,\n",
      "          2.1997e-02, -3.3650e-03,  1.0018e-03,  1.4125e-03, -1.5803e-02,\n",
      "          1.4734e-03,  4.6142e-03, -3.8165e-03, -1.0188e-01,  2.9228e-03,\n",
      "          3.7644e-03, -1.0606e-01, -1.2809e-01, -8.7535e-03, -1.1267e-01],\n",
      "        [ 8.6447e-05,  9.3696e-04,  9.9994e-01,  8.1543e-05, -2.0040e-03,\n",
      "         -2.6597e-03,  1.2036e-03,  5.6094e-04,  1.5821e-03,  7.4774e-04,\n",
      "          4.3509e-04,  1.7149e-03,  1.4363e-04,  3.6806e-04,  2.3855e-03,\n",
      "         -9.0434e-05, -2.5506e-05,  1.3986e-04,  1.2350e-03,  4.2681e-04,\n",
      "          4.7426e-04, -2.4605e-03, -2.2490e-03,  5.3260e-04,  1.6775e-04,\n",
      "          6.8461e-06,  2.2799e-04, -1.9539e-03,  1.4358e-04, -1.9498e-03,\n",
      "          1.2185e-03, -1.5398e-04,  1.2492e-03,  3.8668e-04, -7.3732e-04,\n",
      "         -8.9540e-05, -2.7034e-03, -2.6547e-03, -2.6683e-03,  1.5697e-03,\n",
      "          1.5993e-05, -9.3770e-05, -2.7651e-03, -1.3187e-03, -2.3019e-04,\n",
      "          6.7459e-04, -1.9599e-03,  1.9069e-04, -9.3688e-06, -1.8573e-04,\n",
      "          1.7411e-03,  6.4477e-04,  2.4834e-03,  6.4913e-04,  7.4595e-04,\n",
      "          3.8589e-04, -6.7082e-04,  7.5702e-05,  2.1629e-03, -3.8309e-04,\n",
      "         -2.6739e-03,  3.5364e-04,  4.9184e-04,  6.7293e-04,  3.4655e-03,\n",
      "          3.0666e-05,  2.2202e-04,  2.5406e-04, -2.7173e-04,  1.4758e-03,\n",
      "          7.4237e-04, -2.4423e-03, -1.2204e-03, -6.5206e-04,  2.6919e-03,\n",
      "         -2.0798e-03, -2.3891e-03,  3.9483e-05, -1.0583e-03,  9.5776e-05,\n",
      "          1.8755e-03,  1.5013e-03, -5.3487e-04, -9.7980e-04,  4.7048e-04,\n",
      "         -2.4574e-03,  1.0519e-04, -2.8958e-05, -1.0820e-04,  7.9180e-04,\n",
      "         -1.0667e-04, -6.1901e-04,  1.2432e-04,  6.3489e-04, -1.3100e-04,\n",
      "         -1.8897e-03,  7.5904e-04,  1.4099e-03,  2.4329e-04,  9.3472e-04],\n",
      "        [-1.9114e-04, -3.7194e-03,  8.1543e-05,  9.9981e-01,  2.2958e-03,\n",
      "          3.9983e-03, -4.2004e-03, -1.1808e-03, -4.7730e-03, -1.1443e-03,\n",
      "         -4.7352e-04, -2.5224e-03, -3.1238e-04, -8.0396e-04, -3.4193e-03,\n",
      "          2.0436e-04,  4.0400e-05, -3.1315e-04, -4.1097e-03, -5.5050e-04,\n",
      "         -9.0010e-04,  3.1486e-03,  2.6758e-03, -6.4903e-04, -3.7179e-04,\n",
      "         -1.6865e-05, -4.8048e-04,  2.3199e-03, -3.4644e-04,  2.2169e-03,\n",
      "         -1.5715e-03,  2.6361e-04, -4.2478e-03, -2.4328e-03,  7.9561e-04,\n",
      "          1.1662e-04,  3.4225e-03,  3.5102e-03,  3.3090e-03, -2.3998e-03,\n",
      "         -3.6302e-05,  2.1513e-04,  3.5107e-03,  1.4619e-03,  4.7965e-04,\n",
      "         -3.1626e-03,  2.5973e-03, -4.5123e-04,  2.1151e-05,  2.7445e-04,\n",
      "         -2.8424e-03, -8.2862e-04, -2.9437e-03, -3.0665e-03, -1.1656e-03,\n",
      "         -8.3869e-04,  9.6605e-04, -1.6799e-04, -2.4766e-03,  5.9429e-04,\n",
      "          3.4082e-03, -7.8121e-04, -6.4729e-04, -9.0353e-04, -4.5121e-03,\n",
      "         -7.3636e-05, -4.6781e-04, -5.5526e-04,  3.6891e-04, -2.4584e-03,\n",
      "         -3.3210e-03,  2.9543e-03,  1.3658e-03,  9.3732e-04, -3.3799e-03,\n",
      "          4.1350e-03,  3.0619e-03, -8.2719e-05,  1.0054e-03, -1.9969e-04,\n",
      "         -3.9358e-03, -4.6673e-03,  6.2673e-04,  9.2960e-04, -1.0096e-03,\n",
      "          3.1069e-03, -2.6009e-04,  6.7008e-05,  1.4186e-04, -1.2311e-03,\n",
      "          1.4306e-04,  7.4419e-04, -2.3227e-04, -3.0975e-03,  2.1423e-04,\n",
      "          2.1343e-03, -3.3143e-03, -4.4755e-03, -5.1146e-04, -3.6138e-03],\n",
      "        [ 2.5282e-03,  5.4404e-03, -2.0040e-03,  2.2958e-03,  9.1814e-01,\n",
      "         -9.8017e-02,  1.4320e-02,  1.6847e-02,  2.8337e-02,  2.6344e-02,\n",
      "          1.6867e-02,  6.4465e-02,  4.2371e-03,  1.0827e-02,  8.9830e-02,\n",
      "         -2.6173e-03, -8.5810e-04,  4.0631e-03,  1.7544e-02,  1.5645e-02,\n",
      "          1.4945e-02, -9.6899e-02, -9.1179e-02,  2.0009e-02,  4.8993e-03,\n",
      "          1.8870e-04,  6.8434e-03, -7.8248e-02,  4.0050e-03, -7.9694e-02,\n",
      "          4.6721e-02, -5.1331e-03,  1.6282e-02, -9.0256e-03, -2.8240e-02,\n",
      "         -3.2300e-03, -1.0816e-01, -1.0343e-01, -1.0747e-01,  5.7370e-02,\n",
      "          4.6128e-04, -2.6893e-03, -1.1020e-01, -5.2265e-02, -6.9348e-03,\n",
      "         -1.5139e-03, -7.4724e-02,  5.3778e-03, -2.7101e-04, -6.3802e-03,\n",
      "          6.1523e-02,  2.4071e-02,  1.0244e-01, -1.4873e-03,  2.5853e-02,\n",
      "          1.1380e-02, -2.3355e-02,  2.2106e-03,  9.0409e-02, -1.3559e-02,\n",
      "         -1.0611e-01,  1.0344e-02,  1.8050e-02,  2.4693e-02,  1.3859e-01,\n",
      "          8.5776e-04,  6.6542e-03,  7.4738e-03, -9.5918e-03,  5.1330e-02,\n",
      "         -3.2941e-04, -9.8916e-02, -4.7820e-02, -2.2704e-02,  1.0879e-01,\n",
      "         -6.4663e-02, -9.3821e-02,  1.1862e-03, -4.4519e-02,  2.8853e-03,\n",
      "          5.5799e-02,  2.5347e-02, -1.9906e-02, -4.0889e-02,  1.3998e-02,\n",
      "         -9.7335e-02,  2.8931e-03, -8.2673e-04, -3.9039e-03,  2.7846e-02,\n",
      "         -3.7938e-03, -2.2710e-02,  3.9359e-03, -3.3494e-03, -4.3497e-03,\n",
      "         -7.7222e-02,  1.1760e-03,  2.2845e-02,  7.2989e-03,  6.8772e-03]])\n"
     ]
    }
   ],
   "source": [
    "xor(data[1][0:3])\n",
    "loss=xor.get_loss(data[1][3:4])\n",
    "print(\"loss\",loss,xor.value,data[1][3:4])\n",
    "loss.backward()\n",
    "xor.do_post_gradient()\n",
    "xor.do_pre_update()\n",
    "xopt.step()\n",
    "xopt.zero_grad()\n",
    "\n",
    "for j in range(iterations):\n",
    "    xor(data[2][0:3])\n",
    "    xor.get_loss(data[2][3:4]).backward()\n",
    "    xor.do_post_gradient()\n",
    "    xopt.step()\n",
    "    xopt.zero_grad()\n",
    "\n",
    "print(\"input\",data[2][0:3],\"output\",xor(data[2][0:3]).data,\"target\",data[2][3:4])\n",
    "# print(\"novelty of input, is\",xor.l1_filter.novelty(xor.inputs))\n",
    "# print(\"novelty of hidden, is\",xor.head_filter.novelty(xor.l1_out))\n",
    "\n",
    "xor(data[2][0:3])\n",
    "loss=xor.get_loss(data[2][3:4])\n",
    "print(\"loss\",loss,xor.value,data[2][3:4])\n",
    "loss.backward()\n",
    "xor.do_post_gradient()\n",
    "xor.do_pre_update()\n",
    "xopt.step()\n",
    "xopt.zero_grad()\n",
    "\n",
    "for j in range(iterations):\n",
    "    xor(data[3][0:3])\n",
    "    xor.get_loss(data[3][3:4]).backward()\n",
    "    xor.do_post_gradient()\n",
    "    xopt.step()\n",
    "    xopt.zero_grad()\n",
    "\n",
    "print(\"input\",data[3][0:3],\"output\",xor(data[3][0:3]).data,\"target\",data[3][3:4])\n",
    "# print(\"novelty of input, is\",xor.l1_filter.novelty(xor.inputs))\n",
    "# print(\"novelty of hidden, is\",xor.head_filter.novelty(xor.l1_out))\n",
    "\n",
    "xor(data[3][0:3])\n",
    "loss=xor.get_loss(data[3][3:4])\n",
    "print(\"loss\",loss,xor.value,data[3][3:4])\n",
    "    \n",
    "print(\"final\")\n",
    "for j in range(4):\n",
    "    print(\"input\",data[j][0:3],\"output\",xor(data[j][0:3]).data,\"target\",data[j][3:4])\n",
    "#     print(\"novelty of input, is\",xor.l1_filter.novelty(xor.inputs))\n",
    "#     print(\"novelty of hidden, is\",xor.head_filter.novelty(xor.l1_out))\n",
    "    \n",
    "print(xor.l1_filter.O)\n",
    "print(xor.head_filter.O[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 7.0000,  0.1000, 11.0000])\n",
      "tensor([0.2000])\n",
      "norms tensor(13.0388) tensor(0.0001)\n",
      "suppressing\n",
      "tensor([5.6000, 0.0800, 8.8000]) tensor([ 6.5899, -3.0059, -0.5444])\n",
      "tensor(13.0388) tensor(10.4310)\n",
      "tensor([0.1810])\n",
      "norms tensor(10.4310) tensor(0.0001)\n",
      "suppressing\n",
      "tensor([3.6207, 0.0517, 5.6896]) tensor([ 6.0101, -3.0141, -1.4555])\n",
      "tensor(13.0388) tensor(6.7442)\n",
      "tensor([0.1637])\n",
      "norms tensor(6.7442) tensor(0.0001)\n",
      "suppressing\n",
      "tensor([-0.6637, -0.0095, -1.0430]) tensor([ 4.7551, -3.0321, -3.4277])\n",
      "tensor(13.0388) tensor(1.2363)\n",
      "tensor([0.1482])\n",
      "norms tensor(1.2363) tensor(0.0001)\n",
      "suppressing\n",
      "tensor([-116.0302,   -1.6576, -182.3331]) tensor([-29.0385,  -3.5148, -56.5319])\n",
      "tensor(13.0388) tensor(216.1275)\n",
      "tensor([0.1341])\n",
      "norms tensor(216.1275) tensor(0.0001)\n",
      "suppressing\n",
      "tensor([-116.0336,   -1.6576, -182.3385]) tensor([-29.0395,  -3.5148, -56.5335])\n",
      "tensor(13.0388) tensor(216.1339)\n",
      "tensor([0.1213])\n",
      "norms tensor(216.1339) tensor(0.0001)\n",
      "suppressing\n",
      "tensor([-116.0367,   -1.6577, -182.3433]) tensor([-29.0404,  -3.5149, -56.5349])\n",
      "tensor(13.0388) tensor(216.1396)\n",
      "tensor([0.1098])\n",
      "norms tensor(216.1396) tensor(0.0001)\n",
      "suppressing\n",
      "tensor([-116.0395,   -1.6577, -182.3477]) tensor([-29.0412,  -3.5149, -56.5362])\n",
      "tensor(13.0388) tensor(216.1449)\n",
      "tensor([0.0993])\n",
      "norms tensor(216.1449) tensor(0.0001)\n",
      "suppressing\n",
      "tensor([-116.0420,   -1.6577, -182.3517]) tensor([-29.0419,  -3.5149, -56.5373])\n",
      "tensor(13.0388) tensor(216.1495)\n",
      "tensor([0.0899])\n",
      "norms tensor(216.1495) tensor(0.0001)\n",
      "suppressing\n",
      "tensor([-116.0443,   -1.6578, -182.3553]) tensor([-29.0426,  -3.5149, -56.5384])\n",
      "tensor(13.0388) tensor(216.1538)\n",
      "tensor([0.0813])\n",
      "norms tensor(216.1538) tensor(0.0001)\n",
      "suppressing\n",
      "tensor([-116.0463,   -1.6578, -182.3585]) tensor([-29.0432,  -3.5149, -56.5393])\n",
      "tensor(13.0388) tensor(216.1577)\n"
     ]
    }
   ],
   "source": [
    "    \n",
    "f = Novelty_Filter(3)\n",
    "X = torch.tensor([7,.1,11], requires_grad=False, dtype=dtype, device=device)\n",
    "X2 = torch.tensor([7,-3,.1], requires_grad=False, dtype=dtype, device=device)\n",
    "v = torch.ones([1], requires_grad=False, dtype=dtype, device=device)\n",
    "print(X)\n",
    "alpha = 0.2\n",
    "for i in range(10):\n",
    "    certainty_factor = alpha/torch.exp(abs(i*.1*v))\n",
    "    print(certainty_factor)\n",
    "    f.addBasis(X,certainty_factor)\n",
    "    print(f.novelty(X),f.novelty(X2))\n",
    "    print(torch.norm(X),torch.norm(f.novelty(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xor = Xor()\n",
    "# xopt = optim.Adam(xor.parameters(), lr=3e-2,weight_decay=0.00001)\n",
    "\n",
    "# print(data)\n",
    "# for i in range(1000):\n",
    "#     for j in range(4):\n",
    "#         xor(data[j][0:2])\n",
    "#         xor.get_loss(data[j][2:3]).backward()\n",
    "#     xopt.step()\n",
    "#     xopt.zero_grad()\n",
    "\n",
    "# for j in range(4):\n",
    "#     print(xor(data[j][0:2]).data,data[j][2:3])\n",
    "    \n",
    "# xor = Xor()\n",
    "# xopt = optim.Adam(xor.parameters(), lr=3e-2,weight_decay=0.00001)\n",
    "\n",
    "# print(\"---\")\n",
    "# for i in range(1000):\n",
    "#     for j in range(4):\n",
    "#         xor(data[j][0:2])\n",
    "#         xor.get_loss(data[j][2:3]).backward()\n",
    "#         xopt.step()\n",
    "#         xopt.zero_grad()\n",
    "        \n",
    "# for j in range(4):\n",
    "#     print(xor(data[j][0:2]).data,data[j][2:3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "#from T. Kohonen '84, p. 119, equation 4.63, p. 122, equation 4.68\n",
    "class Adaptive_Novelty(Fast_Novelty):\n",
    "    \n",
    "    def __init__(self,size,alpha=0.95,gamma=1e-3):\n",
    "        super(Adaptive_Novelty,self).__init__(size,alpha)\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        \n",
    "    def addBasis(self,X):\n",
    "        X = X.data\n",
    "        #Slowly learn to remember this (linear) pattern\n",
    "        O2 = self.O.pow(2)\n",
    "        self.O -= self.alpha*O2*X.ger(X)*O2\n",
    "        #Very gradually forget everything we've learned\n",
    "        if self.gamma > 0.:\n",
    "            self.O += self.gamma * (self.O - O2)\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "O = torch.eye(3, requires_grad=False, dtype=dtype, device=device)\n",
    "a = 0.003\n",
    "g = 0.01\n",
    "X = torch.tensor([7,0.17,11], requires_grad=False, dtype=dtype, device=device).t()\n",
    "X2 = torch.tensor([7,-3,.1], requires_grad=False, dtype=dtype, device=device).t()\n",
    "O2 = np.square(O)\n",
    "print(a*(O2*X*X.t()*O2))\n",
    "print(a*(O2*(X*X.t())*O2))\n",
    "print(a*O2*X.ger(X)*O2)\n",
    "for i in range(1000):\n",
    "    n = O @ X\n",
    "    O2 = np.square(O)\n",
    "    O -= a*O2*X.ger(X)*O - g * (O - O2)\n",
    "    \n",
    "#     O = O - a*O2*X.ger(X)*O + g * (O - O2)\n",
    "print(X,X2)\n",
    "print(O @ X, O@X2)\n",
    "print(X - O@X, X2 - O@X2)\n",
    "\n",
    "print('---')\n",
    "# O = torch.eye(3, requires_grad=False, dtype=dtype, device=device)\n",
    "for i in range(1000):\n",
    "    n = O @ X2\n",
    "    O2 = np.square(O)\n",
    "    O -= a*O2*X2.ger(X2)*O - g * (O - O2)\n",
    "print(X,X2)\n",
    "print(O @ X, O@X2)\n",
    "print(X - O@X, X2 - O@X2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor([7,0.17,11], requires_grad=False, dtype=dtype, device=device).t()\n",
    "X2 = torch.tensor([7,-3,.1], requires_grad=False, dtype=dtype, device=device).t()\n",
    "filter = Adaptive_Novelty(3,alpha=0.01,gamma=5e-2)\n",
    "filter.addBasis(X)\n",
    "print(filter.novelty(X2),filter.novelty(filter.novelty(filter.novelty(X2))))\n",
    "filter.addBasis(X2)\n",
    "\n",
    "filter2 = Adaptive_Novelty(3,alpha=0.01,gamma=0.)\n",
    "print(filter2.novelty(X2))\n",
    "filter2.addBasis(X)\n",
    "print(filter2.novelty(X2))\n",
    "filter2.addBasis(filter2.novelty(X2))\n",
    "print(filter.O,filter2.O)\n",
    "print(filter.project(X2),filter2.project(X2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor([1,0,0], requires_grad=False, dtype=dtype, device=device).t()\n",
    "X2 = torch.tensor([0,1,1], requires_grad=False, dtype=dtype, device=device).t()\n",
    "O = torch.eye(3, requires_grad=False, dtype=torch.float)\n",
    "a = 7e-3\n",
    "print(O @ X,O@X2)\n",
    "d=O@X\n",
    "d2=O@X2\n",
    "for i in range(100000):\n",
    "    n = O @ X\n",
    "    O2 = O.pow(2)\n",
    "    O = O - a*O2*X.ger(X)*O2\n",
    "print(O @ X, O@X2)\n",
    "print(X - O@X, X2 - O@X2)\n",
    "print(O@X/d)\n",
    "print(O@X2/d2)\n",
    "# for i in range(10000):\n",
    "#     n = O @ X2\n",
    "#     O2 = O.pow(2)\n",
    "#     O = O - a*O2*X2.ger(X2)*O2\n",
    "# print(O @ X, O@X2)\n",
    "# print(X - O@X, X2 - O@X2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ortho = Orthogonalizer(3)\n",
    "print(ortho.novelty(X),ortho.novelty(X2))\n",
    "ortho.addBasis(X2)\n",
    "print(ortho.novelty(X),ortho.novelty(X2))\n",
    "print(ortho.project(X),ortho.project(X2))\n",
    "\n",
    "Y = torch.stack([X,X,X]).numpy()\n",
    "Y2 = torch.stack([X2,X2,X2]).numpy()\n",
    "print(Y,Y2)\n",
    "Q,r = np.linalg.qr(Y)\n",
    "print(Q@Q.T)\n",
    "print(ortho.O)\n",
    "print(Q @ Q.T @ Y)#Projection from Q\n",
    "print(Q @ Q.T @ Y2)#Projection from Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ortho = Forgetalizer(3,alpha=0.99)\n",
    "# ortho = Adaptive_Novelty(3)\n",
    "print(ortho.alpha,ortho.gamma)\n",
    "VX = torch.tensor([1,0.0,1], requires_grad=False, dtype=dtype, device=device).t()\n",
    "V1 = torch.tensor([0,1,0], requires_grad=False,, dtype=dtype, device=device).t()\n",
    "print(V1)\n",
    "print(ortho.O)\n",
    "print(ortho.novelty(V1))\n",
    "print(ortho.project(V1))\n",
    "ortho.addBasis(V1)\n",
    "print(ortho.novelty(V1))\n",
    "print(ortho.project(V1))\n",
    "print(ortho.O)\n",
    "\n",
    "\n",
    "for i in range(100):\n",
    "    ortho.addBasis(V1)\n",
    "print(ortho.novelty(V1))\n",
    "print(ortho.project(V1))\n",
    "for i in range(1000):\n",
    "    ortho.addBasis(VX)\n",
    "print(ortho.novelty(V1))\n",
    "print(ortho.project(V1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "V1 = torch.tensor([[1,0,0],[0,1,0]], requires_grad=False, dtype=dtype, device=device).t()\n",
    "ortho = Orthogonalizer(3)\n",
    "print(ortho.novelty(V1))\n",
    "for x in V1.t():\n",
    "    print(ortho.novelty(x.unsqueeze(0).t()))\n",
    "for x in ortho.novelty(V1).t():\n",
    "    print(x.unsqueeze(0).t())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "I = torch.eye(3, requires_grad=False, dtype=dtype, device=device)\n",
    "V1 = torch.tensor([1,0.5,0.5], requires_grad=False, dtype=dtype, device=device)\n",
    "W = torch.eye(3, requires_grad=True, dtype=dtype, device=device)\n",
    "print(V1)\n",
    "print(W)\n",
    "O = W @ V1\n",
    "print(O,O.norm())\n",
    "optimizer = optim.Adam({W}, lr=1e-4)\n",
    "    \n",
    "i = 0\n",
    "while True:\n",
    "    i += 1\n",
    "    optimizer.zero_grad()\n",
    "    O = W @ V1\n",
    "    loss = O.norm() \n",
    "    if i % 1000 == 0:\n",
    "        print(i,loss)\n",
    "    if(loss < 0.0012):\n",
    "        break\n",
    "    loss.backward()                                      \n",
    "    optimizer.step()\n",
    "print(i,W,O)\n",
    "V2 = torch.tensor([0,1,0], requires_grad=False, dtype=dtype, device=device)\n",
    "i = 0\n",
    "while True:\n",
    "    i += 1\n",
    "    optimizer.zero_grad()\n",
    "    O = W @ V2\n",
    "    loss = O.norm() \n",
    "    if i % 1000 == 0:\n",
    "        print(i,loss)\n",
    "    if(loss < 0.0012):\n",
    "        break\n",
    "    loss.backward()                                      \n",
    "    optimizer.step()\n",
    "print(W@V1,W@V2)\n",
    "print(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ortho = Orthogonalizer(3)\n",
    "V1 = torch.tensor([0.5,0,0.1], requires_grad=False, dtype=dtype, device=device).t()\n",
    "V2 = torch.tensor([1,0,1], requires_grad=False, dtype=dtype, device=device).t()\n",
    "V3 = torch.tensor([1,0.5,1], requires_grad=False, dtype=dtype, device=device).t()\n",
    "print(\"n v1\",ortho.novelty(V1))\n",
    "print(ortho.O)\n",
    "print(\"n v1\",ortho.novelty(V1))\n",
    "print(\"n v2\",ortho.novelty(V2))\n",
    "print(\"V1\",V1)\n",
    "print(\"add V1\")\n",
    "ortho.addBasis(V1)\n",
    "print(ortho.O)\n",
    "print(\"p v1\",ortho.project(V1))\n",
    "print(\"n v1\",ortho.novelty(V1))\n",
    "print(\"n v2\",ortho.novelty(V2))\n",
    "print(\"add V1\")\n",
    "ortho.addBasis(V1)\n",
    "print(ortho.novelty(V1))\n",
    "print(\"add V1\")\n",
    "ortho.addBasis(V1)\n",
    "print(ortho.novelty(V1))\n",
    "print(\"add V1\")\n",
    "ortho.addBasis(V1)\n",
    "print(ortho.novelty(V1))\n",
    "print(\"add V1\")\n",
    "ortho.addBasis(V1)\n",
    "print(ortho.novelty(V1))\n",
    "print(\"add V1\")\n",
    "ortho.addBasis(V1)\n",
    "print(\"O\",ortho.O)\n",
    "print(ortho.novelty(V1))\n",
    "print(ortho.novelty(V2))\n",
    "print(ortho.novelty(V3))\n",
    "print(ortho.project(V1))\n",
    "print(ortho.project(V2))\n",
    "print(ortho.project(V3))\n",
    "\n",
    "ortho.addBasis(V2)\n",
    "print(ortho.novelty(V1))\n",
    "print(ortho.novelty(V2))\n",
    "print(ortho.novelty(V3))\n",
    "print(ortho.project(V1))\n",
    "print(ortho.project(V2))\n",
    "print(ortho.project(V3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ortho.addBasis(V1)\n",
    "print(ortho.novelty(V1))\n",
    "print(ortho.novelty(V2))\n",
    "print(ortho.novelty(V3))\n",
    "ortho.addBasis(V2)\n",
    "print(ortho.novelty(V1))\n",
    "print(ortho.novelty(V2))\n",
    "print(ortho.novelty(V3))\n",
    "ortho.addBasis(V3)\n",
    "print(ortho.novelty(V1))\n",
    "print(ortho.novelty(V2))\n",
    "print(ortho.novelty(V3))\n",
    "print(ortho.O)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(V1,V1.t())\n",
    "print(ortho.O)\n",
    "print(ortho.project(V1))\n",
    "print(ortho.project(V2))\n",
    "ortho.addBasis(V1)\n",
    "print(ortho.O)\n",
    "print(ortho.project(V1))\n",
    "print(ortho.project(V2))\n",
    "ortho.addBasis(V2)\n",
    "print(ortho.O)\n",
    "print(ortho.project(V1))\n",
    "print(ortho.project(V2))\n",
    "ortho.addBasis(V2)\n",
    "print(ortho.O)\n",
    "print(ortho.project(V1))\n",
    "print(ortho.project(V2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ortho = Orthogonalizer(3)\n",
    "V1 = torch.tensor([[0.5,0.25,0.25]], requires_grad=False, dtype=dtype, device=device).t()\n",
    "print(\"V1.p\",ortho.project(V1))\n",
    "print(\"V1.n\",ortho.novelty(V1))\n",
    "print(ortho.O)\n",
    "ortho.addBasis(V1)\n",
    "print(ortho.O)\n",
    "print(\"V1.p\",ortho.project(V1))\n",
    "print(\"V1.n\",ortho.novelty(V1))\n",
    "ortho.addBasis(V1)\n",
    "print(ortho.O)\n",
    "print(\"V1.p\",ortho.project(V1))\n",
    "print(\"V1.n\",ortho.novelty(V1))\n",
    "ortho.addBasis(V1)\n",
    "print(ortho.O)\n",
    "print(\"V1.p\",ortho.project(V1))\n",
    "print(\"V1.n\",ortho.novelty(V1))\n",
    "ortho.addBasis(V1)\n",
    "print(ortho.O)\n",
    "print(\"V1.p\",ortho.project(V1))\n",
    "print(\"V1.n\",ortho.novelty(V1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V2 = torch.tensor([[1,3,4]], requires_grad=False, dtype=dtype, device=device).t()\n",
    "print(\"x\",ortho.novelty(V2))\n",
    "ortho.addBasis(V2)\n",
    "print(ortho.project(V2))\n",
    "ortho.addBasis(V2)\n",
    "print(ortho.project(V2))\n",
    "ortho.addBasis(V2)\n",
    "print(ortho.project(V2))\n",
    "ortho.addBasis(V2)\n",
    "print(ortho.project(V2))\n",
    "print(ortho.project(V1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### gram-schmidt orthogonalization. Add a new basis one column at a time.\n",
    "class Orthogonalizer:\n",
    "    def __init__(self,size,thresh=1e-10):\n",
    "        self.O = torch.eye(size, requires_grad=False, dtype=torch.float)\n",
    "        self.O2 = torch.eye(size, requires_grad=False, dtype=torch.float)\n",
    "        self.threshold = thresh\n",
    "        return\n",
    "    def novelty(self,X):\n",
    "        return torch.mm(self.O,X)\n",
    "    def project(self,X):\n",
    "        return X - self.novelty(X)\n",
    "    def addBasis(self,X):\n",
    "        for x in X.t():\n",
    "            a = self.novelty(x.unsqueeze(0).t())\n",
    "            if (torch.abs(a).max() > self.threshold):\n",
    "                self.O -= torch.div(torch.mm(a,a.t()),torch.norm(a).pow(2))\n",
    "        return\n",
    "    \n",
    "#gram-schmidt orthogonalization. Add a new basis one column at a time.\n",
    "class Orthogonalizer2:\n",
    "    def __init__(self,size,thresh=1e-10):\n",
    "        self.O = torch.eye(size, requires_grad=False, dtype=torch.float)\n",
    "        self.O2 = torch.eye(size, requires_grad=False, dtype=torch.float)\n",
    "        self.threshold = thresh\n",
    "        return\n",
    "    def novelty(self,X):\n",
    "        return torch.mm(self.O,X)\n",
    "    def project(self,X):\n",
    "        return torch.div(torch.mm(self.O,X),self.O)\n",
    "    def addBasis(self,X):\n",
    "        for x in X.t():\n",
    "            a = self.novelty(x.unsqueeze(0).t())\n",
    "            if (torch.abs(a).max() > self.threshold):\n",
    "                self.O -= torch.div(torch.mm(a.t(),a),torch.norm(a).pow(2))\n",
    "        return\n",
    "    \n",
    "class Forgetful_Orthogonalizer(Orthogonalizer):\n",
    "    def addBasis(self,X):\n",
    "        #Reduce magnitude of prior orthoganlizations \n",
    "        super(Forgetful_Orthogonalizer, self).addBasis(X)\n",
    "        self.O = torch.tanh(self.O - self.O2) + self.O2\n",
    "        return\n",
    "    \n",
    "def compare(V,ortho,fortho):\n",
    "    ortho.addBasis(V)\n",
    "    fortho.addBasis(V)\n",
    "    print('Value {}\\tNovelty: {}\\tF Novelty: {}\\tProjection: {}\\tF Projection {}'.format(\n",
    "        V, ortho.novelty(V),fortho.novelty(V),ortho.project(V),fortho.project(V)))\n",
    "def compare2(V,ortho):\n",
    "    ortho.addBasis(V)\n",
    "    Q,_ = np.linalg.qr(V)\n",
    "    print('\\tO {}\\t Q P{}\\tValue {}\\tNovelty: {}\\tProjection: {}'.format(\n",
    "        ortho.O,Q,V, ortho.novelty(V),ortho.project(V),fortho.project(V)))\n",
    "\n",
    "def make_householder(a):\n",
    "    v = a / (a[0] + np.copysign(np.linalg.norm(a), a[0]))\n",
    "    v[0] = 1\n",
    "    H = np.eye(a.shape[0])\n",
    "    H -= (2 / np.dot(v, v)) * np.dot(v[:, None], v[None, :])\n",
    "    return H\n",
    "def householder_v(a):\n",
    "    \"\"\"Use this version of householder to reproduce the output of np.linalg.qr \n",
    "    exactly (specifically, to match the sign convention it uses)\n",
    "\n",
    "    based on https://rosettacode.org/wiki/QR_decomposition#Python\n",
    "    \"\"\"\n",
    "    v = a / (a[0] + np.copysign(np.linalg.norm(a), a[0]))\n",
    "    v[0] = 1\n",
    "    tau = 2 / (v.T @ v)\n",
    "\n",
    "    return v,tau\n",
    "    \n",
    "ortho = Orthogonalizer(3)\n",
    "fortho = Forgetful_Orthogonalizer(3)\n",
    "V1 = torch.tensor([[0.5,0.25,0.25]], requires_grad=False, dtype=torch.float).t()\n",
    "print(\"x\")\n",
    "print(ortho.project(V1))\n",
    "print(ortho.novelty(V1))\n",
    "ortho.addBasis(V1)\n",
    "print(ortho.project(V1))\n",
    "ortho.addBasis(V1)\n",
    "print(ortho.project(V1))\n",
    "ortho.addBasis(V1)\n",
    "print(ortho.project(V1))\n",
    "ortho.addBasis(V1)\n",
    "print(ortho.project(V1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V1 = torch.tensor([[0.5,0.25,0.25]], requires_grad=False, dtype=torch.float).t()\n",
    "V2 = torch.tensor([[2,3,4]], requires_grad=False, dtype=torch.float).t()\n",
    "Q,r = np.linalg.qr(V1)\n",
    "print(\"Qr\",np.linalg.qr(V1))\n",
    "print(\"householder\",householder_v(V1.numpy()))\n",
    "# print(\"householder2\",make_householder(V1.numpy()))\n",
    "h,r2 = householder_v(V1.numpy())\n",
    "Q = torch.tensor(Q)\n",
    "print(\"Qt.Q\",Q @ Q.t())\n",
    "print(\"Q.Qt\",torch.mm(Q,Q.t()))\n",
    "print(\"Qt.Q\",torch.mm(Q.t(),Q))\n",
    "print(Q @ Q.t() @ V1)#Projection from Q\n",
    "print(h @ h.T @ V1.numpy())#Projection from h\n",
    "print(V1 - torch.mm(torch.mm(Q,Q.t()),V1))#Novelty from Q\n",
    "print(\"\")\n",
    "\n",
    "\n",
    "Q,r = np.linalg.qr(V2)\n",
    "Q = torch.tensor(Q)\n",
    "print(\"Q\",Q)\n",
    "print(\"Q.Qt\",torch.mm(Q,Q.t()))\n",
    "print(\"Qt.Q\",torch.mm(Q.t(),Q))\n",
    "print(torch.mm(torch.mm(Q,Q.t()),V2))#Projection from Q\n",
    "print(V2 - torch.mm(torch.mm(Q,Q.t()),V2))#Novelty from Q\n",
    "print(\"\")\n",
    "\n",
    "\n",
    "print(torch.mm(torch.mm(Q,Q.t()),V1))#Projection from Q\n",
    "print(V1 - torch.mm(torch.mm(Q,Q.t()),V1))#Novelty from Q\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B: tensor([[3., 1., 3., 3., 3., 3.],\n",
    "        [3., 3., 6., 4., 4., 4.],\n",
    "        [4., 3., 5., 5., 5., 5.]]), projection: tensor([[5.9071, 2.0330, 7.4282, 5.7807, 5.7807, 5.7807],\n",
    "        [4.1150, 2.9704, 7.2674, 4.8306, 4.8306, 4.8306],\n",
    "        [1.4824, 1.9224, 1.4342, 2.4399, 2.4399, 2.4399]])\n",
    "\n",
    "def gram_schmidt(A):\n",
    "    \"\"\"Orthogonalize a set of vectors stored as the columns of matrix A.\"\"\"\n",
    "    # Get the number of vectors.\n",
    "    n = A.shape[1]\n",
    "    for j in range(n):\n",
    "        # To orthogonalize the vector in column j with respect to the\n",
    "        # previous vectors, subtract from it its projection onto the\n",
    "        # each of the previous vectors.\n",
    "        for k in range(j):\n",
    "            A[:, j] -= np.dot(A[:, k], A[:, j]) * A[:, k]\n",
    "        A[:, j] = A[:, j] / np.linalg.norm(A[:, j])\n",
    "    return A\n",
    "A = np.array([[1.0, 1.0, 0.0], [1.0, 3.0, 1.0], [2.0, -1.0, 1.0]])\n",
    "# print(gram_schmidt(A))\n",
    "\n",
    "def gram_schmidt_columns(X):\n",
    "    Q, R = np.linalg.qr(X)\n",
    "    return Q,R\n",
    "Q,R = gram_schmidt_columns(V4)\n",
    "print(\"zz\",V4.numpy().dot(R.T))\n",
    "print(V4)\n",
    "print('sss',gram_schmidt_columns(V4))\n",
    "z = Orthogonalizer(3)\n",
    "z.addBasis(V4)\n",
    "print(Q.dot(Q.T).dot(V4))\n",
    "print(V4.numpy()-Q.dot(Q.T).dot(V4))\n",
    "print(z.novelty(V4))\n",
    "print(z.project(V4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-6 tensor([0.0025])\n",
      "-5 tensor([0.0067])\n",
      "-4 tensor([0.0183])\n",
      "-3 tensor([0.0498])\n",
      "-2 tensor([0.1353])\n",
      "-1 tensor([0.3679])\n",
      "0 tensor([1.])\n",
      "1 tensor([0.3679])\n",
      "2 tensor([0.1353])\n",
      "3 tensor([0.0498])\n",
      "4 tensor([0.0183])\n",
      "5 tensor([0.0067])\n"
     ]
    }
   ],
   "source": [
    "eps = 1e-12\n",
    "#            certainty_factor = self.alpha/torch.exp(abs(self.loss))\n",
    "for i in range(-6,6):\n",
    "    X = torch.tensor([i], requires_grad=False, dtype=dtype, device=device)\n",
    "    print(i,1./torch.exp(abs(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1/torch.exp(abs(torch.tensor([1e-13], requires_grad=False, dtype=dtype, device=device)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 7.4518e-01, -2.9972e-01,  1.5403e-01],\n",
      "        [ 7.7383e-01,  1.5247e-01, -2.9849e-01],\n",
      "        [ 1.3003e-01, -1.8735e-01,  7.7313e-01],\n",
      "        [ 1.0315e+00, -6.5747e-01, -7.7077e-01],\n",
      "        [ 4.6050e-01, -4.6900e-01,  8.4203e-01],\n",
      "        [ 1.7573e+00,  1.0854e+00, -4.3304e-01],\n",
      "        [-2.3718e-01,  7.7311e-01, -4.1140e-01],\n",
      "        [-6.9385e-01,  8.8059e-01,  9.6271e-01],\n",
      "        [-2.3377e-01,  5.3642e-01, -3.8494e-01],\n",
      "        [ 9.3499e-01,  5.7672e-01,  8.4462e-01],\n",
      "        [-8.7055e-01, -7.5306e-01,  4.6861e-02],\n",
      "        [ 3.2025e-02, -2.1910e-01, -4.0968e-01],\n",
      "        [ 8.6351e-01,  3.4322e-01,  1.5330e+00],\n",
      "        [ 7.5326e-02,  3.9386e-01,  5.3867e-02],\n",
      "        [-2.4905e-01, -7.0694e-01,  8.3978e-01],\n",
      "        [ 1.1292e+00,  1.7568e-01,  1.1436e+00],\n",
      "        [-1.7406e-01, -1.2942e-01,  1.3245e+00],\n",
      "        [-2.0527e-01, -3.2130e-02,  5.3246e-01],\n",
      "        [ 1.3162e-01, -9.5116e-01,  3.9188e-01],\n",
      "        [ 3.3760e-01, -1.4998e-01,  7.7361e-01],\n",
      "        [ 1.3474e-02, -9.6342e-01, -3.4190e-01],\n",
      "        [ 4.6454e-01, -5.6313e-01,  9.1108e-01],\n",
      "        [ 3.2289e-01, -8.9279e-01,  4.7884e-01],\n",
      "        [ 1.4292e-03,  5.8308e-02, -9.1642e-01],\n",
      "        [-1.0456e+00, -9.5090e-02,  1.4676e-01],\n",
      "        [-6.7021e-01,  1.8037e-01, -5.7696e-01],\n",
      "        [-2.7777e-02, -6.4598e-01,  9.6601e-01],\n",
      "        [-4.6303e-01, -1.9641e-01, -5.4163e-01],\n",
      "        [ 9.7650e-02,  3.6938e-02, -1.7381e-01],\n",
      "        [ 7.9088e-01,  1.0416e-01, -5.9667e-01],\n",
      "        [-2.5675e-01, -3.0934e-01,  3.5818e-01],\n",
      "        [-5.7043e-03,  7.0526e-01,  5.4669e-01],\n",
      "        [-6.0342e-01, -1.1305e-02, -1.0765e+00],\n",
      "        [-2.2076e-01,  1.3432e-01,  3.8487e-01],\n",
      "        [ 4.7717e-01, -1.7045e-01,  7.1444e-01],\n",
      "        [ 4.8952e-01,  6.6801e-01, -1.5354e-01],\n",
      "        [-1.4859e-01,  4.0094e-01, -4.9175e-01],\n",
      "        [ 8.5788e-01, -5.3455e-01, -9.2153e-01],\n",
      "        [-6.4044e-01, -5.8337e-01,  8.1824e-02],\n",
      "        [ 6.3623e-01,  7.2815e-01,  4.4859e-01],\n",
      "        [ 3.3791e-01, -1.0494e+00,  3.4921e-01],\n",
      "        [-8.3627e-01, -5.2165e-01,  2.3105e-01],\n",
      "        [-4.6322e-01, -4.9105e-01, -6.5673e-01],\n",
      "        [-5.9477e-01,  6.4692e-01, -7.0948e-03],\n",
      "        [ 3.3925e-01, -4.8098e-01,  2.6975e-02],\n",
      "        [ 3.6121e-01, -1.0960e+00,  4.5174e-01],\n",
      "        [-2.9563e-01, -6.5615e-01,  2.3262e-01],\n",
      "        [ 4.1256e-01, -1.9078e-01, -6.3906e-01],\n",
      "        [ 9.7981e-01, -4.6795e-01,  9.7783e-01],\n",
      "        [ 1.0705e-01, -2.3693e-01, -5.4623e-01],\n",
      "        [ 1.1511e+00, -5.1534e-01, -1.7446e-01],\n",
      "        [ 5.1056e-01, -7.9811e-01, -1.1179e+00],\n",
      "        [ 1.6598e-01,  8.6727e-02, -9.5515e-01],\n",
      "        [ 9.8557e-01,  6.4677e-01, -6.4223e-01],\n",
      "        [-5.8631e-01, -3.3657e-01,  6.1782e-01],\n",
      "        [ 3.9604e-01, -3.9972e-01, -5.9614e-01],\n",
      "        [ 2.1362e-01,  5.3346e-01, -5.7716e-01],\n",
      "        [ 4.4036e-01,  1.9125e-01,  6.8232e-01],\n",
      "        [-3.5848e-01, -1.6675e-01,  5.9867e-01],\n",
      "        [-1.3814e-02,  1.1476e+00, -5.4655e-01],\n",
      "        [ 7.4557e-01, -6.1758e-01, -2.3269e-01],\n",
      "        [-6.2784e-01,  9.0590e-01, -2.6345e-01],\n",
      "        [ 7.7524e-01, -4.1015e-01,  4.1247e-01],\n",
      "        [-9.9321e-04, -2.4302e-01, -2.5440e-01],\n",
      "        [-1.0134e-01,  2.1553e-01,  4.5379e-02],\n",
      "        [-1.3587e+00, -6.0030e-01, -1.4403e+00],\n",
      "        [ 1.4887e-01,  5.2224e-01, -9.5965e-01],\n",
      "        [-1.8017e-01,  4.2737e-02,  8.6899e-01],\n",
      "        [-5.8731e-01,  3.8096e-01,  8.4085e-01],\n",
      "        [-5.1405e-01,  3.5104e-03,  4.4030e-01],\n",
      "        [-1.8905e-01, -1.0459e+00, -5.4451e-01],\n",
      "        [-5.9625e-01,  7.1646e-01,  9.9115e-02],\n",
      "        [-1.5846e-01, -9.5658e-01, -8.9455e-01],\n",
      "        [ 4.7332e-01,  1.2954e-01,  1.0023e-01],\n",
      "        [ 3.3653e-01, -1.3643e-01, -2.5359e-01],\n",
      "        [-4.2444e-01,  2.3069e-01, -1.3810e+00],\n",
      "        [-7.7014e-03,  5.7288e-02,  1.1348e+00],\n",
      "        [ 5.4517e-01,  4.0857e-01, -8.9104e-01],\n",
      "        [-2.7185e-02, -6.5362e-01, -9.8661e-01],\n",
      "        [-5.1795e-01, -4.9792e-02, -7.4743e-01],\n",
      "        [-4.8355e-01, -6.5250e-01,  7.2322e-01],\n",
      "        [-2.4372e-01, -2.6148e-01,  1.0195e-01],\n",
      "        [ 7.4162e-03, -3.3200e-01,  9.8162e-01],\n",
      "        [ 4.1053e-01,  5.4510e-01, -5.9647e-01],\n",
      "        [ 2.0981e-01,  6.2110e-02,  7.3229e-01],\n",
      "        [ 3.7309e-01,  3.4893e-01, -3.6622e-01],\n",
      "        [ 8.1493e-01,  4.0293e-01,  5.0051e-01],\n",
      "        [-2.1867e-01, -4.2866e-01,  2.2261e-01],\n",
      "        [ 3.1534e-01,  9.7540e-01,  3.2071e-01],\n",
      "        [ 3.5333e-02, -1.0958e-01,  2.1060e-01],\n",
      "        [ 1.3227e-01, -7.9239e-01,  1.3048e+00],\n",
      "        [ 2.5431e-01,  6.2585e-01,  4.8403e-01],\n",
      "        [-4.1398e-01, -6.7662e-02,  6.5297e-01],\n",
      "        [-9.3782e-02, -4.7238e-01,  5.3632e-01],\n",
      "        [ 9.0403e-01, -6.3963e-02,  6.2852e-01],\n",
      "        [-1.4458e-01, -3.7743e-01, -2.9106e-01],\n",
      "        [-9.7587e-01,  1.6029e-01,  1.1280e+00],\n",
      "        [ 1.9814e-01, -4.5558e-01,  7.7222e-01],\n",
      "        [ 2.6792e-01,  1.4779e+00,  1.4328e-01],\n",
      "        [ 9.5318e-02, -9.3180e-01, -2.1328e-01]], requires_grad=True)\n",
      "tensor(0.2764)\n",
      "tensor(0.2764)\n",
      "tensor(0.)\n",
      "norms tensor(0.2764) tensor(2.7643e-06)\n",
      "suppressing\n",
      "tensor(8.8727e-08)\n",
      "tensor(0.2764)\n",
      "norms tensor(8.8727e-08) tensor(2.7643e-06)\n",
      "tensor(8.8727e-08)\n",
      "tensor(0.2764)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "xor = Stable_Xor(1.)\n",
    "print(xor.l1.weight)\n",
    "xor(data[0][0:3])\n",
    "xor.get_loss(data[0][3:4]).backward()\n",
    "print(xor.l1.weight.grad.view(xor.l1_length).norm())\n",
    "print(xor.l1_filter.novelty(xor.l1.weight.grad.view(xor.l1_length)).norm())\n",
    "print(xor.l1_filter.project(xor.l1.weight.grad.view(xor.l1_length)).norm())\n",
    "# print(xor.l1_filter.O)\n",
    "xor.l1_filter.addBasis(xor.l1.weight.grad.view(xor.l1_length),1.)\n",
    "# print(xor.l1_filter.O)\n",
    "print(xor.l1_filter.novelty(xor.l1.weight.grad.view(xor.l1_length)).norm())\n",
    "print(xor.l1_filter.project(xor.l1.weight.grad.view(xor.l1_length)).norm())\n",
    "xor.l1_filter.addBasis(xor.l1.weight.grad.view(xor.l1_length),1.)\n",
    "# print(xor.l1_filter.O)\n",
    "print(xor.l1_filter.novelty(xor.l1.weight.grad.view(xor.l1_length)).norm())\n",
    "print(xor.l1_filter.project(xor.l1.weight.grad.view(xor.l1_length)).norm())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
