{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "cuda not used\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "print(torch.cuda.is_available())\n",
    "if torch.cuda.is_available() and False:\n",
    "    print (\"cuda in use\")\n",
    "    device = torch.device('cuda') \n",
    "    torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "else:\n",
    "    print (\"cuda not used\")\n",
    "    device = torch.device('cpu')\n",
    "    torch.set_default_tensor_type('torch.FloatTensor')\n",
    "    dtype = torch.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='PyTorch actor-critic example')\n",
    "parser.add_argument('--gamma', type=float, default=0.99, metavar='G', help='discount factor (default: 0.99)')\n",
    "parser.add_argument('--seed', type=int, default=543, metavar='N', help='random seed (default: 543)')\n",
    "# parser.add_argument('--render', action='store_true', help='render the environment')\n",
    "parser.add_argument('--render', type=bool,default=False, help='render the environment')\n",
    "parser.add_argument('--trace', type=bool,default=False, help='render the environment')\n",
    "parser.add_argument('--log-interval', type=int, default=100, metavar='N', help='interval between training status logs (default: 10)')\n",
    "parser.add_argument('-f','--file',help='Path for input file. (Dummy arg to enable execution in notebook.)' )\n",
    "args = parser.parse_args() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "        \n",
    "#       Cart Pole state values\n",
    "#         Num\tObservation           Min         Max\n",
    "#         0\tCart Position             -4.8            4.8\n",
    "#         1\tCart Velocity             -Inf            Inf\n",
    "#         2\tPole Angle                -24 deg        24 deg\n",
    "#         3\tPole Velocity At Tip      -Inf            Inf\n",
    "\n",
    "        \n",
    "class World():\n",
    "    \n",
    "    def __init__(self,hidden_nodes=12):\n",
    "        self.env = gym.make('CartPole-v0')\n",
    "#         self.env = gym.make('CartPole-v1')\n",
    "#         self.env = gym.make('Acrobot-v1')\n",
    "        self.zeros = torch.zeros([self.dimension_count()], requires_grad=False, dtype=dtype,device=device)\n",
    "        self.loss_function = torch.nn.L1Loss(reduction='sum')#Mean Absolute Error\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.done = False\n",
    "        self.state = torch.tensor(self.env.reset(), requires_grad=False, dtype=dtype, device=device)\n",
    "    \n",
    "    def dimension_count(self):\n",
    "        return self.env.observation_space.shape[0]\n",
    "        \n",
    "    def action_count(self):\n",
    "        return self.env.action_space.n\n",
    "    \n",
    "    def step(self,action):\n",
    "        self.prior_state = self.state\n",
    "        self.state, self.reward, self.done, _ = self.env.step(action)\n",
    "        self.state = torch.tensor(self.state, requires_grad=False, dtype=dtype, device=device)\n",
    "        if args.render: self.env.render()\n",
    "    \n",
    "    def value(self):\n",
    "        return 1./torch.log(1.01 + self.loss_function(self.zeros,self.state))\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    \n",
    "    def __init__(self,world: World,hidden_nodes=32):\n",
    "        super(Critic, self).__init__()\n",
    "        self.world = world\n",
    "        self.one = torch.ones([1], requires_grad=False, dtype=dtype, device=device)\n",
    "        self.zero = torch.zeros([1], requires_grad=False, dtype=dtype, device=device)\n",
    "        self.l1 = nn.Linear(world.dimension_count(),hidden_nodes)\n",
    "        self.l1.weight.data.normal_(0.0, np.sqrt(1./(world.dimension_count())))\n",
    "        self.head = nn.Linear(hidden_nodes, 1)\n",
    "        self.head.weight.data.normal_(0.0, np.sqrt(1./(hidden_nodes)))\n",
    "        self.prev_value = self.zero\n",
    "        self.value = self.zero\n",
    "        self.loss_function = torch.nn.L1Loss(reduction=\"mean\")\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=3e-3,weight_decay=0.0001)#lr=4e-5,weight_decay=0.00001)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        self.prev_value = self.value\n",
    "        self.l1_out = F.selu(self.l1(state))\n",
    "        self.value = self.head(self.l1_out)\n",
    "        return self.value\n",
    "    \n",
    "    def step(self):\n",
    "        self.optimizer.step()\n",
    "        \n",
    "    \n",
    "    #What the previous value should have been knowing what we know after the last state transition\n",
    "    def hindsight_value(self):\n",
    "        #Do not include gradient of the critic value here, just the data.\n",
    "        v = self.world.value() # or world_actor.value if dreaming.... come back to this.\n",
    "        return v * self.zero if self.world_actor.world.done else v + args.gamma * self.value.data\n",
    "         \n",
    "    #Temporal Difference Loss is for the previous state!\n",
    "    def get_loss(self):\n",
    "        self.loss = self.loss_function(self.prev_value,self.hindsight_value())\n",
    "        if args.trace: print(\"Critic value and loss:\",self.prev_value,self.loss)\n",
    "        return self.loss\n",
    "\n",
    "class World_Actor(nn.Module):\n",
    "    \n",
    "    def __init__(self,world):\n",
    "        super(World_Actor, self).__init__()\n",
    "        self.world = world\n",
    "        input_size = world.dimension_count()\n",
    "        self.zeros = torch.zeros([input_size], requires_grad=False, dtype=dtype,device=device)\n",
    "        self.actor_training = True\n",
    "        self.world_training = True\n",
    "        \n",
    "        hidden_action_nodes = 24\n",
    "        self.actor1 = nn.Linear(input_size,hidden_action_nodes)\n",
    "        self.actor1.weight.data.normal_(0.0, np.sqrt(1./input_size))\n",
    "        self.actor_optimizer = optim.Adam(self.actor1.parameters(), lr=3e-2,weight_decay=0.0001)#lr=4e-5,weight_decay=0.00001)\n",
    "        \n",
    "        self.actor2 = nn.Linear(hidden_action_nodes,hidden_action_nodes)\n",
    "        self.actor2.weight.data.normal_(0.0, np.sqrt(1./hidden_action_nodes))\n",
    "        self.actor_optimizer.add_param_group({'params': self.actor2.weight})\n",
    "        \n",
    "        self.actor_final = nn.Linear(hidden_action_nodes, world.action_count())\n",
    "        self.actor_final.weight.data.normal_(0.0, np.sqrt(1./(hidden_action_nodes)))\n",
    "        self.actor_optimizer.add_param_group({'params': self.actor_final.weight})\n",
    "        \n",
    "        hidden_world_nodes = 12\n",
    "        parms = {}\n",
    "        input_size = world.dimension_count()+world.action_count()\n",
    "        \n",
    "        self.world1 = nn.Linear(input_size,hidden_world_nodes)\n",
    "        self.world1.weight.data.normal_(0.0, np.sqrt(1./input_size))\n",
    "        self.world_optimizer = optim.Adam(self.world1.parameters(), lr=3e-4,weight_decay=0.0001)#lr=4e-5,weight_decay=0.00001)\n",
    "        \n",
    "        self.world2 = nn.Linear(hidden_world_nodes,hidden_world_nodes)\n",
    "        self.world2.weight.data.normal_(0.0, np.sqrt(1./hidden_world_nodes))\n",
    "        self.world_optimizer.add_param_group({'params': self.world2.weight})\n",
    "        \n",
    "        self.world_final = nn.Linear(hidden_world_nodes, world.dimension_count())\n",
    "        self.world_final.weight.data.normal_(0.0, np.sqrt(1./hidden_world_nodes))\n",
    "        self.world_optimizer.add_param_group({'params': self.world_final.weight})\n",
    "        \n",
    "        self.loss_function = nn.MSELoss()\n",
    "        self.mean_loss = 100.0\n",
    "        \n",
    "    def call_actor(self,state):\n",
    "        self.actor1_out = F.selu(self.actor1(state))\n",
    "        self.actor2_out = F.selu(self.actor2(self.actor1_out))\n",
    "        self.actor_value = F.softmax(F.selu(self.actor_final(self.actor2_out)),dim=0)\n",
    "        self.categories = Categorical(self.actor_value)\n",
    "        self.action = self.categories.sample()\n",
    "        self.policy_action = self.actor_value.argmax();\n",
    "        if args.trace: print(\"Action Scores:\",self.categories.probs,\"Selected Action:\",self.action.item(),\"Policy Action:\", self.policy_action)\n",
    "        \n",
    "    def random_action(self, state):\n",
    "        self.action = torch.tensor([random.randint(0,self.world.action_count()-1)], requires_grad=False, dtype=torch.int, device=device)\n",
    "        self.actor_value = torch.zeros([self.world.action_count()], requires_grad=False, dtype=dtype, device=device)\n",
    "        self.actor_value[self.action.item()] = 1.\n",
    "        self.categories = Categorical(self.actor_value)\n",
    "            \n",
    "    def call_world(self, state):\n",
    "        state_action = torch.cat([state, self.actor_value], dim=0)\n",
    "        self.world1_out = F.selu(self.world1(state_action))\n",
    "        self.world2_out = F.selu(self.world2(self.world1_out))\n",
    "        self.world_value = self.world_final(self.world2_out)\n",
    "        if args.trace: print(\"world value:\",self.world_value)\n",
    "    \n",
    "    def forward(self,state):\n",
    "        self.prior_state = state\n",
    "        if self.actor_training:\n",
    "            self.call_actor(state)\n",
    "        else:\n",
    "            self.random_action(state)\n",
    "        self.call_world(state)\n",
    "        return self.world_value\n",
    "    \n",
    "    def value(self):\n",
    "        return self.loss_function(self.zeros,self.world_value)\n",
    "    \n",
    "    def get_actor_loss(self):\n",
    "        #target of zeros is the perfect pole balance at the center.\n",
    "        loss = self.critic.get_loss()\n",
    "#         loss = self.loss_function(self.zeros,self.world_value)\n",
    "        return loss\n",
    "    \n",
    "    def get_world_loss(self,target):\n",
    "        loss = self.loss_function(target,self.world_value)\n",
    "        self.mean_loss = 0.95 * self.mean_loss + 0.05 * loss\n",
    "        return loss\n",
    "    \n",
    "    def update_actor(self):\n",
    "        loss = self.get_actor_loss()\n",
    "        loss.backward(retain_graph=True)\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "    def update_world(self,target):\n",
    "        loss = self.get_world_loss(target)\n",
    "        loss.backward(retain_graph=True)\n",
    "        self.world_optimizer.step()\n",
    "        \n",
    "    def update_critic():\n",
    "        loss = self.critic.get_loss(target)\n",
    "        loss.backward(retain_graph=True)\n",
    "        self.critic.optimizer.step()\n",
    "        \n",
    "    def calibrate(self,target):\n",
    "        if self.actor_training:\n",
    "            self.update_actor()\n",
    "            self.actor_optimizer.zero_grad()\n",
    "        if self.world_training:\n",
    "            self.update_world(target)\n",
    "            self.world_optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.trace = True\n",
    "world = World()\n",
    "state = torch.tensor(world.env.reset(),requires_grad=False, dtype=dtype, device=device)\n",
    "world_actor = World_Actor(world)\n",
    "critic = Critic(world)\n",
    "critic.world_actor = world_actor\n",
    "world_actor.critic = critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "tensor([-0.5053, -0.4807, -0.9853, -0.5358], grad_fn=<SelectBackward>)\n",
      "None\n",
      "tensor([ 0.0974, -0.3094,  0.3273, -0.3574,  0.3676, -0.3686],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Action Scores: tensor([0.2582, 0.7418], grad_fn=<DivBackward0>) Selected Action: 1 Policy Action: tensor(1)\n",
      "world value: tensor([-0.1903, -0.1714, -0.5341,  1.2331], grad_fn=<AddBackward0>)\n",
      "loss tensor(0.4410, grad_fn=<MeanBackward0>)\n",
      "Critic value and loss: tensor([0.6205], grad_fn=<AddBackward0>) tensor(2.2481, grad_fn=<L1LossBackward>)\n",
      "loss tensor(2.2481, grad_fn=<L1LossBackward>)\n",
      "tensor([[ 4.3868e-04, -5.5687e-04, -1.1274e-04,  5.3265e-04],\n",
      "        [-3.3182e-04,  4.2122e-04,  8.5280e-05, -4.0290e-04],\n",
      "        [ 1.7126e-04, -2.1740e-04, -4.4016e-05,  2.0795e-04],\n",
      "        [-1.4613e-05,  1.8551e-05,  3.7558e-06, -1.7744e-05],\n",
      "        [-6.1862e-04,  7.8529e-04,  1.5899e-04, -7.5114e-04],\n",
      "        [ 2.8455e-04, -3.6121e-04, -7.3131e-05,  3.4550e-04],\n",
      "        [-2.3128e-04,  2.9359e-04,  5.9440e-05, -2.8082e-04],\n",
      "        [ 1.4151e-04, -1.7963e-04, -3.6368e-05,  1.7182e-04],\n",
      "        [ 1.8438e-04, -2.3406e-04, -4.7388e-05,  2.2388e-04],\n",
      "        [-1.6548e-04,  2.1007e-04,  4.2531e-05, -2.0093e-04],\n",
      "        [ 2.4369e-04, -3.0935e-04, -6.2631e-05,  2.9590e-04],\n",
      "        [-7.1068e-05,  9.0215e-05,  1.8265e-05, -8.6292e-05],\n",
      "        [-1.6212e-04,  2.0580e-04,  4.1665e-05, -1.9685e-04],\n",
      "        [ 2.3325e-04, -2.9609e-04, -5.9946e-05,  2.8321e-04],\n",
      "        [-3.1027e-04,  3.9386e-04,  7.9741e-05, -3.7673e-04],\n",
      "        [ 3.8791e-04, -4.9243e-04, -9.9697e-05,  4.7101e-04],\n",
      "        [ 3.4393e-04, -4.3660e-04, -8.8393e-05,  4.1761e-04],\n",
      "        [-1.9453e-04,  2.4694e-04,  4.9996e-05, -2.3621e-04],\n",
      "        [-3.5945e-04,  4.5629e-04,  9.2381e-05, -4.3645e-04],\n",
      "        [-3.9721e-04,  5.0423e-04,  1.0209e-04, -4.8230e-04],\n",
      "        [-5.8753e-04,  7.4583e-04,  1.5100e-04, -7.1339e-04],\n",
      "        [ 5.3948e-05, -6.8483e-05, -1.3865e-05,  6.5504e-05],\n",
      "        [-4.4015e-04,  5.5874e-04,  1.1312e-04, -5.3444e-04],\n",
      "        [-3.3628e-04,  4.2689e-04,  8.6427e-05, -4.0832e-04]])\n",
      "tensor([-0.5053, -0.4807, -0.9853, -0.5358], grad_fn=<SelectBackward>)\n",
      "tensor([-0.0003,  0.0004,  0.0001, -0.0005, -0.0027, -0.0078])\n",
      "tensor([ 0.0977, -0.3097,  0.3270, -0.3571,  0.3679, -0.3683],\n",
      "       grad_fn=<SelectBackward>)\n"
     ]
    }
   ],
   "source": [
    "world_actor.pretraining = True\n",
    "print(world_actor.actor1.weight.grad)\n",
    "print(world_actor.actor1.weight[0])\n",
    "print(world_actor.world1.weight.grad)\n",
    "print(world_actor.world1.weight[0])\n",
    "world_actor.forward(state)\n",
    "critic.forward(state)     \n",
    "world.step(world_actor.action.item())\n",
    "critic.forward(world.state)\n",
    "loss = world_actor.get_world_loss(state)\n",
    "print(\"loss\",loss)\n",
    "loss.backward(retain_graph=True)\n",
    "world_actor.world_optimizer.step()\n",
    "loss = critic.get_loss()\n",
    "print(\"loss\",loss)\n",
    "loss.backward(retain_graph=True)\n",
    "critic.step()\n",
    "print(world_actor.actor1.weight.grad)\n",
    "print(world_actor.actor1.weight[0])\n",
    "print(world_actor.world1.weight.grad[0])\n",
    "print(world_actor.world1.weight[0])\n",
    "\n",
    "world_actor.actor_optimizer.zero_grad()\n",
    "world_actor.world_optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(episodes=1000):\n",
    "    mave_reward = 10.\n",
    "    mave_value = 100.\n",
    "    action_preferences = np.array([0.5,0.5])\n",
    "    \n",
    "    for i_episode in range(1,episodes+1):\n",
    "        ep_reward = 0.\n",
    "        ep_value = 0.\n",
    "        ep_action_preferences = np.array([0.,0.])\n",
    "        world.reset() \n",
    "        critic(world.state)\n",
    "        I = 1.\n",
    "        for moves in range(10000):\n",
    "            \n",
    "            world_actor.forward(world.state)\n",
    "            ep_action_preferences += world_actor.categories.probs.detach().cpu().numpy()\n",
    "            \n",
    "            world.step(world_actor.action.item())\n",
    "            ep_reward += world.reward\n",
    "            \n",
    "            critic(world.state)\n",
    "            world_actor.calibrate(world.state)\n",
    "            \n",
    "            ep_value += critic.value.item()\n",
    "            loss = critic.get_loss()\n",
    "#             print(\"loss\",loss)\n",
    "            loss.backward(retain_graph=True)\n",
    "            critic.step()\n",
    "            \n",
    "            if(world.done):\n",
    "                if args.trace: print(\"DONE\")\n",
    "                break\n",
    "\n",
    "        ep_action_preferences /= moves\n",
    "        action_preferences =  0.05 * ep_action_preferences + (1 - 0.05) * action_preferences\n",
    "        mave_reward = 0.05 * ep_reward + (1 - 0.05) * mave_reward\n",
    "        mave_value = 0.05 * ep_value + (1 - 0.05) * mave_value\n",
    "        if i_episode % args.log_interval == 0:\n",
    "            print('Episode {}\\tLast reward: {:.2f}\\tMoving average reward: {:.2f}\\tAction Preferences: {:.2f},{:.2f}\\tMoving average model loss: {:.5f}\\tCritic value: {:.2f}'.format(\n",
    "                  i_episode, ep_reward, mave_reward, action_preferences[0],action_preferences[1],world_actor.mean_loss,mave_value))\n",
    "        if mave_reward > world.env.spec.reward_threshold:\n",
    "            print(\"Episode {}\\tSolved! Running reward is now {} and \"\n",
    "                  \"the last episode runs to {} time steps!\".format(i_episode,mave_reward, moves))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_trainer():\n",
    "    args.trace = False\n",
    "    args.render = False\n",
    "    global world\n",
    "    global critic\n",
    "    global world_actor\n",
    "    global actor_optimizer\n",
    "    global world_optimizer\n",
    "    world = World()\n",
    "    critic = Critic(world)\n",
    "    world_actor = World_Actor(world)\n",
    "    \n",
    "    \n",
    "    critic = Critic(world)\n",
    "    critic.world_actor = world_actor\n",
    "    world_actor.critic = critic\n",
    "    world_actor.pretraining = False\n",
    "    world.env.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tLast reward: 22.00\tMoving average reward: 24.26\tAction Preferences: 0.54,0.51\tMoving average model loss: 0.00\tCritic value: 9088.22\n",
      "Episode 200\tLast reward: 21.00\tMoving average reward: 21.74\tAction Preferences: 0.50,0.56\tMoving average model loss: 0.00\tCritic value: -15389.52\n",
      "Episode 300\tLast reward: 12.00\tMoving average reward: 20.83\tAction Preferences: 0.52,0.54\tMoving average model loss: 0.00\tCritic value: -33228.85\n",
      "Episode 400\tLast reward: 12.00\tMoving average reward: 22.16\tAction Preferences: 0.57,0.49\tMoving average model loss: 0.00\tCritic value: -34270.17\n",
      "Episode 500\tLast reward: 28.00\tMoving average reward: 22.60\tAction Preferences: 0.54,0.51\tMoving average model loss: 0.00\tCritic value: 56159.84\n",
      "Episode 600\tLast reward: 12.00\tMoving average reward: 21.41\tAction Preferences: 0.53,0.53\tMoving average model loss: 0.00\tCritic value: -104808.99\n",
      "Episode 700\tLast reward: 33.00\tMoving average reward: 23.75\tAction Preferences: 0.53,0.52\tMoving average model loss: 0.00\tCritic value: 19201.08\n",
      "Episode 800\tLast reward: 14.00\tMoving average reward: 19.52\tAction Preferences: 0.52,0.55\tMoving average model loss: 0.00\tCritic value: 27304.17\n",
      "Episode 900\tLast reward: 12.00\tMoving average reward: 22.36\tAction Preferences: 0.56,0.50\tMoving average model loss: 0.00\tCritic value: 251377.34\n",
      "Episode 1000\tLast reward: 24.00\tMoving average reward: 22.56\tAction Preferences: 0.51,0.55\tMoving average model loss: 0.00\tCritic value: 43629.17\n",
      "Episode 1100\tLast reward: 25.00\tMoving average reward: 24.96\tAction Preferences: 0.53,0.53\tMoving average model loss: 0.00\tCritic value: 17633.74\n",
      "Episode 1200\tLast reward: 14.00\tMoving average reward: 20.51\tAction Preferences: 0.57,0.49\tMoving average model loss: 0.00\tCritic value: 14656.98\n",
      "Episode 1300\tLast reward: 14.00\tMoving average reward: 23.61\tAction Preferences: 0.52,0.54\tMoving average model loss: 0.00\tCritic value: -6840.04\n",
      "Episode 1400\tLast reward: 18.00\tMoving average reward: 20.14\tAction Preferences: 0.55,0.51\tMoving average model loss: 0.00\tCritic value: 36175.69\n",
      "Episode 1500\tLast reward: 28.00\tMoving average reward: 22.68\tAction Preferences: 0.54,0.51\tMoving average model loss: 0.00\tCritic value: -4017.80\n",
      "Episode 1600\tLast reward: 16.00\tMoving average reward: 24.78\tAction Preferences: 0.54,0.51\tMoving average model loss: 0.00\tCritic value: 7327.14\n",
      "Episode 1700\tLast reward: 17.00\tMoving average reward: 23.90\tAction Preferences: 0.51,0.54\tMoving average model loss: 0.00\tCritic value: -904.90\n",
      "Episode 1800\tLast reward: 15.00\tMoving average reward: 24.89\tAction Preferences: 0.52,0.53\tMoving average model loss: 0.00\tCritic value: 81481.88\n",
      "Episode 1900\tLast reward: 13.00\tMoving average reward: 22.12\tAction Preferences: 0.55,0.51\tMoving average model loss: 0.00\tCritic value: 91157.97\n",
      "Episode 2000\tLast reward: 17.00\tMoving average reward: 21.59\tAction Preferences: 0.50,0.56\tMoving average model loss: 0.00\tCritic value: -14475.81\n",
      "Episode 100\tLast reward: 20.00\tMoving average reward: 26.20\tAction Preferences: 0.48,0.57\tMoving average model loss: 0.00\tCritic value: 12605.20\n",
      "Episode 200\tLast reward: 14.00\tMoving average reward: 27.42\tAction Preferences: 0.48,0.57\tMoving average model loss: 0.00\tCritic value: -11227.54\n",
      "Episode 300\tLast reward: 30.00\tMoving average reward: 26.96\tAction Preferences: 0.48,0.57\tMoving average model loss: 0.00\tCritic value: 9918.63\n",
      "Episode 400\tLast reward: 17.00\tMoving average reward: 22.89\tAction Preferences: 0.49,0.57\tMoving average model loss: 0.00\tCritic value: -7508.12\n",
      "Episode 500\tLast reward: 31.00\tMoving average reward: 21.04\tAction Preferences: 0.49,0.57\tMoving average model loss: 0.00\tCritic value: -8104.69\n"
     ]
    }
   ],
   "source": [
    "reset_trainer()\n",
    "\n",
    "args.trace = False\n",
    "world_actor.actor_training = False\n",
    "world_actor.world_training = True\n",
    "train(2000)\n",
    "world_actor.actor_training = True\n",
    "world_actor.world_training = False\n",
    "train(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.render = True\n",
    "train(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "world.reset()\n",
    "for i in range(3):\n",
    "    args.trace = True\n",
    "    actor.forward(world.prior_state)\n",
    "    world.step(actor)\n",
    "    print(actor.categories.log_prob(actor.action))\n",
    "\n",
    "    world.model_optimizer.zero_grad()\n",
    "    world.input_action = actor.value.clone().detach().requires_grad_(True)\n",
    "    world.model(torch.cat([world.prior_state, world.input_action], dim=0))\n",
    "    print(\"World input state:\",world.prior_state)\n",
    "    print(\"World output state:\",world.state)\n",
    "    print(\"model output state:\",world.model.value)\n",
    "    loss = world.model.get_loss(world.state)\n",
    "    loss.backward(retain_graph=True)\n",
    "    print(\"model loss\",loss)\n",
    "    # world.model_optimizer.step()    \n",
    "\n",
    "    actor_optimizer.zero_grad()\n",
    "    loss = world.model.get_actor_loss()\n",
    "    loss.backward(retain_graph=True)\n",
    "    print(\"get actor loss from world\",loss)\n",
    "    print(\"world input action grad\",world.input_action.grad)\n",
    "\n",
    "    loss = actor.get_loss()\n",
    "    print(\"actual actor loss\",loss)\n",
    "    loss.backward()\n",
    "    print(\"\")\n",
    "    actor_optimizer.step()\n",
    "    actor_optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action scores: tensor([0.1061, 0.8939], grad_fn=<DivBackward0>) Action: 0\n",
    "tensor(-2.2434, grad_fn=<SqueezeBackward1>)\n",
    "model output state: tensor([-0.1267, -0.8792,  0.1683,  1.5418], grad_fn=<AddBackward0>)\n",
    "get actor loss from world tensor(0.7986, grad_fn=<MeanBackward0>)\n",
    "world input action grad tensor([-0.4064])\n",
    "actual actor loss tensor([-0.9117], grad_fn=<MulBackward0>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.trace = False\n",
    "train(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "world.reset()\n",
    "actor.forward(world.model.value)\n",
    "print(\"forward\")\n",
    "world.step(actor.choose_action())\n",
    "print(actor.l1.weight[0])\n",
    "print(\"step\")\n",
    "actor_optimizer.zero_grad()\n",
    "print(\"zero grad\")\n",
    "loss = world.model.get_actor_loss()\n",
    "print(loss)\n",
    "print(\"actor loss\")\n",
    "loss.backward()    \n",
    "# actor_optimizer.step()\n",
    "# actor.forward(world.model.value)\n",
    "# world.step(actor.choose_action())\n",
    "# actor_optimizer.zero_grad()\n",
    "# loss = world.model.get_actor_loss()\n",
    "# loss.backward()    |\n",
    "# actor_optimizer.step()\n",
    "print(world.input_action.grad)\n",
    "loss = actor.get_loss()\n",
    "print(loss)\n",
    "loss.backward()\n",
    "actor_optimizer.step()\n",
    "# print(world.model.l1.weight.grad)\n",
    "print(actor.l1.weight[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "args.trace = False\n",
    "args.render = False\n",
    "args.log_interval = 100\n",
    "for i in range(1):\n",
    "    print(\"New training test\")\n",
    "#     reset_trainer()\n",
    "#     prime_model(3000)\n",
    "#     prime_actor(1)\n",
    "    train(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.log_interval = 10\n",
    "args.trace = True\n",
    "args.render = False\n",
    "train(10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.7000, 0.8000])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(1.1711)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = torch.tensor([0.5,0.6,0.7,0.8])\n",
    "print(v[2:4])\n",
    "x = Categorical(v)\n",
    "x.entropy().sqrt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "world.input_action.grad.detach().requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor.categories.log_prob(actor.action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor.categories.log_prob(actor.action) * world.input_action.grad.detach().requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
