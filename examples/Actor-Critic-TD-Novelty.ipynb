{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "import gc\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "if torch.cuda.is_available() and False:\n",
    "    print (\"cuda in use\")\n",
    "    device = torch.device('cuda') \n",
    "    torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "    dtype = torch.float32\n",
    "else:\n",
    "    print (\"cuda not used\")\n",
    "    device = torch.device('cpu')\n",
    "    torch.set_default_tensor_type('torch.FloatTensor')\n",
    "    dtype = torch.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='PyTorch actor-critic example')\n",
    "parser.add_argument('--gamma', type=float, default=0.99, metavar='G', help='discount factor (default: 0.99)')\n",
    "parser.add_argument('--seed', type=int, default=543, metavar='N', help='random seed (default: 543)')\n",
    "# parser.add_argument('--render', action='store_true', help='render the environment')\n",
    "parser.add_argument('--render', type=bool,default=False, help='render the environment')\n",
    "parser.add_argument('--trace', type=bool,default=False, help='render the environment')\n",
    "parser.add_argument('--log-interval', type=int, default=100, metavar='N', help='interval between training status logs (default: 10)')\n",
    "parser.add_argument('-f','--file',help='Path for input file. (Dummy arg to enable execution in notebook.)' )\n",
    "args = parser.parse_args() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from T. Kohonen '84, p. 183, equation 6.33\n",
    "\n",
    "#Set alpha to one for one-step orthogonalization.\n",
    "class Novelty_Filter():\n",
    "\n",
    "    def __init__(self,size,epsilon=1e-3):\n",
    "        self.O = torch.eye(size, requires_grad=False, dtype=dtype, device=device)\n",
    "        self.epsilon = epsilon\n",
    "        return\n",
    "\n",
    "    def addBasis(self,X,cf=1.):\n",
    "        l = self.novelty(X)\n",
    "        n = l.norm()\n",
    "        if(cf * n > self.epsilon * X.norm()):\n",
    "            self.O -= cf * l.ger(l) / n.pow(2)\n",
    "        return\n",
    "\n",
    "    def novelty(self,X):\n",
    "        return self.O @ X\n",
    "\n",
    "    def project(self,X):\n",
    "        return X - self.novelty(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def selu(x):\n",
    "    alpha = 1.6732632423543772848170429916717\n",
    "    scale = 1.0507009873554804934193349852946\n",
    "    return scale * F.elu(x, alpha)\n",
    "\n",
    "class World():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.env = gym.make('CartPole-v0')\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.reward = 0.0\n",
    "        self.done = False\n",
    "        self.state = torch.tensor(self.env.reset(), requires_grad=False, dtype=dtype, device=device)\n",
    "        \n",
    "    def action_count(self):\n",
    "        return self.env.action_space.n\n",
    "    \n",
    "    def dimension_count(self):\n",
    "        return self.env.observation_space.shape[0]\n",
    "    \n",
    "    def step(self,action):\n",
    "        self.state, self.reward, self.done, _ = self.env.step(action.item())\n",
    "        self.state = torch.tensor(self.state, requires_grad=False, dtype=dtype, device=device)\n",
    "        if args.render: self.env.render()\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    \n",
    "    def __init__(self,world: World,hidden_nodes=32):\n",
    "        super(Critic, self).__init__()\n",
    "        self.world = world\n",
    "        self.one = torch.ones([1], requires_grad=False, dtype=dtype, device=device)\n",
    "        self.zero = torch.zeros([1], requires_grad=False, dtype=dtype, device=device)\n",
    "        self.l1 = nn.Linear(world.dimension_count(),hidden_nodes)\n",
    "        self.l1.weight.data.normal_(0.0, np.sqrt(1./(self.world.dimension_count())))\n",
    "        self.head = nn.Linear(hidden_nodes, 1)\n",
    "        self.l1.weight.data.normal_(0.0, np.sqrt(1./hidden_nodes))\n",
    "        self.prev_value = self.zero\n",
    "        self.value = self.zero\n",
    "        \n",
    "    def forward(self, state):\n",
    "        self.prev_value = self.value\n",
    "        self.inputs = state\n",
    "        self.l1_out = selu(self.l1(state))\n",
    "        self.value = self.head(self.l1_out)\n",
    "        return self.value\n",
    "    \n",
    "    #What the previous value should have been knowing what we know after the last state transition\n",
    "    def hindsight_value(self):\n",
    "        #Do not include gradient of the critic value here, just the data.\n",
    "        return self.world.reward * self.zero if self.world.done else self.world.reward + args.gamma * self.value.data\n",
    "         \n",
    "    #Temporal Difference Loss is for the previous state!\n",
    "    def get_loss(self):\n",
    "        self.loss = F.mse_loss(self.prev_value,self.hindsight_value())\n",
    "        if args.trace: print(\"Critic value and loss:\",self.prev_value,self.loss)\n",
    "        return self.loss\n",
    "    \n",
    "    def do_post_gradient(self):#NONE on main class\n",
    "        pass\n",
    "    \n",
    "    def do_pre_update(self):\n",
    "        pass\n",
    "    \n",
    "    def gc(self):\n",
    "        del self.loss\n",
    "    \n",
    "class Actor(nn.Module):\n",
    "    \n",
    "    def __init__(self, critic: Critic,hidden_nodes=64):\n",
    "        super(Actor, self).__init__()  \n",
    "        self.critic = critic\n",
    "        self.l1_zeros = torch.zeros([critic.world.dimension_count()], requires_grad=False, dtype=dtype, device=device)\n",
    "        self.l1 = nn.Linear(critic.world.dimension_count(),hidden_nodes)\n",
    "        self.l1.weight.data.normal_(0.0, np.sqrt(1./(critic.world.dimension_count())))\n",
    "        self.head = nn.Linear(hidden_nodes, critic.world.action_count())\n",
    "        self.head.weight.data.normal_(0.0, np.sqrt(1./hidden_nodes))\n",
    "                \n",
    "    def forward(self, state):\n",
    "        self.inputs = state\n",
    "        self.l1_out = selu(self.l1(state))\n",
    "        self.value = F.softmax(selu(self.head(self.l1_out)),dim=0)\n",
    "        return self.value\n",
    "    \n",
    "    def randomize(self):\n",
    "        self.l1_out = self.l1_zeros\n",
    "        self.value = F.softmax(torch.rand([self.head.out_features], requires_grad=False, dtype=dtype, device=device), dim=0)\n",
    "        return self.value\n",
    "    \n",
    "    def choose_action(self):\n",
    "        self.categories = Categorical(self.value)\n",
    "        self.action = self.categories.sample()\n",
    "        if args.trace: print(\"action scores:\",self.categories.probs,\"Action:\",self.action.item())\n",
    "        return self.action\n",
    "\n",
    "    #The \"advantage\" is how much better the state is after the action than we expected it would be\n",
    "    def get_loss(self):\n",
    "        #Do not include gradient of prev_value here, just the data.\n",
    "        advantage = self.critic.hindsight_value() - self.critic.prev_value.data\n",
    "        self.loss = -self.categories.log_prob(self.action)*advantage\n",
    "        if args.trace: print(\"actor loss:\", self.loss)\n",
    "        return self.loss\n",
    "    \n",
    "    def do_post_gradient(self):#NONE on main class\n",
    "        pass\n",
    "    \n",
    "    def do_pre_update(self):\n",
    "        pass\n",
    "        \n",
    "    def gc(self):\n",
    "        del self.loss\n",
    "\n",
    "class Stable_Critic(Critic):\n",
    "    \n",
    "    def __init__(self,world: World,hidden_nodes=32, alpha=0.001):\n",
    "        super(Stable_Critic, self).__init__(world, hidden_nodes)  \n",
    "        self.alpha = alpha\n",
    "        self.eps = 1.e-12\n",
    "        self.l1_filter = Novelty_Filter(self.l1.in_features)\n",
    "        self.head_filter = Novelty_Filter(self.head.in_features)\n",
    "        self.smoothed_loss = 1.e-12\n",
    "        \n",
    "    def get_loss(self):\n",
    "        super(Stable_Critic,self).get_loss()\n",
    "        self.smoothed_loss = (self.smoothed_loss * 0.999 + self.loss.abs() * 0.001).detach()\n",
    "#         print(\"Critic losses\",self.loss,self.smoothed_loss)\n",
    "        return self.loss\n",
    "        \n",
    "    def get_certainty(self):\n",
    "        return self.alpha/torch.exp(abs(self.loss.data*4./self.smoothed_loss))\n",
    "        \n",
    "    def do_post_gradient(self):\n",
    "        self.l1.weight.grad *=  self.l1_filter.novelty(self.inputs) / (self.inputs + self.eps)\n",
    "        self.head.weight.grad *= self.head_filter.novelty(self.l1_out.data) / (self.l1_out.data + self.eps)\n",
    "    \n",
    "    def do_pre_update(self):\n",
    "        certainty_factor = self.get_certainty()\n",
    "        self.l1_filter.addBasis(self.inputs.data,certainty_factor)\n",
    "        self.head_filter.addBasis(self.l1_out.data,certainty_factor)\n",
    "            \n",
    "    def gc(self):\n",
    "        super(Stable_Critic, self).gc()  \n",
    "        try: \n",
    "            0 == 0\n",
    "            del self.l1_novelty\n",
    "        except AttributeError:\n",
    "            0 == 0\n",
    "        try: \n",
    "            0 == 0\n",
    "            del self.head_novelty\n",
    "        except AttributeError:\n",
    "            0 == 0\n",
    "        \n",
    "class Stable_Actor(Actor):\n",
    "    \n",
    "    def __init__(self, critic: Critic,hidden_nodes = 64,alpha=0.001):\n",
    "        super(Stable_Actor, self).__init__(critic,hidden_nodes)  \n",
    "        self.alpha = alpha\n",
    "        self.eps = 1.e-12\n",
    "        self.l1_filter = Novelty_Filter(self.l1.in_features)\n",
    "        self.head_filter = Novelty_Filter(self.head.in_features)\n",
    "        self.smoothed_loss = 1.e-12\n",
    "        \n",
    "    def get_loss(self):\n",
    "        super(Stable_Actor,self).get_loss()\n",
    "        self.smoothed_loss = (self.smoothed_loss * 0.999 + self.loss.abs() * 0.001).detach()\n",
    "#         print(\"Actor losses\",self.loss,self.smoothed_loss)\n",
    "        return self.loss\n",
    "        \n",
    "    def get_certainty(self):\n",
    "        return self.alpha/torch.exp(abs(self.loss.data*4./self.smoothed_loss))\n",
    "        \n",
    "    def do_post_gradient(self):\n",
    "        self.l1.weight.grad *=  self.l1_filter.novelty(self.inputs) / (self.inputs + self.eps)\n",
    "        self.head.weight.grad *= self.head_filter.novelty(self.l1_out.data) / (self.l1_out.data + self.eps)\n",
    "    \n",
    "    def do_pre_update(self):\n",
    "        certainty_factor = self.get_certainty()\n",
    "        self.l1_filter.addBasis(self.inputs.data,certainty_factor)\n",
    "        self.head_filter.addBasis(self.l1_out.data,certainty_factor)\n",
    "        \n",
    "    def gc(self):\n",
    "        super(Stable_Actor, self).gc()  \n",
    "        try: \n",
    "            0 == 0\n",
    "            del self.l1_novelty\n",
    "        except AttributeError:\n",
    "            0 == 0\n",
    "        try: \n",
    "            0 == 0\n",
    "            del self.head_novelty\n",
    "        except AttributeError:\n",
    "            0 == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(episodes=1000):\n",
    "    mave_reward = 10\n",
    "    mave_value = 10.\n",
    "    action_preferences = np.array([0.5,0.5])\n",
    "    \n",
    "    for i_episode in range(1,episodes+1):\n",
    "        ep_reward = 0\n",
    "        ep_value = 0.\n",
    "        world.reset()\n",
    "        critic.forward(world.state)       \n",
    "        moves = 0\n",
    "        ep_action_preferences = np.array([0.,0.])\n",
    "        action_relevance = 1.\n",
    "        for t in range(10000):\n",
    "            actor.forward(world.state)\n",
    "            world.step(actor.choose_action())\n",
    "            ep_action_preferences += actor.categories.probs.detach().cpu().numpy()\n",
    "            moves += 1\n",
    "            critic.forward(world.state)\n",
    "            \n",
    "            ep_value += critic.value.item()\n",
    "            ep_reward += world.reward\n",
    "\n",
    "            critic_optimizer.zero_grad()\n",
    "            critic.get_loss().backward()\n",
    "            critic.do_post_gradient()\n",
    "            critic.do_pre_update()\n",
    "            critic_optimizer.step()\n",
    "#             critic.gc()\n",
    " \n",
    "            actor_optimizer.zero_grad()\n",
    "            (action_relevance * actor.get_loss()).backward()\n",
    "            actor.do_post_gradient()\n",
    "            actor.do_pre_update()\n",
    "            actor_optimizer.step()\n",
    "            action_relevance *= args.gamma\n",
    "            loss = None\n",
    "#             actor.gc()\n",
    "            \n",
    "            if(world.done):\n",
    "                if args.trace: print(\"DONE\")\n",
    "                break\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        ep_action_preferences /= moves\n",
    "        action_preferences =  0.05 * ep_action_preferences + (1 - 0.05) * action_preferences\n",
    "        mave_value /= moves\n",
    "        mave_reward = 0.05 * ep_reward + (1 - 0.05) * mave_reward\n",
    "        mave_value = 0.05 * ep_value + (1 - 0.05) * mave_value\n",
    "        if i_episode % args.log_interval == 0:\n",
    "            print('Episode {}\\tLast reward: {:.2f}\\tMoving average reward: {:.2f}\\tMoving average critic value: {:.2f}\\tAction Preferences: {:.2f},{:.2f}'.format(\n",
    "                  i_episode, ep_reward, mave_reward, mave_value,action_preferences[0],action_preferences[1]))\n",
    "        if mave_reward > world.env.spec.reward_threshold:\n",
    "            print(\"Episode {}\\tSolved! Running reward is now {} and \"\n",
    "                  \"the last episode runs to {} time steps!\".format(i_episode,mave_reward, t+1))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prime_actor(episodes=1000):\n",
    "\n",
    "    for t in range(episodes):\n",
    "        world.reset()\n",
    "        critic.forward(world.state)  \n",
    "        actor.forward(world.state)\n",
    "        world.step(actor.choose_action())\n",
    "        critic.forward(world.state)\n",
    "\n",
    "        actor_optimizer.zero_grad()\n",
    "        loss = actor.get_loss()\n",
    "        loss.backward()    \n",
    "#         actor.do_post_gradient()\n",
    "        actor_optimizer.step()\n",
    "        actor.gc()\n",
    "        \n",
    "        if t % args.log_interval == 0:\n",
    "            print(actor.categories.probs,critic.value)\n",
    "        if(world.done): print(\"DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prime_critic(episodes=1000):\n",
    "    mave_reward = 10.\n",
    "    mave_value = 10.\n",
    "    for i_episode in range(1,episodes+1):\n",
    "        ep_reward = 0.\n",
    "        ep_value = 0.\n",
    "        world.reset()\n",
    "        critic.forward(world.state)\n",
    "        moves = 0\n",
    "        for t in range(1000):\n",
    "            actor.randomize()\n",
    "            world.step(actor.choose_action())\n",
    "            moves += 1\n",
    "            ep_reward += world.reward\n",
    "            critic.forward(world.state)\n",
    "            ep_value += critic.value.item()\n",
    "            \n",
    "            critic_optimizer.zero_grad()\n",
    "            loss = critic.get_loss()\n",
    "            loss.backward()\n",
    "#             critic.do_post_gradient()\n",
    "            critic_optimizer.step()\n",
    "            critic.gc()\n",
    "            if(world.done):\n",
    "                if args.trace: print(\"DONE\")\n",
    "                break\n",
    "        ep_value /= moves\n",
    "        mave_reward = 0.05 * ep_reward + (1 - 0.05) * mave_reward\n",
    "        mave_value = 0.05 * ep_value + (1 - 0.05) * mave_value\n",
    "        if i_episode % args.log_interval == 0:\n",
    "            print('Episode {}\\tLast reward: {:.2f}\\tMoving average reward: {:.2f}\\tMoving average critic value: {:.2f}'.format(\n",
    "                  i_episode, ep_reward, mave_reward, mave_value))\n",
    "        if mave_reward > world.env.spec.reward_threshold:\n",
    "            print(\"Episode {}\\tSolved! Running reward is now {} and \"\n",
    "                  \"the last episode runs to {} time steps!\".format(i_episode,mave_reward, t+1))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_trainer():\n",
    "    args.trace = False\n",
    "    args.render = False\n",
    "    global world\n",
    "    global actor\n",
    "    global critic\n",
    "    global actor_optimizer\n",
    "    global critic_optimizer\n",
    "    world = World()\n",
    "    critic = Stable_Critic(world,32,0.002)#world,32,0.001)\n",
    "#     critic = Critic(world)\n",
    "    actor = Stable_Actor(critic,64,0.002)#critic,64,0.002)\n",
    "#     actor = Actor(critic)\n",
    "    world.env.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    actor_optimizer = optim.Adam(actor.parameters(), lr=4e-5,weight_decay=0.0001)#lr=4e-5,weight_decay=0.00001)\n",
    "    critic_optimizer = optim.Adam(critic.parameters(), lr=5e-3,weight_decay=0.0001)#lr=5e-3,weight_decay=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tLast reward: 30.00\tMoving average reward: 23.96\tMoving average critic value: 16.70\n",
      "Episode 200\tLast reward: 13.00\tMoving average reward: 20.81\tMoving average critic value: 12.19\n",
      "Episode 300\tLast reward: 15.00\tMoving average reward: 20.68\tMoving average critic value: 12.03\n",
      "Episode 400\tLast reward: 45.00\tMoving average reward: 27.92\tMoving average critic value: 13.20\n",
      "Episode 500\tLast reward: 39.00\tMoving average reward: 23.52\tMoving average critic value: 12.12\n",
      "Episode 600\tLast reward: 26.00\tMoving average reward: 21.54\tMoving average critic value: 11.94\n",
      "Episode 700\tLast reward: 21.00\tMoving average reward: 23.35\tMoving average critic value: 12.11\n",
      "Episode 800\tLast reward: 20.00\tMoving average reward: 18.64\tMoving average critic value: 9.24\n",
      "Episode 900\tLast reward: 32.00\tMoving average reward: 21.69\tMoving average critic value: 9.87\n",
      "Episode 1000\tLast reward: 27.00\tMoving average reward: 23.24\tMoving average critic value: 11.46\n",
      "tensor([0.4067, 0.5933], grad_fn=<DivBackward0>) tensor([18.0035], grad_fn=<AddBackward0>)\n",
      "Episode 100\tLast reward: 57.00\tMoving average reward: 112.87\tMoving average critic value: 174.76\tAction Preferences: 0.48,0.52\n",
      "Episode 200\tLast reward: 114.00\tMoving average reward: 167.29\tMoving average critic value: 320.85\tAction Preferences: 0.50,0.50\n",
      "Episode 292\tSolved! Running reward is now 195.1001327129338 and the last episode runs to 200 time steps!\n",
      "Episode 100\tLast reward: 30.00\tMoving average reward: 23.96\tMoving average critic value: 16.68\n",
      "Episode 200\tLast reward: 13.00\tMoving average reward: 20.81\tMoving average critic value: 12.10\n",
      "Episode 300\tLast reward: 15.00\tMoving average reward: 20.68\tMoving average critic value: 11.94\n",
      "Episode 400\tLast reward: 45.00\tMoving average reward: 27.92\tMoving average critic value: 13.16\n",
      "Episode 500\tLast reward: 39.00\tMoving average reward: 23.52\tMoving average critic value: 12.08\n",
      "Episode 600\tLast reward: 26.00\tMoving average reward: 21.54\tMoving average critic value: 11.93\n",
      "Episode 700\tLast reward: 21.00\tMoving average reward: 23.35\tMoving average critic value: 12.00\n",
      "Episode 800\tLast reward: 20.00\tMoving average reward: 18.64\tMoving average critic value: 9.28\n",
      "Episode 900\tLast reward: 32.00\tMoving average reward: 21.69\tMoving average critic value: 9.95\n",
      "Episode 1000\tLast reward: 27.00\tMoving average reward: 23.24\tMoving average critic value: 11.42\n",
      "tensor([0.7754, 0.2246], grad_fn=<DivBackward0>) tensor([19.5766], grad_fn=<AddBackward0>)\n",
      "Episode 100\tLast reward: 151.00\tMoving average reward: 122.98\tMoving average critic value: 409.93\tAction Preferences: 0.51,0.49\n",
      "Episode 184\tSolved! Running reward is now 195.15865868437282 and the last episode runs to 200 time steps!\n",
      "Episode 100\tLast reward: 30.00\tMoving average reward: 23.96\tMoving average critic value: 16.94\n",
      "Episode 200\tLast reward: 13.00\tMoving average reward: 20.81\tMoving average critic value: 12.27\n",
      "Episode 300\tLast reward: 15.00\tMoving average reward: 20.68\tMoving average critic value: 11.98\n",
      "Episode 400\tLast reward: 45.00\tMoving average reward: 27.92\tMoving average critic value: 13.23\n",
      "Episode 500\tLast reward: 39.00\tMoving average reward: 23.52\tMoving average critic value: 12.01\n",
      "Episode 600\tLast reward: 26.00\tMoving average reward: 21.54\tMoving average critic value: 11.93\n",
      "Episode 700\tLast reward: 21.00\tMoving average reward: 23.35\tMoving average critic value: 12.11\n",
      "Episode 800\tLast reward: 20.00\tMoving average reward: 18.64\tMoving average critic value: 9.25\n",
      "Episode 900\tLast reward: 32.00\tMoving average reward: 21.69\tMoving average critic value: 9.85\n",
      "Episode 1000\tLast reward: 27.00\tMoving average reward: 23.24\tMoving average critic value: 11.43\n",
      "tensor([0.6102, 0.3898], grad_fn=<DivBackward0>) tensor([17.8936], grad_fn=<AddBackward0>)\n",
      "Episode 100\tLast reward: 29.00\tMoving average reward: 26.37\tMoving average critic value: 30.24\tAction Preferences: 0.55,0.45\n",
      "Episode 200\tLast reward: 118.00\tMoving average reward: 104.53\tMoving average critic value: 305.16\tAction Preferences: 0.54,0.46\n",
      "Episode 300\tLast reward: 180.00\tMoving average reward: 177.46\tMoving average critic value: 469.46\tAction Preferences: 0.51,0.49\n",
      "Episode 332\tSolved! Running reward is now 195.0309412674273 and the last episode runs to 200 time steps!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "args.trace = False\n",
    "args.log_interval = 100\n",
    "for i in range(3):\n",
    "    reset_trainer()\n",
    "    prime_critic(1000)\n",
    "    prime_actor(50)\n",
    "    train(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.log_interval = 10\n",
    "args.trace = False\n",
    "args.render = False\n",
    "\n",
    "reset_trainer()\n",
    "train(10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(critic.filter1.O,critic.filter2.O)\n",
    "print(actor.filter1.O,actor.filter2.O)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 1e-12\n",
    "#    certainty_factor = 0.1/torch.exp(abs(self.loss))\n",
    "for i in range(-6,6):\n",
    "    X = torch.tensor([i], requires_grad=False, dtype=dtype, device=device)\n",
    "    print(i,0.0001/torch.exp(abs(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4112, -0.0249,  0.0298, -0.0159],\n",
       "        [-0.0249,  0.3935, -0.0209,  0.0370],\n",
       "        [ 0.0298, -0.0209,  0.5550, -0.0433],\n",
       "        [-0.0159,  0.0370, -0.0433,  0.3683]])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "critic.l1_filter.O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4873, -0.0536,  0.0392, -0.0297],\n",
       "        [-0.0536,  0.4505, -0.0662,  0.0567],\n",
       "        [ 0.0392, -0.0662,  0.8194, -0.0619],\n",
       "        [-0.0297,  0.0567, -0.0619,  0.4468]])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "actor.l1_filter.O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-25.8279], grad_fn=<MulBackward0>) tensor([0.4362])\n",
      "tensor(2973.9817, grad_fn=<MseLossBackward>) tensor(19.4608)\n"
     ]
    }
   ],
   "source": [
    "print(actor.loss,actor.smoothed_loss)\n",
    "print(critic.loss,critic.smoothed_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.5032, -0.1820,  0.0473, -0.2592])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actor.inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.5068, -0.3778,  0.0421,  0.0480])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "critic.inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.trace = False\n",
    "# train(100)\n",
    "args.trace = True\n",
    "train(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
