{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "cuda not used\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "print(torch.cuda.is_available())\n",
    "if torch.cuda.is_available() and False:\n",
    "    print (\"cuda in use\")\n",
    "    device = torch.device('cuda') \n",
    "    torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "    dtype = torch.cuda.FloatTensor\n",
    "else:\n",
    "    print (\"cuda not used\")\n",
    "    device = torch.device('cpu')\n",
    "    torch.set_default_tensor_type('torch.FloatTensor')\n",
    "    dtype = torch.float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from T. Kohonen '84, p. 183, equation 6.33\n",
    "\n",
    "#Set alpha to one for one-step orthogonalization.\n",
    "class Novelty_Filter():\n",
    "    \n",
    "    def __init__(self,size,epsilon=1e-4):\n",
    "        self.O = torch.eye(size, requires_grad=False, dtype=dtype, device=device)\n",
    "        self.epsilon = epsilon\n",
    "        return\n",
    "        \n",
    "    def addBasis(self,X,cf=1.):\n",
    "        l = self.novelty(X)\n",
    "        n = l.norm()\n",
    "        print(\"norms\",n,X.norm())\n",
    "        if(cf * n > self.epsilon * X.norm()):\n",
    "            print(\"suppressing\")\n",
    "            self.O -= cf * l.ger(l) / n.pow(2)\n",
    "        return\n",
    "    \n",
    "#     def addBasis(self,X,cf=0.):\n",
    "# #         print(\"cf\",cf)\n",
    "#         #Memorize this (linear) pattern\n",
    "#         X = self.novelty(X)\n",
    "#         n = torch.norm(X)\n",
    "#         if (n*cf > self.epsilon):\n",
    "#             self.O -= cf * X.ger(X) / n.pow(2)\n",
    "#         return\n",
    "    \n",
    "    def novelty(self,X):\n",
    "        return self.O @ X\n",
    "    \n",
    "    def project(self,X):\n",
    "        return X - self.novelty(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data tensor([[1., 1., 1., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 1., 0., 1.]])\n"
     ]
    }
   ],
   "source": [
    "def selu(x):\n",
    "    alpha = 1.6732632423543772848170429916717\n",
    "    scale = 1.0507009873554804934193349852946\n",
    "    return scale * F.elu(x, alpha)\n",
    "\n",
    "class Xor(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Xor, self).__init__() \n",
    "        hidden_nodes = 51\n",
    "        self.l1 = nn.Linear(3,hidden_nodes,bias=False)\n",
    "        self.l1.weight.data.normal_(0.0, np.sqrt(1./3.))\n",
    "        self.head = nn.Linear(hidden_nodes, 1,bias=False)\n",
    "        self.head.weight.data.normal_(0.0, np.sqrt(1./hidden_nodes))\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.l1_out = selu(self.l1(inputs))\n",
    "        self.value = self.head(self.l1_out)\n",
    "        return self.value\n",
    "    \n",
    "    def get_loss(self,target):\n",
    "        self.loss = F.mse_loss(target,self.value)\n",
    "        self.smoothed_loss = self.smoothed_loss * 0.95 + self.loss * 0.05\n",
    "        return self.loss\n",
    "\n",
    "class Stable_Xor(Xor):\n",
    "    \n",
    "    def __init__(self,alpha=0.):\n",
    "        super(Stable_Xor, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.eps = 1.e-12\n",
    "        self.l1_filter = Novelty_Filter(self.l1.in_features)\n",
    "        self.head_filter = Novelty_Filter(self.head.in_features)\n",
    "        self.smoothed_loss = 1.e-12\n",
    "        \n",
    "    def get_loss(self,target):\n",
    "        super(Stable_Xor, self).get_loss()\n",
    "        self.smoothed_loss = (self.smoothed_loss * 0.95 + self.loss * 0.05).detach()\n",
    "        return self.loss\n",
    "        \n",
    "    def get_certainty():\n",
    "        return self.alpha/torch.exp(abs(self.loss.data*4./self.smoothed_loss))\n",
    "        \n",
    "    def do_post_gradient(self):\n",
    "        self.l1.weight.grad *=  self.l1_filter.novelty(self.inputs) / (self.inputs + self.eps)\n",
    "        self.head.weight.grad *= self.head_filter.novelty(self.l1_out.data) / (self.l1_out.data + self.eps)\n",
    "    \n",
    "    def do_pre_update(self):\n",
    "        certainty_factor = self.get_certainty()\n",
    "        print(\"cfx\",certainty_factor)\n",
    "        self.l1_filter.addBasis(self.inputs.data,certainty_factor)\n",
    "        self.head_filter.addBasis(self.l1_out.data,certainty_factor)\n",
    "        \n",
    "data = torch.tensor([[1,1,1,0],[1,0,0,0],[1,0,1,1],[1,1,0,1]], requires_grad=False, dtype=dtype, device=device)\n",
    "print(\"data\",data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "input tensor([1., 1., 1.]) output tensor([-0.0004]) target tensor([0.])\n",
      "loss tensor(1.4983e-07, grad_fn=<MeanBackward0>) tensor([-0.0004], grad_fn=<SqueezeBackward3>) tensor([0.])\n",
      "cfx tensor(1.0000)\n",
      "norms tensor(1.7321) tensor(1.7321)\n",
      "suppressing\n",
      "norms tensor(3.5217) tensor(3.5217)\n",
      "suppressing\n",
      "Current Forecasts\n",
      "input tensor([1., 1., 1.]) output tensor([0.0057]) target tensor([0.])\n",
      "input tensor([1., 0., 0.]) output tensor([-0.4531]) target tensor([0.])\n",
      "input tensor([1., 0., 1.]) output tensor([0.4286]) target tensor([1.])\n",
      "input tensor([1., 1., 0.]) output tensor([-0.9214]) target tensor([1.])\n",
      "End\n",
      "input tensor([1., 0., 0.]) output tensor([-0.0016]) target tensor([0.])\n",
      "loss tensor(2.5112e-06, grad_fn=<MeanBackward0>) tensor([-0.0016], grad_fn=<SqueezeBackward3>) tensor([0.])\n",
      "cfx tensor(1.0000)\n",
      "norms tensor(0.8165) tensor(1.)\n",
      "suppressing\n",
      "norms tensor(3.7344) tensor(3.8567)\n",
      "suppressing\n",
      "Current Forecasts\n",
      "input tensor([1., 1., 1.]) output tensor([0.0282]) target tensor([0.])\n",
      "input tensor([1., 0., 0.]) output tensor([0.0002]) target tensor([0.])\n",
      "input tensor([1., 0., 1.]) output tensor([0.5558]) target tensor([1.])\n",
      "input tensor([1., 1., 0.]) output tensor([-0.5839]) target tensor([1.])\n",
      "End\n",
      "input tensor([1., 0., 1.]) output tensor([1.0019]) target tensor([1.])\n",
      "loss tensor(3.5051e-06, grad_fn=<MeanBackward0>) tensor([1.0019], grad_fn=<SqueezeBackward3>) tensor([1.])\n",
      "cfx tensor(1.0000)\n",
      "norms tensor(0.7071) tensor(1.4142)\n",
      "suppressing\n",
      "norms tensor(3.1161) tensor(3.9282)\n",
      "suppressing\n",
      "Current Forecasts\n",
      "input tensor([1., 1., 1.]) output tensor([0.0049]) target tensor([0.])\n",
      "input tensor([1., 0., 0.]) output tensor([0.0155]) target tensor([0.])\n",
      "input tensor([1., 0., 1.]) output tensor([1.0023]) target tensor([1.])\n",
      "input tensor([1., 1., 0.]) output tensor([-1.0279]) target tensor([1.])\n",
      "End\n",
      "input tensor([1., 1., 0.]) output tensor([0.9924]) target tensor([1.])\n",
      "loss tensor(5.8131e-05, grad_fn=<MeanBackward0>) tensor([0.9924], grad_fn=<SqueezeBackward3>) tensor([1.])\n",
      "cfx tensor(0.9999)\n",
      "norms tensor(3.6071e-06) tensor(1.4142)\n",
      "norms tensor(0.4968) tensor(4.7361)\n",
      "suppressing\n",
      "Current Forecasts\n",
      "input tensor([1., 1., 1.]) output tensor([0.1357]) target tensor([0.])\n",
      "input tensor([1., 0., 0.]) output tensor([0.2676]) target tensor([0.])\n",
      "input tensor([1., 0., 1.]) output tensor([0.3814]) target tensor([1.])\n",
      "input tensor([1., 1., 0.]) output tensor([0.9918]) target tensor([1.])\n",
      "End\n"
     ]
    }
   ],
   "source": [
    "    \n",
    "def showme():\n",
    "    print(\"Current Forecasts\")\n",
    "    for j in range(4):\n",
    "        print(\"input\",data[j][0:3],\"output\",xor(data[j][0:3]).data,\"target\",data[j][3:4])\n",
    "    print(\"End\")\n",
    "    \n",
    "xor = Stable_Xor(1.)\n",
    "xopt = optim.Adam(xor.parameters(), lr=1e-1,weight_decay=0.000001)\n",
    "iterations=100\n",
    "stable = True\n",
    "\n",
    "\n",
    "\n",
    "print(\"---\")\n",
    "for j in range(iterations):\n",
    "    xor(data[0][0:3])\n",
    "    xor.get_loss(data[0][3:4]).backward()\n",
    "    if stable: xor.do_post_gradient()\n",
    "    xopt.step()\n",
    "    xopt.zero_grad()\n",
    "\n",
    "print(\"input\",data[0][0:3],\"output\",xor(data[0][0:3]).data,\"target\",data[0][3:4])\n",
    "    \n",
    "xor(data[0][0:3])\n",
    "loss=xor.get_loss(data[0][3:4])\n",
    "print(\"loss\",loss,xor.value,data[0][3:4])\n",
    "loss.backward()\n",
    "if stable: xor.do_post_gradient()\n",
    "if stable: xor.do_pre_update()\n",
    "xopt.step()\n",
    "xopt.zero_grad()\n",
    "\n",
    "showme()\n",
    "for j in range(iterations):\n",
    "    xor(data[1][0:3])\n",
    "    xor.get_loss(data[1][3:4]).backward()\n",
    "    if stable: xor.do_post_gradient()\n",
    "    xopt.step()\n",
    "    xopt.zero_grad()\n",
    "    \n",
    "print(\"input\",data[1][0:3],\"output\",xor(data[1][0:3]).data,\"target\",data[1][3:4])\n",
    "xor(data[1][0:3])\n",
    "loss=xor.get_loss(data[1][3:4])\n",
    "print(\"loss\",loss,xor.value,data[1][3:4])\n",
    "loss.backward()\n",
    "xor.do_post_gradient()\n",
    "xor.do_pre_update()\n",
    "xopt.step()\n",
    "xopt.zero_grad()\n",
    "\n",
    "showme()\n",
    "for j in range(iterations):\n",
    "    xor(data[2][0:3])\n",
    "    xor.get_loss(data[2][3:4]).backward()\n",
    "    if stable: xor.do_post_gradient()\n",
    "    xopt.step()\n",
    "    xopt.zero_grad()\n",
    "    \n",
    "print(\"input\",data[2][0:3],\"output\",xor(data[2][0:3]).data,\"target\",data[2][3:4])\n",
    "xor(data[2][0:3])\n",
    "loss=xor.get_loss(data[2][3:4])\n",
    "print(\"loss\",loss,xor.value,data[2][3:4])\n",
    "loss.backward()\n",
    "if stable: xor.do_post_gradient()\n",
    "if stable: xor.do_pre_update()\n",
    "xopt.step()\n",
    "xopt.zero_grad()\n",
    "showme()\n",
    "for j in range(iterations):\n",
    "    xor(data[3][0:3])\n",
    "    loss =  xor.get_loss(data[3][3:4])\n",
    "    loss.backward()\n",
    "    if stable: xor.do_post_gradient()\n",
    "    xopt.step()\n",
    "    xopt.zero_grad()\n",
    "    \n",
    "print(\"input\",data[3][0:3],\"output\",xor(data[3][0:3]).data,\"target\",data[3][3:4])\n",
    "xor(data[3][0:3])\n",
    "loss=xor.get_loss(data[3][3:4])\n",
    "print(\"loss\",loss,xor.value,data[3][3:4])\n",
    "loss.backward()\n",
    "if stable: xor.do_post_gradient()\n",
    "if stable: xor.do_pre_update()\n",
    "xopt.step()\n",
    "xopt.zero_grad()\n",
    "    \n",
    "showme()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Forecasts\n",
      "input tensor([1., 1., 1.]) output tensor([-1.7869]) target tensor([0.])\n",
      "input tensor([1., 0., 0.]) output tensor([-2.2013]) target tensor([0.])\n",
      "input tensor([1., 0., 1.]) output tensor([-3.4242]) target tensor([1.])\n",
      "input tensor([1., 1., 0.]) output tensor([0.1817]) target tensor([1.])\n",
      "End\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    for j in range(4):\n",
    "        xor(data[j][0:3])\n",
    "        loss =  xor.get_loss(data[j][3:4])\n",
    "        loss.backward()\n",
    "        xor.do_post_gradient()\n",
    "        xopt.step()\n",
    "        xopt.zero_grad()\n",
    "showme()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "xor = Stable_Xor(1.)\n",
    "xopt = optim.Adam(xor.parameters(), lr=1e-1,weight_decay=0.000001)\n",
    "iterations=500\n",
    "print(\"---\")\n",
    "for j in range(iterations):\n",
    "    xor(data[0][0:3])\n",
    "    xor.get_loss(data[0][3:4]).backward()\n",
    "    xor.do_post_gradient()\n",
    "    xopt.step()\n",
    "    xopt.zero_grad()\n",
    "\n",
    "print(\"input\",data[0][0:3],\"output\",xor(data[0][0:3]).data,\"target\",data[0][3:4])\n",
    "print(\"novelty of input, is\",xor.l1_filter.novelty(xor.inputs))\n",
    "print(\"novelty of hidden, is\",xor.head_filter.novelty(xor.l1_out))\n",
    "    \n",
    "xor(data[0][0:3])\n",
    "loss=xor.get_loss(data[0][3:4])\n",
    "print(\"loss\",loss,xor.value,data[0][3:4])\n",
    "loss.backward()\n",
    "xor.do_post_gradient()\n",
    "xor.do_pre_update()\n",
    "xopt.step()\n",
    "xopt.zero_grad()\n",
    "print(\"post novelty of input, is\",xor.l1_filter.novelty(xor.inputs))\n",
    "print(\"post novelty of hidden, is\",xor.head_filter.novelty(xor.l1_out))\n",
    "\n",
    "print(\"input\",data[1][0:3],\"output\",xor(data[1][0:3]).data,\"target\",data[1][3:4])\n",
    "print(\"novelty of input, is\",xor.l1_filter.novelty(xor.inputs))\n",
    "print(\"novelty of hidden, is\",xor.head_filter.novelty(xor.l1_out))\n",
    "for j in range(iterations):\n",
    "    xor(data[1][0:3])\n",
    "    xor.get_loss(data[1][3:4]).backward()\n",
    "    xor.do_post_gradient()\n",
    "    xopt.step()\n",
    "    xopt.zero_grad()\n",
    "\n",
    "print(\"input\",data[1][0:3],\"output\",xor(data[1][0:3]).data,\"target\",data[1][3:4])\n",
    "print(\"novelty of input, is\",xor.l1_filter.novelty(xor.inputs))\n",
    "print(\"post novelty of hidden, is\",xor.head_filter.novelty(xor.l1_out))\n",
    "# print(\"novelty of input, is\",xor.l1_filter.novelty(xor.inputs))\n",
    "# print(\"novelty of hidden, is\",xor.head_filter.novelty(xor.l1_out))\n",
    "    \n",
    "print(\"final\")\n",
    "for j in range(4):\n",
    "    print(\"input\",data[j][0:3],\"output\",xor(data[j][0:3]).data,\"target\",data[j][3:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xor(data[1][0:3])\n",
    "loss=xor.get_loss(data[1][3:4])\n",
    "print(\"loss\",loss,xor.value,data[1][3:4])\n",
    "loss.backward()\n",
    "xor.do_post_gradient()\n",
    "xor.do_pre_update()\n",
    "xopt.step()\n",
    "xopt.zero_grad()\n",
    "\n",
    "for j in range(iterations):\n",
    "    xor(data[2][0:3])\n",
    "    xor.get_loss(data[2][3:4]).backward()\n",
    "    xor.do_post_gradient()\n",
    "    xopt.step()\n",
    "    xopt.zero_grad()\n",
    "\n",
    "print(\"input\",data[2][0:3],\"output\",xor(data[2][0:3]).data,\"target\",data[2][3:4])\n",
    "# print(\"novelty of input, is\",xor.l1_filter.novelty(xor.inputs))\n",
    "# print(\"novelty of hidden, is\",xor.head_filter.novelty(xor.l1_out))\n",
    "\n",
    "xor(data[2][0:3])\n",
    "loss=xor.get_loss(data[2][3:4])\n",
    "print(\"loss\",loss,xor.value,data[2][3:4])\n",
    "loss.backward()\n",
    "xor.do_post_gradient()\n",
    "xor.do_pre_update()\n",
    "xopt.step()\n",
    "xopt.zero_grad()\n",
    "\n",
    "for j in range(iterations):\n",
    "    xor(data[3][0:3])\n",
    "    xor.get_loss(data[3][3:4]).backward()\n",
    "    xor.do_post_gradient()\n",
    "    xopt.step()\n",
    "    xopt.zero_grad()\n",
    "\n",
    "print(\"input\",data[3][0:3],\"output\",xor(data[3][0:3]).data,\"target\",data[3][3:4])\n",
    "# print(\"novelty of input, is\",xor.l1_filter.novelty(xor.inputs))\n",
    "# print(\"novelty of hidden, is\",xor.head_filter.novelty(xor.l1_out))\n",
    "\n",
    "xor(data[3][0:3])\n",
    "loss=xor.get_loss(data[3][3:4])\n",
    "print(\"loss\",loss,xor.value,data[3][3:4])\n",
    "    \n",
    "print(\"final\")\n",
    "for j in range(4):\n",
    "    print(\"input\",data[j][0:3],\"output\",xor(data[j][0:3]).data,\"target\",data[j][3:4])\n",
    "#     print(\"novelty of input, is\",xor.l1_filter.novelty(xor.inputs))\n",
    "#     print(\"novelty of hidden, is\",xor.head_filter.novelty(xor.l1_out))\n",
    "    \n",
    "print(xor.l1_filter.O)\n",
    "print(xor.head_filter.O[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "f = Novelty_Filter(3)\n",
    "X = torch.tensor([7,.1,11], requires_grad=False, dtype=dtype, device=device)\n",
    "X2 = torch.tensor([7,-3,.1], requires_grad=False, dtype=dtype, device=device)\n",
    "v = torch.ones([1], requires_grad=False, dtype=dtype, device=device)\n",
    "print(X)\n",
    "alpha = 0.2\n",
    "for i in range(10):\n",
    "    certainty_factor = alpha/torch.exp(abs(i*.1*v))\n",
    "    print(certainty_factor)\n",
    "    f.addBasis(X,certainty_factor)\n",
    "    print(f.novelty(X),f.novelty(X2))\n",
    "    print(torch.norm(X),torch.norm(f.novelty(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xor = Xor()\n",
    "# xopt = optim.Adam(xor.parameters(), lr=3e-2,weight_decay=0.00001)\n",
    "\n",
    "# print(data)\n",
    "# for i in range(1000):\n",
    "#     for j in range(4):\n",
    "#         xor(data[j][0:2])\n",
    "#         xor.get_loss(data[j][2:3]).backward()\n",
    "#     xopt.step()\n",
    "#     xopt.zero_grad()\n",
    "\n",
    "# for j in range(4):\n",
    "#     print(xor(data[j][0:2]).data,data[j][2:3])\n",
    "    \n",
    "# xor = Xor()\n",
    "# xopt = optim.Adam(xor.parameters(), lr=3e-2,weight_decay=0.00001)\n",
    "\n",
    "# print(\"---\")\n",
    "# for i in range(1000):\n",
    "#     for j in range(4):\n",
    "#         xor(data[j][0:2])\n",
    "#         xor.get_loss(data[j][2:3]).backward()\n",
    "#         xopt.step()\n",
    "#         xopt.zero_grad()\n",
    "        \n",
    "# for j in range(4):\n",
    "#     print(xor(data[j][0:2]).data,data[j][2:3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "#from T. Kohonen '84, p. 119, equation 4.63, p. 122, equation 4.68\n",
    "class Adaptive_Novelty(Fast_Novelty):\n",
    "    \n",
    "    def __init__(self,size,alpha=0.95,gamma=1e-3):\n",
    "        super(Adaptive_Novelty,self).__init__(size,alpha)\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        \n",
    "    def addBasis(self,X):\n",
    "        X = X.data\n",
    "        #Slowly learn to remember this (linear) pattern\n",
    "        O2 = self.O.pow(2)\n",
    "        self.O -= self.alpha*O2*X.ger(X)*O2\n",
    "        #Very gradually forget everything we've learned\n",
    "        if self.gamma > 0.:\n",
    "            self.O += self.gamma * (self.O - O2)\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "O = torch.eye(3, requires_grad=False, dtype=dtype, device=device)\n",
    "a = 0.003\n",
    "g = 0.01\n",
    "X = torch.tensor([7,0.17,11], requires_grad=False, dtype=dtype, device=device).t()\n",
    "X2 = torch.tensor([7,-3,.1], requires_grad=False, dtype=dtype, device=device).t()\n",
    "O2 = np.square(O)\n",
    "print(a*(O2*X*X.t()*O2))\n",
    "print(a*(O2*(X*X.t())*O2))\n",
    "print(a*O2*X.ger(X)*O2)\n",
    "for i in range(1000):\n",
    "    n = O @ X\n",
    "    O2 = np.square(O)\n",
    "    O -= a*O2*X.ger(X)*O - g * (O - O2)\n",
    "    \n",
    "#     O = O - a*O2*X.ger(X)*O + g * (O - O2)\n",
    "print(X,X2)\n",
    "print(O @ X, O@X2)\n",
    "print(X - O@X, X2 - O@X2)\n",
    "\n",
    "print('---')\n",
    "# O = torch.eye(3, requires_grad=False, dtype=dtype, device=device)\n",
    "for i in range(1000):\n",
    "    n = O @ X2\n",
    "    O2 = np.square(O)\n",
    "    O -= a*O2*X2.ger(X2)*O - g * (O - O2)\n",
    "print(X,X2)\n",
    "print(O @ X, O@X2)\n",
    "print(X - O@X, X2 - O@X2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor([7,0.17,11], requires_grad=False, dtype=dtype, device=device).t()\n",
    "X2 = torch.tensor([7,-3,.1], requires_grad=False, dtype=dtype, device=device).t()\n",
    "filter = Adaptive_Novelty(3,alpha=0.01,gamma=5e-2)\n",
    "filter.addBasis(X)\n",
    "print(filter.novelty(X2),filter.novelty(filter.novelty(filter.novelty(X2))))\n",
    "filter.addBasis(X2)\n",
    "\n",
    "filter2 = Adaptive_Novelty(3,alpha=0.01,gamma=0.)\n",
    "print(filter2.novelty(X2))\n",
    "filter2.addBasis(X)\n",
    "print(filter2.novelty(X2))\n",
    "filter2.addBasis(filter2.novelty(X2))\n",
    "print(filter.O,filter2.O)\n",
    "print(filter.project(X2),filter2.project(X2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor([1,0,0], requires_grad=False, dtype=dtype, device=device).t()\n",
    "X2 = torch.tensor([0,1,1], requires_grad=False, dtype=dtype, device=device).t()\n",
    "O = torch.eye(3, requires_grad=False, dtype=torch.float)\n",
    "a = 7e-3\n",
    "print(O @ X,O@X2)\n",
    "d=O@X\n",
    "d2=O@X2\n",
    "for i in range(100000):\n",
    "    n = O @ X\n",
    "    O2 = O.pow(2)\n",
    "    O = O - a*O2*X.ger(X)*O2\n",
    "print(O @ X, O@X2)\n",
    "print(X - O@X, X2 - O@X2)\n",
    "print(O@X/d)\n",
    "print(O@X2/d2)\n",
    "# for i in range(10000):\n",
    "#     n = O @ X2\n",
    "#     O2 = O.pow(2)\n",
    "#     O = O - a*O2*X2.ger(X2)*O2\n",
    "# print(O @ X, O@X2)\n",
    "# print(X - O@X, X2 - O@X2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ortho = Orthogonalizer(3)\n",
    "print(ortho.novelty(X),ortho.novelty(X2))\n",
    "ortho.addBasis(X2)\n",
    "print(ortho.novelty(X),ortho.novelty(X2))\n",
    "print(ortho.project(X),ortho.project(X2))\n",
    "\n",
    "Y = torch.stack([X,X,X]).numpy()\n",
    "Y2 = torch.stack([X2,X2,X2]).numpy()\n",
    "print(Y,Y2)\n",
    "Q,r = np.linalg.qr(Y)\n",
    "print(Q@Q.T)\n",
    "print(ortho.O)\n",
    "print(Q @ Q.T @ Y)#Projection from Q\n",
    "print(Q @ Q.T @ Y2)#Projection from Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data tensor([[1., 1., 1., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 1., 0., 1.]])\n",
      "tensor([[1., 0., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 0., 1.]])\n",
      "adding tensor([1., 1., 1.])\n",
      "norms tensor(1.7321) tensor(1.7321)\n",
      "suppressing\n",
      "tensor([[ 0.6667, -0.3333, -0.3333],\n",
      "        [-0.3333,  0.6667, -0.3333],\n",
      "        [-0.3333, -0.3333,  0.6667]])\n",
      "----------\n",
      "tensor([1., 1., 1.]) tensor([1., 1., 1.])\n",
      "tensor([1., 0., 0.]) tensor([0.3333, 0.3333, 0.3333])\n",
      "tensor([1., 0., 1.]) tensor([0.6667, 0.6667, 0.6667])\n",
      "tensor([1., 1., 0.]) tensor([0.6667, 0.6667, 0.6667])\n",
      "==========\n",
      "adding tensor([1., 0., 0.])\n",
      "norms tensor(0.8165) tensor(1.)\n",
      "suppressing\n",
      "tensor([[ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.5000, -0.5000],\n",
      "        [ 0.0000, -0.5000,  0.5000]])\n",
      "----------\n",
      "tensor([1., 1., 1.]) tensor([1., 1., 1.])\n",
      "tensor([1., 0., 0.]) tensor([1., 0., 0.])\n",
      "tensor([1., 0., 1.]) tensor([1.0000, 0.5000, 0.5000])\n",
      "tensor([1., 1., 0.]) tensor([1.0000, 0.5000, 0.5000])\n",
      "==========\n",
      "adding tensor([1., 0., 1.])\n",
      "norms tensor(0.7071) tensor(1.4142)\n",
      "suppressing\n",
      "tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00, -1.7881e-07,  5.9605e-08],\n",
      "        [ 0.0000e+00,  5.9605e-08, -2.9802e-08]])\n",
      "----------\n",
      "tensor([1., 1., 1.]) tensor([1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 0., 0.]) tensor([1., 0., 0.])\n",
      "tensor([1., 0., 1.]) tensor([ 1.0000e+00, -5.9605e-08,  1.0000e+00])\n",
      "tensor([1., 1., 0.]) tensor([ 1.0000e+00,  1.0000e+00, -5.9605e-08])\n",
      "==========\n",
      "adding tensor([1., 1., 0.])\n",
      "norms tensor(1.8849e-07) tensor(1.4142)\n",
      "tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00, -1.7881e-07,  5.9605e-08],\n",
      "        [ 0.0000e+00,  5.9605e-08, -2.9802e-08]])\n",
      "----------\n",
      "tensor([1., 1., 1.]) tensor([1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 0., 0.]) tensor([1., 0., 0.])\n",
      "tensor([1., 0., 1.]) tensor([ 1.0000e+00, -5.9605e-08,  1.0000e+00])\n",
      "tensor([1., 1., 0.]) tensor([ 1.0000e+00,  1.0000e+00, -5.9605e-08])\n",
      "==========\n"
     ]
    }
   ],
   "source": [
    "def showme2():\n",
    "    print('----------')\n",
    "    print(data[0][0:3],ortho.project(data[0][0:3]))\n",
    "    print(data[1][0:3],ortho.project(data[1][0:3]))\n",
    "    print(data[2][0:3],ortho.project(data[2][0:3]))\n",
    "    print(data[3][0:3],ortho.project(data[3][0:3]))\n",
    "    print('==========')\n",
    "\n",
    "data = torch.tensor([[1,1,1,0],[1,0,0,0],[1,0,1,1],[1,1,0,1]], requires_grad=False, dtype=dtype, device=device)\n",
    "print(\"data\",data)\n",
    "\n",
    "ortho = Novelty_Filter(3)\n",
    "print(ortho.O)\n",
    "print(\"adding\",data[0][0:3])\n",
    "ortho.addBasis(data[0][0:3])\n",
    "print(ortho.O)\n",
    "showme2()\n",
    "print(\"adding\",data[1][0:3])\n",
    "ortho.addBasis(data[1][0:3])\n",
    "print(ortho.O)\n",
    "showme2()\n",
    "print(\"adding\",data[2][0:3])\n",
    "ortho.addBasis(data[2][0:3])\n",
    "print(ortho.O)\n",
    "showme2()\n",
    "print(\"adding\",data[3][0:3])\n",
    "ortho.addBasis(data[3][0:3])\n",
    "print(ortho.O)\n",
    "\n",
    "showme2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "I = torch.eye(3, requires_grad=False, dtype=dtype, device=device)\n",
    "V1 = torch.tensor([1,0.5,0.5], requires_grad=False, dtype=dtype, device=device)\n",
    "W = torch.eye(3, requires_grad=True, dtype=dtype, device=device)\n",
    "print(V1)\n",
    "print(W)\n",
    "O = W @ V1\n",
    "print(O,O.norm())\n",
    "optimizer = optim.Adam({W}, lr=1e-4)\n",
    "    \n",
    "i = 0\n",
    "while True:\n",
    "    i += 1\n",
    "    optimizer.zero_grad()\n",
    "    O = W @ V1\n",
    "    loss = O.norm() \n",
    "    if i % 1000 == 0:\n",
    "        print(i,loss)\n",
    "    if(loss < 0.0012):\n",
    "        break\n",
    "    loss.backward()                                      \n",
    "    optimizer.step()\n",
    "print(i,W,O)\n",
    "V2 = torch.tensor([0,1,0], requires_grad=False, dtype=dtype, device=device)\n",
    "i = 0\n",
    "while True:\n",
    "    i += 1\n",
    "    optimizer.zero_grad()\n",
    "    O = W @ V2\n",
    "    loss = O.norm() \n",
    "    if i % 1000 == 0:\n",
    "        print(i,loss)\n",
    "    if(loss < 0.0012):\n",
    "        break\n",
    "    loss.backward()                                      \n",
    "    optimizer.step()\n",
    "print(W@V1,W@V2)\n",
    "print(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ortho = Orthogonalizer(3)\n",
    "V1 = torch.tensor([0.5,0,0.1], requires_grad=False, dtype=dtype, device=device).t()\n",
    "V2 = torch.tensor([1,0,1], requires_grad=False, dtype=dtype, device=device).t()\n",
    "V3 = torch.tensor([1,0.5,1], requires_grad=False, dtype=dtype, device=device).t()\n",
    "print(\"n v1\",ortho.novelty(V1))\n",
    "print(ortho.O)\n",
    "print(\"n v1\",ortho.novelty(V1))\n",
    "print(\"n v2\",ortho.novelty(V2))\n",
    "print(\"V1\",V1)\n",
    "print(\"add V1\")\n",
    "ortho.addBasis(V1)\n",
    "print(ortho.O)\n",
    "print(\"p v1\",ortho.project(V1))\n",
    "print(\"n v1\",ortho.novelty(V1))\n",
    "print(\"n v2\",ortho.novelty(V2))\n",
    "print(\"add V1\")\n",
    "ortho.addBasis(V1)\n",
    "print(ortho.novelty(V1))\n",
    "print(\"add V1\")\n",
    "ortho.addBasis(V1)\n",
    "print(ortho.novelty(V1))\n",
    "print(\"add V1\")\n",
    "ortho.addBasis(V1)\n",
    "print(ortho.novelty(V1))\n",
    "print(\"add V1\")\n",
    "ortho.addBasis(V1)\n",
    "print(ortho.novelty(V1))\n",
    "print(\"add V1\")\n",
    "ortho.addBasis(V1)\n",
    "print(\"O\",ortho.O)\n",
    "print(ortho.novelty(V1))\n",
    "print(ortho.novelty(V2))\n",
    "print(ortho.novelty(V3))\n",
    "print(ortho.project(V1))\n",
    "print(ortho.project(V2))\n",
    "print(ortho.project(V3))\n",
    "\n",
    "ortho.addBasis(V2)\n",
    "print(ortho.novelty(V1))\n",
    "print(ortho.novelty(V2))\n",
    "print(ortho.novelty(V3))\n",
    "print(ortho.project(V1))\n",
    "print(ortho.project(V2))\n",
    "print(ortho.project(V3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ortho.addBasis(V1)\n",
    "print(ortho.novelty(V1))\n",
    "print(ortho.novelty(V2))\n",
    "print(ortho.novelty(V3))\n",
    "ortho.addBasis(V2)\n",
    "print(ortho.novelty(V1))\n",
    "print(ortho.novelty(V2))\n",
    "print(ortho.novelty(V3))\n",
    "ortho.addBasis(V3)\n",
    "print(ortho.novelty(V1))\n",
    "print(ortho.novelty(V2))\n",
    "print(ortho.novelty(V3))\n",
    "print(ortho.O)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(V1,V1.t())\n",
    "print(ortho.O)\n",
    "print(ortho.project(V1))\n",
    "print(ortho.project(V2))\n",
    "ortho.addBasis(V1)\n",
    "print(ortho.O)\n",
    "print(ortho.project(V1))\n",
    "print(ortho.project(V2))\n",
    "ortho.addBasis(V2)\n",
    "print(ortho.O)\n",
    "print(ortho.project(V1))\n",
    "print(ortho.project(V2))\n",
    "ortho.addBasis(V2)\n",
    "print(ortho.O)\n",
    "print(ortho.project(V1))\n",
    "print(ortho.project(V2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ortho = Orthogonalizer(3)\n",
    "V1 = torch.tensor([[0.5,0.25,0.25]], requires_grad=False, dtype=dtype, device=device).t()\n",
    "print(\"V1.p\",ortho.project(V1))\n",
    "print(\"V1.n\",ortho.novelty(V1))\n",
    "print(ortho.O)\n",
    "ortho.addBasis(V1)\n",
    "print(ortho.O)\n",
    "print(\"V1.p\",ortho.project(V1))\n",
    "print(\"V1.n\",ortho.novelty(V1))\n",
    "ortho.addBasis(V1)\n",
    "print(ortho.O)\n",
    "print(\"V1.p\",ortho.project(V1))\n",
    "print(\"V1.n\",ortho.novelty(V1))\n",
    "ortho.addBasis(V1)\n",
    "print(ortho.O)\n",
    "print(\"V1.p\",ortho.project(V1))\n",
    "print(\"V1.n\",ortho.novelty(V1))\n",
    "ortho.addBasis(V1)\n",
    "print(ortho.O)\n",
    "print(\"V1.p\",ortho.project(V1))\n",
    "print(\"V1.n\",ortho.novelty(V1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V2 = torch.tensor([[1,3,4]], requires_grad=False, dtype=dtype, device=device).t()\n",
    "print(\"x\",ortho.novelty(V2))\n",
    "ortho.addBasis(V2)\n",
    "print(ortho.project(V2))\n",
    "ortho.addBasis(V2)\n",
    "print(ortho.project(V2))\n",
    "ortho.addBasis(V2)\n",
    "print(ortho.project(V2))\n",
    "ortho.addBasis(V2)\n",
    "print(ortho.project(V2))\n",
    "print(ortho.project(V1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### gram-schmidt orthogonalization. Add a new basis one column at a time.\n",
    "class Orthogonalizer:\n",
    "    def __init__(self,size,thresh=1e-10):\n",
    "        self.O = torch.eye(size, requires_grad=False, dtype=torch.float)\n",
    "        self.O2 = torch.eye(size, requires_grad=False, dtype=torch.float)\n",
    "        self.threshold = thresh\n",
    "        return\n",
    "    def novelty(self,X):\n",
    "        return torch.mm(self.O,X)\n",
    "    def project(self,X):\n",
    "        return X - self.novelty(X)\n",
    "    def addBasis(self,X):\n",
    "        for x in X.t():\n",
    "            a = self.novelty(x.unsqueeze(0).t())\n",
    "            if (torch.abs(a).max() > self.threshold):\n",
    "                self.O -= torch.div(torch.mm(a,a.t()),torch.norm(a).pow(2))\n",
    "        return\n",
    "    \n",
    "#gram-schmidt orthogonalization. Add a new basis one column at a time.\n",
    "class Orthogonalizer2:\n",
    "    def __init__(self,size,thresh=1e-10):\n",
    "        self.O = torch.eye(size, requires_grad=False, dtype=torch.float)\n",
    "        self.O2 = torch.eye(size, requires_grad=False, dtype=torch.float)\n",
    "        self.threshold = thresh\n",
    "        return\n",
    "    def novelty(self,X):\n",
    "        return torch.mm(self.O,X)\n",
    "    def project(self,X):\n",
    "        return torch.div(torch.mm(self.O,X),self.O)\n",
    "    def addBasis(self,X):\n",
    "        for x in X.t():\n",
    "            a = self.novelty(x.unsqueeze(0).t())\n",
    "            if (torch.abs(a).max() > self.threshold):\n",
    "                self.O -= torch.div(torch.mm(a.t(),a),torch.norm(a).pow(2))\n",
    "        return\n",
    "    \n",
    "class Forgetful_Orthogonalizer(Orthogonalizer):\n",
    "    def addBasis(self,X):\n",
    "        #Reduce magnitude of prior orthoganlizations \n",
    "        super(Forgetful_Orthogonalizer, self).addBasis(X)\n",
    "        self.O = torch.tanh(self.O - self.O2) + self.O2\n",
    "        return\n",
    "    \n",
    "def compare(V,ortho,fortho):\n",
    "    ortho.addBasis(V)\n",
    "    fortho.addBasis(V)\n",
    "    print('Value {}\\tNovelty: {}\\tF Novelty: {}\\tProjection: {}\\tF Projection {}'.format(\n",
    "        V, ortho.novelty(V),fortho.novelty(V),ortho.project(V),fortho.project(V)))\n",
    "def compare2(V,ortho):\n",
    "    ortho.addBasis(V)\n",
    "    Q,_ = np.linalg.qr(V)\n",
    "    print('\\tO {}\\t Q P{}\\tValue {}\\tNovelty: {}\\tProjection: {}'.format(\n",
    "        ortho.O,Q,V, ortho.novelty(V),ortho.project(V),fortho.project(V)))\n",
    "\n",
    "def make_householder(a):\n",
    "    v = a / (a[0] + np.copysign(np.linalg.norm(a), a[0]))\n",
    "    v[0] = 1\n",
    "    H = np.eye(a.shape[0])\n",
    "    H -= (2 / np.dot(v, v)) * np.dot(v[:, None], v[None, :])\n",
    "    return H\n",
    "def householder_v(a):\n",
    "    \"\"\"Use this version of householder to reproduce the output of np.linalg.qr \n",
    "    exactly (specifically, to match the sign convention it uses)\n",
    "\n",
    "    based on https://rosettacode.org/wiki/QR_decomposition#Python\n",
    "    \"\"\"\n",
    "    v = a / (a[0] + np.copysign(np.linalg.norm(a), a[0]))\n",
    "    v[0] = 1\n",
    "    tau = 2 / (v.T @ v)\n",
    "\n",
    "    return v,tau\n",
    "    \n",
    "ortho = Orthogonalizer(3)\n",
    "fortho = Forgetful_Orthogonalizer(3)\n",
    "V1 = torch.tensor([[0.5,0.25,0.25]], requires_grad=False, dtype=torch.float).t()\n",
    "print(\"x\")\n",
    "print(ortho.project(V1))\n",
    "print(ortho.novelty(V1))\n",
    "ortho.addBasis(V1)\n",
    "print(ortho.project(V1))\n",
    "ortho.addBasis(V1)\n",
    "print(ortho.project(V1))\n",
    "ortho.addBasis(V1)\n",
    "print(ortho.project(V1))\n",
    "ortho.addBasis(V1)\n",
    "print(ortho.project(V1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V1 = torch.tensor([[0.5,0.25,0.25]], requires_grad=False, dtype=torch.float).t()\n",
    "V2 = torch.tensor([[2,3,4]], requires_grad=False, dtype=torch.float).t()\n",
    "Q,r = np.linalg.qr(V1)\n",
    "print(\"Qr\",np.linalg.qr(V1))\n",
    "print(\"householder\",householder_v(V1.numpy()))\n",
    "# print(\"householder2\",make_householder(V1.numpy()))\n",
    "h,r2 = householder_v(V1.numpy())\n",
    "Q = torch.tensor(Q)\n",
    "print(\"Qt.Q\",Q @ Q.t())\n",
    "print(\"Q.Qt\",torch.mm(Q,Q.t()))\n",
    "print(\"Qt.Q\",torch.mm(Q.t(),Q))\n",
    "print(Q @ Q.t() @ V1)#Projection from Q\n",
    "print(h @ h.T @ V1.numpy())#Projection from h\n",
    "print(V1 - torch.mm(torch.mm(Q,Q.t()),V1))#Novelty from Q\n",
    "print(\"\")\n",
    "\n",
    "\n",
    "Q,r = np.linalg.qr(V2)\n",
    "Q = torch.tensor(Q)\n",
    "print(\"Q\",Q)\n",
    "print(\"Q.Qt\",torch.mm(Q,Q.t()))\n",
    "print(\"Qt.Q\",torch.mm(Q.t(),Q))\n",
    "print(torch.mm(torch.mm(Q,Q.t()),V2))#Projection from Q\n",
    "print(V2 - torch.mm(torch.mm(Q,Q.t()),V2))#Novelty from Q\n",
    "print(\"\")\n",
    "\n",
    "\n",
    "print(torch.mm(torch.mm(Q,Q.t()),V1))#Projection from Q\n",
    "print(V1 - torch.mm(torch.mm(Q,Q.t()),V1))#Novelty from Q\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B: tensor([[3., 1., 3., 3., 3., 3.],\n",
    "        [3., 3., 6., 4., 4., 4.],\n",
    "        [4., 3., 5., 5., 5., 5.]]), projection: tensor([[5.9071, 2.0330, 7.4282, 5.7807, 5.7807, 5.7807],\n",
    "        [4.1150, 2.9704, 7.2674, 4.8306, 4.8306, 4.8306],\n",
    "        [1.4824, 1.9224, 1.4342, 2.4399, 2.4399, 2.4399]])\n",
    "\n",
    "def gram_schmidt(A):\n",
    "    \"\"\"Orthogonalize a set of vectors stored as the columns of matrix A.\"\"\"\n",
    "    # Get the number of vectors.\n",
    "    n = A.shape[1]\n",
    "    for j in range(n):\n",
    "        # To orthogonalize the vector in column j with respect to the\n",
    "        # previous vectors, subtract from it its projection onto the\n",
    "        # each of the previous vectors.\n",
    "        for k in range(j):\n",
    "            A[:, j] -= np.dot(A[:, k], A[:, j]) * A[:, k]\n",
    "        A[:, j] = A[:, j] / np.linalg.norm(A[:, j])\n",
    "    return A\n",
    "A = np.array([[1.0, 1.0, 0.0], [1.0, 3.0, 1.0], [2.0, -1.0, 1.0]])\n",
    "# print(gram_schmidt(A))\n",
    "\n",
    "def gram_schmidt_columns(X):\n",
    "    Q, R = np.linalg.qr(X)\n",
    "    return Q,R\n",
    "Q,R = gram_schmidt_columns(V4)\n",
    "print(\"zz\",V4.numpy().dot(R.T))\n",
    "print(V4)\n",
    "print('sss',gram_schmidt_columns(V4))\n",
    "z = Orthogonalizer(3)\n",
    "z.addBasis(V4)\n",
    "print(Q.dot(Q.T).dot(V4))\n",
    "print(V4.numpy()-Q.dot(Q.T).dot(V4))\n",
    "print(z.novelty(V4))\n",
    "print(z.project(V4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5\n",
      "tensor([0.]) tensor([1.])\n",
      "tensor([0.2000]) tensor([0.])\n",
      "tensor([0.4000]) tensor([0.])\n",
      "tensor([0.6000]) tensor([0.])\n",
      "tensor([0.8000]) tensor([0.])\n",
      "tensor([1.]) tensor([0.])\n"
     ]
    }
   ],
   "source": [
    "eps = 1\n",
    "\n",
    "#            certainty_factor = self.alpha/torch.exp(abs(self.loss))\n",
    "s = [0,1,2,3,4,5]\n",
    "print(np.mean(s))\n",
    "for i in range(6):\n",
    "    v = i /5\n",
    "    X = torch.tensor([v], requires_grad=False, dtype=dtype, device=device)\n",
    "    print(X,1./torch.exp(X*4/0.0001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
