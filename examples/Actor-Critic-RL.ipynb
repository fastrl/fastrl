{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import gym\n",
    "import numpy as np\n",
    "from itertools import count\n",
    "from collections import namedtuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Published solutions predict a count(actions) vector with each value being the value of taking that action. this is equivalent to a state value but duplicate each state by the number of actions that can achieve it. Is that Q-Learning????\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='PyTorch actor-critic example')\n",
    "parser.add_argument('--gamma', type=float, default=0.999, metavar='G', help='discount factor (default: 0.99)')\n",
    "parser.add_argument('--seed', type=int, default=543, metavar='N', help='random seed (default: 543)')\n",
    "# parser.add_argument('--render', action='store_true', help='render the environment')\n",
    "parser.add_argument('--render', type=bool,default=False, help='render the environment')\n",
    "parser.add_argument('--log-interval', type=int, default=10, metavar='N', help='interval between training status logs (default: 10)')\n",
    "parser.add_argument('-f','--file',help='Path for input file. (Dummy arg to enable execution in notebook.)' )\n",
    "args = parser.parse_args() \n",
    "torch.manual_seed(args.seed)\n",
    "\n",
    "#Get the smallest possible non-zero number on this machine\n",
    "eps = np.finfo(np.float32).eps.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class World():\n",
    "    def __init__(self):\n",
    "        self.env = gym.make('CartPole-v0')\n",
    "        self.env.seed(args.seed)\n",
    "        self.reward = 0.0\n",
    "        self.done = False\n",
    "        self.reset()\n",
    "    def reset(self):\n",
    "        self.state = torch.tensor(self.env.reset(), requires_grad=False, dtype=torch.float)\n",
    "    def action_count(self):\n",
    "        return self.env.action_space.n\n",
    "    def world_dimensions(self):\n",
    "        return self.env.observation_space.shape[0]\n",
    "    def step(self,action):\n",
    "        self.state, self.reward, self.done, _ = self.env.step(action.item())\n",
    "        self.state = torch.tensor(self.state, requires_grad=False, dtype=torch.float)\n",
    "        if args.render:\n",
    "            self.env.render()\n",
    "        \n",
    "class Actor(nn.Module):\n",
    "    \n",
    "    def __init__(self,world: World):\n",
    "        super(Actor, self).__init__()\n",
    "        self.l1 = nn.Linear(world.world_dimensions(),48)\n",
    "        self.l2 = nn.Linear(48,24)\n",
    "        self.head = nn.Linear(24, world.action_count())\n",
    "\n",
    "    def forward(self, state):\n",
    "        a1 = F.softplus(self.l1(state))\n",
    "        a2 = F.softplus(self.l2(a1))\n",
    "        head = self.head(a2)\n",
    "        action_scores = F.softmax(head, dim=-1)\n",
    "        return action_scores\n",
    "    \n",
    "    def choose_action(self,scores):\n",
    "        self.categories = Categorical(scores)\n",
    "        self.action = self.categories.sample()\n",
    "        return self.action\n",
    "    \n",
    "    def advantage_loss(self,critic,world):\n",
    "        advantage = world.reward + critic.value.data * args.gamma - critic.prev_value.data\n",
    "        return self.categories.log_prob(self.action)*advantage\n",
    "    \n",
    "class Critic(nn.Module):\n",
    "    \n",
    "    def __init__(self,world: World):\n",
    "        super(Critic, self).__init__()\n",
    "        self.l1 = nn.Linear(world.world_dimensions(),48)\n",
    "        self.l2 = nn.Linear(48,24)\n",
    "        self.head = nn.Linear(24, 1)\n",
    "        self.value = 10.\n",
    "\n",
    "    def forward(self, state):\n",
    "        self.prev_value = self.value\n",
    "        a1 = F.softplus(self.l1(state))\n",
    "        a2 = F.softplus(self.l2(a1))\n",
    "        self.value = self.head(a2)\n",
    "        return self.value\n",
    "    \n",
    "    def td_loss(self,world):\n",
    "        hindsight_value = world.reward + self.value.data * args.gamma\n",
    "        return hindsight_value - self.prev_value\n",
    "\n",
    "world = World()\n",
    "actor = Actor(world)\n",
    "critic = Critic(world)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-b2259a742057>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     36\u001b[0m             \u001b[0mtotal_r\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"DONE\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-36-b2259a742057>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mcritic_optimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mvalue_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcritic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtd_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mworld\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[0mvalue_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m         \u001b[0mcritic_optimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\fastrl\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m         \"\"\"\n\u001b[1;32m--> 107\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\fastrl\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time."
     ]
    }
   ],
   "source": [
    "args.render = False\n",
    "def main():\n",
    "    action_optimizer = optim.Adagrad(actor.parameters(), lr=5e-3,lr_decay= 0.0,weight_decay=0.0001)\n",
    "    critic_optimizer = optim.Adagrad(critic.parameters(), lr=5e-3,lr_decay= 0.0,weight_decay=0.0001)\n",
    "    value = critic(world.state)\n",
    "    total_r = 0.\n",
    "    for i in range(10000):\n",
    "        action_scores = actor(world.state)\n",
    "        action = actor.choose_action(action_scores)\n",
    "        world.step(action)\n",
    "        total_r += world.reward\n",
    "        critic(world.state)\n",
    "        \n",
    "        critic_optimizer.zero_grad()\n",
    "        value_loss = critic.td_loss(world)\n",
    "        value_loss.backward()                                      \n",
    "        critic_optimizer.step()\n",
    "        \n",
    "        action_optimizer.zero_grad() \n",
    "        action_loss = actor.advantage_loss(critic,world)\n",
    "        action_loss.backward()                                      \n",
    "        action_optimizer.step()\n",
    "        if world.done:\n",
    "            if (total_r < 30):\n",
    "                args.render = False\n",
    "            if (total_r > 120):\n",
    "                args.render = True\n",
    "            print(total_r, \" \", end =\"\")\n",
    "            world.reset()\n",
    "            value = critic(world.state)\n",
    "            total_r = 0.\n",
    "    print(\"DONE\")\n",
    "main()      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
