{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "#torch.set_default_tensor_type('torch.cuda.FloatTensor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1b1c8e7a710>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description='PyTorch actor-critic example')\n",
    "parser.add_argument('--gamma', type=float, default=0.99, metavar='G', help='discount factor (default: 0.99)')\n",
    "parser.add_argument('--seed', type=int, default=543, metavar='N', help='random seed (default: 543)')\n",
    "# parser.add_argument('--render', action='store_true', help='render the environment')\n",
    "parser.add_argument('--render', type=bool,default=False, help='render the environment')\n",
    "parser.add_argument('--trace', type=bool,default=False, help='render the environment')\n",
    "parser.add_argument('--log-interval', type=int, default=100, metavar='N', help='interval between training status logs (default: 10)')\n",
    "parser.add_argument('-f','--file',help='Path for input file. (Dummy arg to enable execution in notebook.)' )\n",
    "args = parser.parse_args() \n",
    "torch.manual_seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class World():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.env = gym.make('CartPole-v0')\n",
    "        self.env.seed(args.seed)\n",
    "        self.reward = 0.0\n",
    "        self.done = False\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.state = torch.tensor(self.env.reset(), requires_grad=False, dtype=torch.float)\n",
    "        \n",
    "    def action_count(self):\n",
    "        return self.env.action_space.n\n",
    "    \n",
    "    def world_dimensions(self):\n",
    "        return self.env.observation_space.shape[0]\n",
    "    \n",
    "    def step(self,action):\n",
    "        self.state, self.reward, self.done, _ = self.env.step(action.item())\n",
    "        self.state = torch.tensor(self.state, requires_grad=False, dtype=torch.float)\n",
    "        if args.render: self.env.render()\n",
    "        \n",
    "class Actor(nn.Module):\n",
    "    \n",
    "    def __init__(self,world: World):\n",
    "        super(Actor, self).__init__()\n",
    "        self.l1 = nn.Linear(world.world_dimensions(),128 - world.world_dimensions())\n",
    "        self.head = nn.Linear(128, world.action_count())\n",
    "\n",
    "    def forward(self, state):\n",
    "        a1 = F.softplus(self.l1(state))\n",
    "        #Adding bypass connections\n",
    "        head = self.head(torch.cat([a1,state]))\n",
    "        action_scores = F.softmax(head, dim=-1)\n",
    "        if(args.trace): print(\"action scores:\",action_scores)\n",
    "        return action_scores\n",
    "    \n",
    "    def choose_action(self,scores):\n",
    "        self.categories = Categorical(scores)\n",
    "        self.action = self.categories.sample()\n",
    "        if(args.trace): print(\"Action:\",self.action.item())\n",
    "        return self.action\n",
    "\n",
    "#The \"advantage\" is how much better the state is after the action than we expected it would be\n",
    "    def advantage_loss(self,critic,world):\n",
    "        #Do not include gradient of prev_value here, just the data.\n",
    "        advantage = critic.hindsight_value(world) - critic.prev_value.data\n",
    "        if(args.trace): print(\"advantage loss:\",-self.categories.log_prob(self.action)*advantage)\n",
    "        return -self.categories.log_prob(self.action)*advantage\n",
    "    \n",
    "class Critic(nn.Module):\n",
    "    \n",
    "    def __init__(self,world: World):\n",
    "        super(Critic, self).__init__()\n",
    "        self.l1 = nn.Linear(world.world_dimensions(),128 - world.world_dimensions())\n",
    "        self.head = nn.Linear(128, 1)\n",
    "        self.one = torch.ones([1], requires_grad=False, dtype=torch.float)\n",
    "        self.value = 0.\n",
    "        \n",
    "    def forward(self, state):\n",
    "        self.prev_value = self.value\n",
    "        a1 = F.softplus(self.l1(state))\n",
    "        #Adding bypass connections\n",
    "        self.value = self.head(torch.cat([a1,state]))\n",
    "        return self.value\n",
    "    \n",
    "#What the previous value should have been knowing what we know after the last state transition\n",
    "    def hindsight_value(self,world):\n",
    "        #Do not include gradient of the critic value here, just the data.\n",
    "        return world.reward * self.one if world.done else world.reward + (args.gamma * self.value.data)\n",
    "         \n",
    "#Temporal Difference Loss is for the previous state!\n",
    "    def td_loss(self,world):\n",
    "        loss = F.mse_loss(self.prev_value,self.hindsight_value(world))\n",
    "        if(args.trace): print(\"Critic value and loss:\",self.prev_value,loss)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(episodes=1000):\n",
    "\n",
    "    world.reset()\n",
    "    critic(world.state)\n",
    "    mave_reward = 10\n",
    "    for i_episode in range(1,episodes+1):\n",
    "        ep_reward = 0\n",
    "        for t in range(1000):\n",
    "            action_scores = actor(world.state)\n",
    "            action = actor.choose_action(action_scores)\n",
    "            world.step(action)\n",
    "            \n",
    "            ep_reward += world.reward\n",
    "            critic(world.state)\n",
    "\n",
    "            critic_optimizer.zero_grad()\n",
    "            critic_loss = critic.td_loss(world)\n",
    "            critic_loss.backward()                                      \n",
    "            critic_optimizer.step()\n",
    "\n",
    "            actor_optimizer.zero_grad() \n",
    "            actor_loss = actor.advantage_loss(critic,world)\n",
    "            actor_loss.backward()                                      \n",
    "            actor_optimizer.step()\n",
    "            if(world.done): break\n",
    "\n",
    "        mave_reward = 0.05 * ep_reward + (1 - 0.05) * mave_reward\n",
    "        if i_episode % args.log_interval == 0:\n",
    "            print('Episode {}\\tLast reward: {:.2f}\\tMoving average reward: {:.2f}'.format(\n",
    "                  i_episode, ep_reward, mave_reward))\n",
    "        I = 1.           \n",
    "        world.reset()\n",
    "        critic(world.state)\n",
    "        if mave_reward > world.env.spec.reward_threshold:\n",
    "            print(\"Solved! Running reward is now {} and \"\n",
    "                  \"the last episode runs to {} time steps!\".format(mave_reward, t+1))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_trainer():\n",
    "    args.trace = False\n",
    "    args.render = False\n",
    "    global world\n",
    "    global actor\n",
    "    global critic\n",
    "    global actor_optimizer\n",
    "    global critic_optimizer\n",
    "    world = World()\n",
    "    actor = Actor(world)\n",
    "    critic = Critic(world)\n",
    "    actor_optimizer = optim.Adam(actor.parameters(), lr=2.7e-4,weight_decay=0.0001)\n",
    "    critic_optimizer = optim.Adam(critic.parameters(), lr=4.6e-3,weight_decay=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_trainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tLast reward: 13.00\tMoving average reward: 13.19\n",
      "Episode 200\tLast reward: 20.00\tMoving average reward: 18.58\n",
      "Episode 300\tLast reward: 15.00\tMoving average reward: 20.24\n",
      "Episode 400\tLast reward: 92.00\tMoving average reward: 30.81\n",
      "Episode 500\tLast reward: 18.00\tMoving average reward: 38.63\n",
      "Episode 600\tLast reward: 58.00\tMoving average reward: 53.36\n",
      "Episode 700\tLast reward: 48.00\tMoving average reward: 62.08\n",
      "Episode 800\tLast reward: 12.00\tMoving average reward: 46.84\n",
      "Episode 900\tLast reward: 45.00\tMoving average reward: 77.46\n",
      "Episode 1000\tLast reward: 200.00\tMoving average reward: 120.95\n"
     ]
    }
   ],
   "source": [
    "args.log_interval = 100\n",
    "args.trace = False\n",
    "train(1000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 10\tLast reward: 140.00\tMoving average reward: 70.68\n",
      "Episode 20\tLast reward: 200.00\tMoving average reward: 79.03\n",
      "Episode 30\tLast reward: 135.00\tMoving average reward: 124.32\n",
      "Episode 40\tLast reward: 200.00\tMoving average reward: 147.24\n",
      "Episode 50\tLast reward: 200.00\tMoving average reward: 124.46\n",
      "Episode 60\tLast reward: 197.00\tMoving average reward: 154.62\n",
      "Episode 70\tLast reward: 200.00\tMoving average reward: 161.45\n",
      "Episode 80\tLast reward: 15.00\tMoving average reward: 112.55\n",
      "Episode 90\tLast reward: 200.00\tMoving average reward: 105.92\n",
      "Episode 100\tLast reward: 196.00\tMoving average reward: 133.45\n",
      "critic prev_value, value loss, advantage loss\n",
      "tensor([42.2285], grad_fn=<AddBackward0>) tensor(1699.7902, grad_fn=<MseLossBackward>) tensor([-14.9868], grad_fn=<MulBackward0>)\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "args.log_interval = 10\n",
    "args.render = True\n",
    "args.trace = False\n",
    "train(100) \n",
    "print(\"critic prev_value, value loss, advantage loss\")\n",
    "print(critic.prev_value,critic.td_loss(world),actor.advantage_loss(critic,world))\n",
    "print(world.done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
