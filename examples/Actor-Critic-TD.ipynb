{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "cuda not used\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "print(torch.cuda.is_available())\n",
    "if torch.cuda.is_available() and False:\n",
    "    print (\"cuda in use\")\n",
    "    device = torch.device('cuda') \n",
    "    torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "else:\n",
    "    print (\"cuda not used\")\n",
    "    device = torch.device('cpu')\n",
    "    torch.set_default_tensor_type('torch.FloatTensor')\n",
    "    dtype = torch.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='PyTorch actor-critic example')\n",
    "parser.add_argument('--gamma', type=float, default=0.99, metavar='G', help='discount factor (default: 0.99)')\n",
    "parser.add_argument('--seed', type=int, default=543, metavar='N', help='random seed (default: 543)')\n",
    "# parser.add_argument('--render', action='store_true', help='render the environment')\n",
    "parser.add_argument('--render', type=bool,default=False, help='render the environment')\n",
    "parser.add_argument('--trace', type=bool,default=False, help='render the environment')\n",
    "parser.add_argument('--log-interval', type=int, default=100, metavar='N', help='interval between training status logs (default: 10)')\n",
    "parser.add_argument('-f','--file',help='Path for input file. (Dummy arg to enable execution in notebook.)' )\n",
    "args = parser.parse_args() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class World():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.env = gym.make('CartPole-v0')\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.reward = 0.0\n",
    "        self.done = False\n",
    "        self.state = torch.tensor(self.env.reset(), requires_grad=False, dtype=dtype, device=device)\n",
    "        \n",
    "    def action_count(self):\n",
    "        return self.env.action_space.n\n",
    "    \n",
    "    def dimension_count(self):\n",
    "        return self.env.observation_space.shape[0]\n",
    "    \n",
    "    def step(self,action):\n",
    "        self.state, self.reward, self.done, _ = self.env.step(action.item())\n",
    "        self.state = torch.tensor(self.state, requires_grad=False, dtype=dtype, device=device)\n",
    "        if args.render: self.env.render()\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    \n",
    "    def __init__(self,world: World,hidden_nodes=32):\n",
    "        super(Critic, self).__init__()\n",
    "        self.world = world\n",
    "        self.one = torch.ones([1], requires_grad=False, dtype=dtype, device=device)\n",
    "        self.zero = torch.zeros([1], requires_grad=False, dtype=dtype, device=device)\n",
    "        self.l1 = nn.Linear(world.dimension_count(),hidden_nodes)\n",
    "        self.head = nn.Linear(hidden_nodes, 1)\n",
    "        self.prev_value = self.zero\n",
    "        self.value = self.zero\n",
    "        \n",
    "    def forward(self, state):\n",
    "        self.prev_value = self.value\n",
    "        self.l1_out = F.relu(self.l1(state))\n",
    "        self.value = self.head(self.l1_out)\n",
    "        return self.value\n",
    "    \n",
    "    #What the previous value should have been knowing what we know after the last state transition\n",
    "    def hindsight_value(self):\n",
    "        #Do not include gradient of the critic value here, just the data.\n",
    "        return self.world.reward * self.zero if self.world.done else self.world.reward + args.gamma * self.value.data\n",
    "         \n",
    "    #Temporal Difference Loss is for the previous state!\n",
    "    def get_loss(self):\n",
    "        self.loss = F.mse_loss(self.prev_value,self.hindsight_value())\n",
    "        if args.trace: print(\"Critic value and loss:\",self.prev_value,self.loss)\n",
    "        return self.loss\n",
    "    \n",
    "class Actor(nn.Module):\n",
    "    \n",
    "    def __init__(self, critic: Critic,hidden_nodes=64):\n",
    "        super(Actor, self).__init__()  \n",
    "        self.critic = critic\n",
    "        self.l1_zeros = torch.zeros([critic.world.dimension_count()], requires_grad=False, dtype=dtype, device=device)\n",
    "        self.l1 = nn.Linear(critic.world.dimension_count(),hidden_nodes)\n",
    "        self.head = nn.Linear(hidden_nodes, critic.world.action_count())\n",
    "                \n",
    "    def forward(self, state):\n",
    "        self.l1_out = F.relu(self.l1(state))\n",
    "        self.value = F.softmax(self.head(self.l1_out),dim=0)\n",
    "        return self.value\n",
    "    \n",
    "    def randomize(self):\n",
    "        self.l1_out = self.l1_zeros\n",
    "        self.value = F.softmax(torch.rand([self.head.out_features], requires_grad=False, dtype=dtype, device=device), dim=0)\n",
    "        return self.value\n",
    "    \n",
    "    def choose_action(self):\n",
    "        self.categories = Categorical(self.value)\n",
    "        self.action = self.categories.sample()\n",
    "        if args.trace: print(\"action scores:\",self.categories.probs,\"Action:\",self.action.item())\n",
    "        return self.action\n",
    "\n",
    "    #The \"advantage\" is how much better the state is after the action than we expected it would be\n",
    "    def get_loss(self):\n",
    "        #Do not include gradient from critic.\n",
    "        advantage = self.critic.hindsight_value() - self.critic.prev_value.data\n",
    "        self.loss = -self.categories.log_prob(self.action)*advantage\n",
    "        if args.trace: print(\"actor loss:\", self.loss)\n",
    "        return self.loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(episodes=1000):\n",
    "    mave_reward = 10\n",
    "    mave_value = 10.\n",
    "    action_preferences = np.array([0.5,0.5])\n",
    "    \n",
    "    for i_episode in range(1,episodes+1):\n",
    "        ep_reward = 0\n",
    "        ep_value = 0.\n",
    "        ep_action_preferences = np.array([0.,0.])\n",
    "        world.reset()\n",
    "        critic.forward(world.state)       \n",
    "        moves = 0\n",
    "        \n",
    "        for t in range(10000):\n",
    "            \n",
    "            #Take an action and evaluate it.\n",
    "            actor.forward(world.state)\n",
    "            world.step(actor.choose_action())\n",
    "            critic.forward(world.state)\n",
    "            ep_reward += world.reward\n",
    "            moves += 1\n",
    "            \n",
    "            #Train the critic's value forecast\n",
    "            ep_value += critic.value.item()\n",
    "            critic_optimizer.zero_grad()\n",
    "            loss = critic.get_loss()\n",
    "            loss.backward()\n",
    "            critic_optimizer.step()\n",
    "            \n",
    "            #Train the action policy\n",
    "            ep_action_preferences += actor.categories.probs.detach().cpu().numpy()\n",
    "            actor_optimizer.zero_grad()\n",
    "            loss = actor.get_loss()\n",
    "            loss.backward()    \n",
    "            actor_optimizer.step()\n",
    "            \n",
    "            if(world.done):\n",
    "                if args.trace: print(\"DONE\")\n",
    "                break\n",
    "\n",
    "        ep_action_preferences /= moves\n",
    "        action_preferences =  0.05 * ep_action_preferences + (1 - 0.05) * action_preferences\n",
    "        mave_value /= moves\n",
    "        mave_reward = 0.05 * ep_reward + (1 - 0.05) * mave_reward\n",
    "        mave_value = 0.05 * ep_value + (1 - 0.05) * mave_value\n",
    "        if i_episode % args.log_interval == 0:\n",
    "            print('Episode {}\\tLast reward: {:.2f}\\tMoving average reward: {:.2f}\\tMoving average critic value: {:.2f}\\tAction Preferences: {:.2f},{:.2f}'.format(\n",
    "                  i_episode, ep_reward, mave_reward, mave_value,action_preferences[0],action_preferences[1]))\n",
    "        if mave_reward > world.env.spec.reward_threshold:\n",
    "            print(\"Episode {}\\tSolved! Running reward is now {} and \"\n",
    "                  \"the last episode runs to {} time steps!\".format(i_episode,mave_reward, moves))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pretrain critic on random action policy\n",
    "def prime_critic(episodes=1000):\n",
    "    for i_episode in range(1,episodes+1):\n",
    "        world.reset()\n",
    "        critic.forward(world.state)\n",
    "        for t in range(1000):\n",
    "            actor.randomize()\n",
    "            world.step(actor.choose_action())\n",
    "            critic.forward(world.state)\n",
    "            critic_optimizer.zero_grad()\n",
    "            critic.get_loss().backward()                          \n",
    "            critic_optimizer.step()\n",
    "            if(world.done):\n",
    "                break\n",
    "                \n",
    "#pretrain policy on current critic evaluation\n",
    "def prime_actor(episodes=1000):\n",
    "    for t in range(episodes):\n",
    "        world.reset()\n",
    "        critic.forward(world.state)  \n",
    "        actor.forward(world.state)\n",
    "        world.step(actor.choose_action())\n",
    "        critic.forward(world.state)\n",
    "        actor_optimizer.zero_grad() \n",
    "        actor.get_loss().backward()                                 \n",
    "        actor_optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_trainer():\n",
    "    args.trace = False\n",
    "    args.render = False\n",
    "    global world\n",
    "    global actor\n",
    "    global critic\n",
    "    global actor_optimizer\n",
    "    global critic_optimizer\n",
    "    world = World()\n",
    "    critic = Critic(world,32)\n",
    "    actor = Actor(critic,64)\n",
    "    world.env.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    actor_optimizer = optim.Adam(actor.parameters(), lr=4e-5,weight_decay=0.0001)#lr=4e-5,weight_decay=0.00001)\n",
    "    critic_optimizer = optim.Adam(critic.parameters(), lr=4e-3,weight_decay=0.0001)#lr=5e-3,weight_decay=0.00001)\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New training test\n",
      "Episode 100\tLast reward: 15.00\tMoving average reward: 31.71\tMoving average critic value: 16.67\tAction Preferences: 0.52,0.48\n",
      "Episode 200\tLast reward: 69.00\tMoving average reward: 56.59\tMoving average critic value: 97.64\tAction Preferences: 0.52,0.48\n",
      "Episode 300\tLast reward: 87.00\tMoving average reward: 91.69\tMoving average critic value: 214.65\tAction Preferences: 0.50,0.50\n",
      "Episode 400\tLast reward: 200.00\tMoving average reward: 139.69\tMoving average critic value: 532.99\tAction Preferences: 0.51,0.49\n",
      "Episode 500\tLast reward: 200.00\tMoving average reward: 173.82\tMoving average critic value: 486.32\tAction Preferences: 0.51,0.49\n",
      "Episode 566\tSolved! Running reward is now 195.10305519132086 and the last episode runs to 200 time steps!\n",
      "New training test\n",
      "Episode 100\tLast reward: 20.00\tMoving average reward: 32.54\tMoving average critic value: 21.14\tAction Preferences: 0.53,0.47\n",
      "Episode 200\tLast reward: 32.00\tMoving average reward: 64.51\tMoving average critic value: 60.43\tAction Preferences: 0.51,0.49\n",
      "Episode 300\tLast reward: 122.00\tMoving average reward: 126.72\tMoving average critic value: 333.65\tAction Preferences: 0.50,0.50\n",
      "Episode 400\tLast reward: 21.00\tMoving average reward: 135.43\tMoving average critic value: 83.34\tAction Preferences: 0.53,0.47\n",
      "Episode 500\tLast reward: 200.00\tMoving average reward: 167.70\tMoving average critic value: 641.39\tAction Preferences: 0.51,0.49\n",
      "Episode 600\tLast reward: 200.00\tMoving average reward: 182.83\tMoving average critic value: 639.79\tAction Preferences: 0.50,0.50\n",
      "Episode 700\tLast reward: 200.00\tMoving average reward: 184.45\tMoving average critic value: 624.63\tAction Preferences: 0.49,0.51\n",
      "Episode 780\tSolved! Running reward is now 195.13338319234956 and the last episode runs to 200 time steps!\n",
      "New training test\n",
      "Episode 100\tLast reward: 39.00\tMoving average reward: 27.43\tMoving average critic value: 33.56\tAction Preferences: 0.51,0.49\n",
      "Episode 200\tLast reward: 55.00\tMoving average reward: 44.93\tMoving average critic value: 79.73\tAction Preferences: 0.51,0.49\n",
      "Episode 300\tLast reward: 34.00\tMoving average reward: 58.96\tMoving average critic value: 36.24\tAction Preferences: 0.50,0.50\n",
      "Episode 400\tLast reward: 200.00\tMoving average reward: 100.49\tMoving average critic value: 500.65\tAction Preferences: 0.52,0.48\n",
      "Episode 500\tLast reward: 116.00\tMoving average reward: 167.70\tMoving average critic value: 281.22\tAction Preferences: 0.49,0.51\n",
      "Episode 600\tLast reward: 200.00\tMoving average reward: 180.52\tMoving average critic value: 649.80\tAction Preferences: 0.51,0.49\n",
      "Episode 700\tLast reward: 200.00\tMoving average reward: 183.61\tMoving average critic value: 726.69\tAction Preferences: 0.50,0.50\n",
      "Episode 731\tSolved! Running reward is now 195.03828471897828 and the last episode runs to 200 time steps!\n",
      "New training test\n",
      "Episode 100\tLast reward: 40.00\tMoving average reward: 42.50\tMoving average critic value: 50.78\tAction Preferences: 0.51,0.49\n",
      "Episode 200\tLast reward: 36.00\tMoving average reward: 41.14\tMoving average critic value: 44.57\tAction Preferences: 0.51,0.49\n",
      "Episode 300\tLast reward: 50.00\tMoving average reward: 57.58\tMoving average critic value: 84.22\tAction Preferences: 0.50,0.50\n",
      "Episode 400\tLast reward: 95.00\tMoving average reward: 75.55\tMoving average critic value: 136.50\tAction Preferences: 0.48,0.52\n",
      "Episode 500\tLast reward: 116.00\tMoving average reward: 146.72\tMoving average critic value: 393.26\tAction Preferences: 0.52,0.48\n",
      "Episode 600\tLast reward: 194.00\tMoving average reward: 184.33\tMoving average critic value: 582.27\tAction Preferences: 0.50,0.50\n",
      "Episode 700\tLast reward: 200.00\tMoving average reward: 188.65\tMoving average critic value: 632.35\tAction Preferences: 0.51,0.49\n",
      "Episode 800\tLast reward: 200.00\tMoving average reward: 188.46\tMoving average critic value: 623.78\tAction Preferences: 0.51,0.49\n",
      "Episode 900\tLast reward: 185.00\tMoving average reward: 180.23\tMoving average critic value: 556.03\tAction Preferences: 0.50,0.50\n",
      "Episode 1000\tLast reward: 127.00\tMoving average reward: 189.07\tMoving average critic value: 342.11\tAction Preferences: 0.50,0.50\n",
      "New training test\n",
      "Episode 100\tLast reward: 24.00\tMoving average reward: 40.84\tMoving average critic value: 29.64\tAction Preferences: 0.51,0.49\n",
      "Episode 200\tLast reward: 141.00\tMoving average reward: 87.90\tMoving average critic value: 347.05\tAction Preferences: 0.50,0.50\n",
      "Episode 300\tLast reward: 49.00\tMoving average reward: 150.50\tMoving average critic value: 155.16\tAction Preferences: 0.48,0.52\n",
      "Episode 400\tLast reward: 200.00\tMoving average reward: 185.77\tMoving average critic value: 618.00\tAction Preferences: 0.50,0.50\n",
      "Episode 448\tSolved! Running reward is now 195.1878730472188 and the last episode runs to 200 time steps!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "args.trace = False\n",
    "args.render = False\n",
    "args.log_interval = 100\n",
    "for i in range(5):\n",
    "    print(\"New training test\")\n",
    "    reset_trainer()\n",
    "    prime_critic(1000)\n",
    "    prime_actor(300)\n",
    "    train(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.log_interval = 10\n",
    "args.trace = False\n",
    "args.render = True\n",
    "train(100) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
