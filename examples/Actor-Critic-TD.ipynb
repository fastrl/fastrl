{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "cuda not used\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "print(torch.cuda.is_available())\n",
    "if torch.cuda.is_available() and False:\n",
    "    print (\"cuda in use\")\n",
    "    device = torch.device('cuda') \n",
    "    torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "else:\n",
    "    print (\"cuda not used\")\n",
    "    device = torch.device('cpu')\n",
    "    torch.set_default_tensor_type('torch.FloatTensor')\n",
    "    dtype = torch.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='PyTorch actor-critic example')\n",
    "parser.add_argument('--gamma', type=float, default=0.99, metavar='G', help='discount factor (default: 0.99)')\n",
    "parser.add_argument('--seed', type=int, default=543, metavar='N', help='random seed (default: 543)')\n",
    "# parser.add_argument('--render', action='store_true', help='render the environment')\n",
    "parser.add_argument('--render', type=bool,default=False, help='render the environment')\n",
    "parser.add_argument('--trace', type=bool,default=False, help='render the environment')\n",
    "parser.add_argument('--log-interval', type=int, default=100, metavar='N', help='interval between training status logs (default: 10)')\n",
    "parser.add_argument('-f','--file',help='Path for input file. (Dummy arg to enable execution in notebook.)' )\n",
    "args = parser.parse_args() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class World():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.env = gym.make('CartPole-v0')\n",
    "#         self.env = gym.make('CartPole-v1')\n",
    "#         self.env = gym.make('Acrobot-v1')\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.reward = 0.0\n",
    "        self.done = False\n",
    "        self.actions = 0\n",
    "        self.state = torch.tensor(self.env.reset(), requires_grad=False, dtype=dtype, device=device)\n",
    "        \n",
    "    def action_count(self):\n",
    "        return self.env.action_space.n\n",
    "    \n",
    "    def dimension_count(self):\n",
    "        return self.env.observation_space.shape[0]\n",
    "    \n",
    "    def step(self,action):\n",
    "        self.state, self.reward, self.done, _ = self.env.step(action.item())\n",
    "        self.state = torch.tensor(self.state, requires_grad=False, dtype=dtype, device=device)\n",
    "        self.actions += 1\n",
    "        if args.render: self.env.render()\n",
    "\n",
    "def selu(x):\n",
    "    alpha = 1.6732632423543772848170429916717\n",
    "    scale = 1.0507009873554804934193349852946\n",
    "    return scale * F.elu(x, alpha)\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    \n",
    "    def __init__(self,world: World,hidden_nodes=32):\n",
    "        super(Critic, self).__init__()\n",
    "        self.world = world\n",
    "        self.one = torch.ones([1], requires_grad=False, dtype=dtype, device=device)\n",
    "        self.zero = torch.zeros([1], requires_grad=False, dtype=dtype, device=device)\n",
    "        self.l1 = nn.Linear(world.dimension_count(),hidden_nodes)\n",
    "        self.l1.weight.data.normal_(0.0, np.sqrt(1./(self.world.dimension_count())))\n",
    "        self.head = nn.Linear(hidden_nodes, 1)\n",
    "        self.head.weight.data.normal_(0.0, np.sqrt(1./(hidden_nodes)))\n",
    "        self.prev_value = self.zero\n",
    "        self.value = self.zero\n",
    "        \n",
    "    def forward(self, state):\n",
    "        self.prev_value = self.value\n",
    "        self.l1_out = selu(self.l1(state))\n",
    "        self.value = self.head(self.l1_out)\n",
    "        return self.value\n",
    "    \n",
    "    #What the previous value should have been knowing what we know after the last state transition\n",
    "    def hindsight_value(self):\n",
    "        #Do not include gradient of the critic value here, just the data.\n",
    "        return self.world.reward * self.zero if self.world.done and self.world.actions < 199 else self.world.reward + args.gamma * self.value.data\n",
    "         \n",
    "    #Temporal Difference Loss is for the previous state!\n",
    "    def get_loss(self):\n",
    "        self.loss = F.mse_loss(self.prev_value,self.hindsight_value())\n",
    "        if args.trace: print(\"Critic value and loss:\",self.prev_value,self.loss)\n",
    "        return self.loss\n",
    "    \n",
    "class Actor(nn.Module):\n",
    "    \n",
    "    def __init__(self, critic: Critic,hidden_nodes=64):\n",
    "        super(Actor, self).__init__()  \n",
    "        self.episode_discount = 1.\n",
    "        self.critic = critic\n",
    "        self.l1_zeros = torch.zeros([critic.world.dimension_count()], requires_grad=False, dtype=dtype, device=device)\n",
    "        self.l1 = nn.Linear(critic.world.dimension_count(),hidden_nodes)\n",
    "        self.l1.weight.data.normal_(0.0, np.sqrt(1./(critic.world.dimension_count())))\n",
    "        self.head = nn.Linear(hidden_nodes, critic.world.action_count())\n",
    "        self.head.weight.data.normal_(0.0, np.sqrt(1./(hidden_nodes)))\n",
    "                \n",
    "    def forward(self, state):\n",
    "        self.l1_out = F.selu(self.l1(state))\n",
    "        self.value = F.softmax(selu(self.head(self.l1_out)),dim=0)\n",
    "        return self.value\n",
    "    \n",
    "    def randomize(self):\n",
    "        self.l1_out = self.l1_zeros\n",
    "        self.value = F.softmax(torch.rand([self.head.out_features], requires_grad=False, dtype=dtype, device=device), dim=0)\n",
    "        return self.value\n",
    "    \n",
    "    def choose_action(self):\n",
    "        self.categories = Categorical(self.value)\n",
    "        self.action = self.categories.sample()\n",
    "        if args.trace: print(\"action scores:\",self.categories.probs,\"Action:\",self.action.item())\n",
    "        return self.action\n",
    "\n",
    "    #The \"advantage\" is how much better the state is after the action than we expected it would be\n",
    "    def get_loss(self):\n",
    "        #Do not include gradient from critic.\n",
    "        advantage = self.critic.hindsight_value() - self.critic.prev_value.data\n",
    "        self.loss = -self.categories.log_prob(self.action)*advantage*self.episode_discount\n",
    "        if args.trace: print(\"actor loss:\", self.loss)\n",
    "        return self.loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(episodes=1000):\n",
    "    mave_reward = 10\n",
    "    mave_value = 10.\n",
    "    action_preferences = np.array([0.5,0.5])\n",
    "    \n",
    "    for i_episode in range(1,episodes+1):\n",
    "        ep_reward = 0\n",
    "        ep_value = 0.\n",
    "        ep_action_preferences = np.array([0.,0.])\n",
    "        world.reset()\n",
    "        critic.forward(world.state)     \n",
    "        I = 1.\n",
    "        for moves in range(10000):\n",
    "            \n",
    "            #Take an action and evaluate it.\n",
    "            actor.episode_discount = I\n",
    "            I *= args.gamma\n",
    "            actor.forward(world.state)\n",
    "            world.step(actor.choose_action())\n",
    "            critic.forward(world.state)\n",
    "            ep_reward += world.reward\n",
    "            \n",
    "            #Train the critic's value forecast\n",
    "            ep_value += critic.value.item()\n",
    "            critic_optimizer.zero_grad()\n",
    "            loss = critic.get_loss()\n",
    "            loss.backward()\n",
    "            critic_optimizer.step()\n",
    "            \n",
    "            #Train the action policy\n",
    "            ep_action_preferences += actor.categories.probs.detach().cpu().numpy()\n",
    "            actor_optimizer.zero_grad()\n",
    "            loss = actor.get_loss()\n",
    "            loss.backward()    \n",
    "            actor_optimizer.step()\n",
    "            \n",
    "            if(world.done):\n",
    "                if args.trace: print(\"DONE\")\n",
    "                break\n",
    "\n",
    "        ep_action_preferences /= moves\n",
    "        action_preferences =  0.05 * ep_action_preferences + (1 - 0.05) * action_preferences\n",
    "        mave_value /= moves\n",
    "        mave_reward = 0.05 * ep_reward + (1 - 0.05) * mave_reward\n",
    "        mave_value = 0.05 * ep_value + (1 - 0.05) * mave_value\n",
    "        if i_episode % args.log_interval == 0:\n",
    "            print('Episode {}\\tLast reward: {:.2f}\\tMoving average reward: {:.2f}\\tMoving average critic value: {:.2f}\\tAction Preferences: {:.2f},{:.2f}'.format(\n",
    "                  i_episode, ep_reward, mave_reward, mave_value,action_preferences[0],action_preferences[1]))\n",
    "        if mave_reward > world.env.spec.reward_threshold:\n",
    "            print(\"Episode {}\\tSolved! Running reward is now {} and \"\n",
    "                  \"the last episode runs to {} time steps!\".format(i_episode,mave_reward, moves))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pretrain critic on random action policy\n",
    "def prime_critic(episodes=1000):\n",
    "    for i_episode in range(1,episodes+1):\n",
    "        world.reset()\n",
    "        critic.forward(world.state)\n",
    "        for t in range(1000):\n",
    "            actor.randomize()\n",
    "            world.step(actor.choose_action())\n",
    "            critic.forward(world.state)\n",
    "            critic_optimizer.zero_grad()\n",
    "            critic.get_loss().backward()                          \n",
    "            critic_optimizer.step()\n",
    "            if(world.done):\n",
    "                break\n",
    "                \n",
    "#pretrain policy on current critic evaluation\n",
    "def prime_actor(episodes=1000):\n",
    "    for t in range(episodes):\n",
    "        world.reset()\n",
    "        critic.forward(world.state)  \n",
    "        actor.forward(world.state)\n",
    "        world.step(actor.choose_action())\n",
    "        critic.forward(world.state)\n",
    "        actor_optimizer.zero_grad() \n",
    "        actor.get_loss().backward()                                 \n",
    "        actor_optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_trainer():\n",
    "    args.trace = False\n",
    "    args.render = False\n",
    "    global world\n",
    "    global actor\n",
    "    global critic\n",
    "    global actor_optimizer\n",
    "    global critic_optimizer\n",
    "    world = World()\n",
    "    critic = Critic(world,32)\n",
    "    actor = Actor(critic,64)\n",
    "    world.env.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    actor_optimizer = optim.Adam(actor.parameters(), lr=4e-5,weight_decay=0.01)#lr=4e-5,weight_decay=0.00001)\n",
    "    critic_optimizer = optim.Adam(critic.parameters(), lr=5e-3,weight_decay=0.01)#lr=5e-3,weight_decay=0.00001)\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New training test\n",
      "Episode 100\tLast reward: 28.00\tMoving average reward: 38.37\tMoving average critic value: 36.71\tAction Preferences: 0.52,0.51\n",
      "Episode 200\tLast reward: 39.00\tMoving average reward: 51.81\tMoving average critic value: 57.87\tAction Preferences: 0.50,0.52\n",
      "Episode 300\tLast reward: 80.00\tMoving average reward: 80.67\tMoving average critic value: 177.52\tAction Preferences: 0.51,0.51\n",
      "Episode 400\tLast reward: 200.00\tMoving average reward: 186.24\tMoving average critic value: 978.45\tAction Preferences: 0.50,0.51\n",
      "Episode 500\tLast reward: 190.00\tMoving average reward: 172.87\tMoving average critic value: 769.32\tAction Preferences: 0.51,0.49\n",
      "Episode 600\tLast reward: 115.00\tMoving average reward: 135.51\tMoving average critic value: 437.68\tAction Preferences: 0.50,0.51\n",
      "Episode 700\tLast reward: 200.00\tMoving average reward: 157.64\tMoving average critic value: 785.80\tAction Preferences: 0.50,0.51\n",
      "Episode 800\tLast reward: 161.00\tMoving average reward: 110.50\tMoving average critic value: 358.68\tAction Preferences: 0.47,0.55\n",
      "Episode 900\tLast reward: 200.00\tMoving average reward: 149.14\tMoving average critic value: 709.12\tAction Preferences: 0.50,0.51\n",
      "Episode 1000\tLast reward: 172.00\tMoving average reward: 135.63\tMoving average critic value: 543.67\tAction Preferences: 0.52,0.49\n",
      "New training test\n",
      "Episode 100\tLast reward: 10.00\tMoving average reward: 14.95\tMoving average critic value: 4.49\tAction Preferences: 0.54,0.54\n",
      "Episode 200\tLast reward: 62.00\tMoving average reward: 61.95\tMoving average critic value: 129.17\tAction Preferences: 0.49,0.53\n",
      "Episode 300\tLast reward: 200.00\tMoving average reward: 111.12\tMoving average critic value: 526.26\tAction Preferences: 0.51,0.50\n",
      "Episode 400\tLast reward: 200.00\tMoving average reward: 150.98\tMoving average critic value: 737.21\tAction Preferences: 0.49,0.51\n",
      "Episode 500\tLast reward: 200.00\tMoving average reward: 184.15\tMoving average critic value: 831.35\tAction Preferences: 0.50,0.50\n",
      "Episode 600\tLast reward: 131.00\tMoving average reward: 130.78\tMoving average critic value: 362.30\tAction Preferences: 0.45,0.56\n",
      "Episode 700\tLast reward: 60.00\tMoving average reward: 141.76\tMoving average critic value: 169.69\tAction Preferences: 0.50,0.50\n",
      "Episode 800\tLast reward: 103.00\tMoving average reward: 135.30\tMoving average critic value: 294.02\tAction Preferences: 0.51,0.50\n",
      "Episode 900\tLast reward: 164.00\tMoving average reward: 150.02\tMoving average critic value: 508.99\tAction Preferences: 0.50,0.50\n",
      "Episode 1000\tLast reward: 92.00\tMoving average reward: 121.53\tMoving average critic value: 127.11\tAction Preferences: 0.48,0.53\n",
      "New training test\n",
      "Episode 100\tLast reward: 23.00\tMoving average reward: 25.17\tMoving average critic value: 22.26\tAction Preferences: 0.52,0.54\n",
      "Episode 200\tLast reward: 23.00\tMoving average reward: 44.70\tMoving average critic value: 33.75\tAction Preferences: 0.52,0.51\n",
      "Episode 300\tLast reward: 88.00\tMoving average reward: 67.53\tMoving average critic value: 195.00\tAction Preferences: 0.52,0.50\n",
      "Episode 400\tLast reward: 157.00\tMoving average reward: 122.96\tMoving average critic value: 577.58\tAction Preferences: 0.49,0.52\n",
      "Episode 500\tLast reward: 200.00\tMoving average reward: 171.01\tMoving average critic value: 603.21\tAction Preferences: 0.49,0.52\n",
      "Episode 600\tLast reward: 200.00\tMoving average reward: 170.03\tMoving average critic value: 720.59\tAction Preferences: 0.48,0.52\n",
      "Episode 700\tLast reward: 176.00\tMoving average reward: 131.15\tMoving average critic value: 593.34\tAction Preferences: 0.48,0.53\n",
      "Episode 800\tLast reward: 200.00\tMoving average reward: 165.89\tMoving average critic value: 622.79\tAction Preferences: 0.51,0.50\n",
      "Episode 900\tLast reward: 200.00\tMoving average reward: 156.57\tMoving average critic value: 700.07\tAction Preferences: 0.52,0.49\n",
      "Episode 1000\tLast reward: 200.00\tMoving average reward: 119.71\tMoving average critic value: 747.64\tAction Preferences: 0.46,0.55\n"
     ]
    }
   ],
   "source": [
    "\n",
    "args.trace = False\n",
    "args.render = False\n",
    "args.log_interval = 100\n",
    "for i in range(3):\n",
    "    print(\"New training test\")\n",
    "    reset_trainer()\n",
    "    prime_critic(1)\n",
    "    prime_actor(1)\n",
    "    train(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.log_interval = 10\n",
    "args.trace = False\n",
    "args.render = True\n",
    "train(100) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
